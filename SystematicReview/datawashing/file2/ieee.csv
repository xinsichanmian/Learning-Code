"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"A Progressive Review - Emerging Technologies for ADAS Driven Solutions","J. Nidamanuri; C. Nibhanupudi; R. Assfalg; H. Venkataraman","CSE, Indian Institute of Information Technology Sri City, 565937 TADA, Andhra Pradesh, India, (e-mail: jaswanth.n@iiits.in); ECE, Indian Institute of Information Technology Sri City, 565937 Sri City, Andhra Pradesh, India, (e-mail: chinmayi.n13@iiits.in); Informatics, DHBW Heidenheim, 163326 Heidenheim, Baden-Wrttemberg, Germany, (e-mail: assfalg@dhbw-heidenheim.de); ECE, Indian Institute of Information Technology Sri City, 565937 TADA, Andhra Pradesh, India, (e-mail: hvraman@iiits.in)","IEEE Transactions on Intelligent Vehicles","","2021","PP","99","1","1","Over the last decade, the Advanced Driver Assistance System (ADAS) concept has evolved significantly. ADAS involves several technologies such as automotive electronics, vehicle-to-vehicle (V2V) vehicle-to-infrastructure (V2I) communication, RADAR, LIDAR, computer vision, and machine learning. Of these, computer vision and machine learning-based solutions have mainly been effective that have allowed real-time vehicle control, driver aided systems, etc. However, most of the existing works deal with the deployment of ADAS and autonomous driving functionality in countries with well-disciplined lane traffic. Nevertheless, these solutions and frameworks do not work in countries and cities with less-disciplined/chaotic traffic. This paper identifies the research gaps, reviews the state-of-the-art looking at the different functionalities of ADAS and its levels of autonomy. Importantly, it provides a detailed description of vision intelligence and computational intelligence for ADAS. In visual intelligence, the estimate of eye gaze and head position is detailed. Notably, the learning algorithms such as supervised, unsupervised, reinforcement learning and deep learning solutions for ADAS are considered and discussed. Significantly, this would enable developing a real-time recommendation system for system-assisted/autonomous vehicular environments with less-disciplined road traffic.","2379-8904","","10.1109/TIV.2021.3122898","Department of Science and Technology Government of India(grant numbers:DST/ECR/2016/001287); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591277","Autonomous Driving;Computer Vision;Machine Learning;Recommender Engine;Smart Transportation","Sensors;Roads;Vehicles;Cameras;Laser radar;Safety;Radar","","","","","","","IEEE","27 Oct 2021","","","IEEE","IEEE Early Access Articles"
"Deep Learning based Eye gaze estimation and prediction","P. L. Mazzeo; D. D'Amico; P. Spagnolo; C. Distante","Institute of Science Application and Intelligent Systems - CNR, C/O DHITECH Campus Universitario,Lecce,ITALY,73100; Università del Salento,Lecce,ITALY,73100; Institute of Science Application and Intelligent Systems - CNR, C/O DHITECH Campus Universitario,Lecce,ITALY,73100; Institute of Science Application and Intelligent Systems - CNR, C/O DHITECH Campus Universitario,Lecce,ITALY,73100","2021 6th International Conference on Smart and Sustainable Technologies (SpliTech)","21 Oct 2021","2021","","","1","6","Eye gaze understanding plays a crucial role in human social interactions. The gaze has emerged as a powerful tool for many applications including health assessment, disease diagnosis, human behavior, communication analysis. Eye gaze estimation and prediction has become a very hot topic in the field of computer vision during the last decades, with a surprising and continuously growing number of application fields. In the last decade deep neural networks have revolutionized the whole machine learning area, and gaze tracking. Appearance-based models use deep convolutional networks (CNNs) to directly estimate the direction of gaze in the camera's frame of reference. This paper focuses on analyzing and investigating different CNN architectures for gaze estimation and prediction. Two tasks have been developed in this work: the gaze estimation and the gaze prediction based on previously estimated gaze-points. In the first task, several CNNs were used for finding the most accurate gaze estimation. In the second task, we predict gaze locations basing on the previously estimated gaze vectors while leveraging spatio-temporal information encoded in a previously recorded eye-images sequences. We used a Long Short Term Memory (LSTM) and Transformers based on self-attention approach and use of positional encoding, in order to predict next gaze locations. Different architectures has been trained on the OPENEDS2020 public dataset which has been revisited in order to improve overall performance. We, also, propose a novel architecture composed by ResNext50 and two fully connected layer. We demonstrate that proposed architectures obtain results that in terms of angular error outperforms the state of art.","","978-953-290-112-2","10.23919/SpliTech52315.2021.9566413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9566413","","Deep learning;Microprocessors;Estimation;Computer architecture;Tools;Transformers;Medical diagnosis","","","","","","51","","21 Oct 2021","","","IEEE","IEEE Conferences"
"Pupil-Contour-Based Gaze Estimation with Real Pupil Axes for Head-Mounted Eye Tracking","Z. Wan; C. -H. Xiong; W. Chen; H. Zhang; S. Wu","School of Information Science and Engineering, Wuhan University of Science and Technology, 47900 Wuhan, Hubei, China, 430081 (e-mail: zhwan@hust.edu.cn); School of Mechanical Science and Engineering, Huazhong University of Science and Technology, 12443 Wuhan, Hubei, China, 430074 (e-mail: chxiong@hust.edu.cn); School of Mechanical Science and Engineering, Huazhong University of Science and Technology, 12443 Wuhan, Hubei, China, 430074 (e-mail: wbchen@hust.edu.cn); Wuhan University of Science and Technology, 47900 Wuhan, Hubei, China, 430081 (e-mail: hyzhang@wust.edu.cn); School of Information Science and Engineering, Wuhan University of Science and Technology, 47900 Wuhan, Hubei, China, 430081 (e-mail: shiqian.wu@wust.edu.cn)","IEEE Transactions on Industrial Informatics","","2021","PP","99","1","1","Accurate gaze estimation that frees from glints and the slippage problem is challenging. Pupil-contour-based gaze estimation methods can meet this challenge, except that the gaze accuracy is low due to neglecting the pupil's corneal refrac-tion. This paper proposes a refraction-aware gaze estimation approach using the real pupil axis, which is calculated from the virtual pupil image based on the derived function between the real pupil and the refracted virtual pupil. We present a 2D gaze estimation method that regresses the real pupil normal's spherical coordinates to the gaze point. The noise and outliers of calibration data are removed by aggregation filtering and Random Sample Consensus, respectively. Moreover, we propose a 3D gaze estimation method that transforms the real pupil axis to the gaze direction. Experimental results show that the proposed gaze estimation approach has comparable accuracy to state-of-the-art pupil-center-based gaze estimation methods, which suffer from the slippage problem.","1941-0050","","10.1109/TII.2021.3118022","National Key RD Program(grant numbers:2018YFB1307201); Hubei Key Technical Innovation Project(grant numbers:ZDCX2019000025); National Natural Science Foundation of China(grant numbers:52027806,52075191,61775172,U1913205,U1913601); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9560022","Eye tracking;gaze estimation;corneal refraction;pupil contour;slippage;glint-free","Pupils;Estimation;Cameras;Optical refraction;Optical imaging;Cornea;Iris","","","","","","","IEEE","6 Oct 2021","","","IEEE","IEEE Early Access Articles"
"Head-free, Human Gaze-driven Assistive Robotic System for Reaching and Grasping","B. Yang; J. Huang; M. Sun; J. Huo; X. Li; C. Xiong","Huazhong University of Science and Technology,The Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation,Wuhan,China,430074; Huazhong University of Science and Technology,The Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation,Wuhan,China,430074; Huazhong University of Science and Technology,The Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation,Wuhan,China,430074; Huazhong University of Science and Technology,The Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation,Wuhan,China,430074; Huazhong University of Science and Technology,The Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation,Wuhan,China,430074; Huazhong University of Science and Technology,School of Mechanical Science and Engineering and the State Key Laboratory of Digital Manufacturing Equipment and Technology,Wuhan,China,430074","2021 40th Chinese Control Conference (CCC)","6 Oct 2021","2021","","","4138","4143","Patients with limb dysfunction have limited mobility, which prevents them from performing daily activities. We have developed an assistive robot system with an intuitive head free gaze interface. The system consists of multiple modules, including 3D gaze estimation, head free coordinate transformation, intention recognition, and robot trajectory planning. The robotic assistive system obtains clues from the user’s gaze to decode their intentions and implement actions. This allows the user only needs to look at the objects to make the robot system reach, grasp, and bring them to the user. The 3D gaze estimation is evaluated with 5 subjects, showing an overall accuracy of 5.53±1.2 cm. The integrated system’s experimental results show that the success rate is 96% in the implementation of automatic trajectory planning, and the success rate is 92% in the implementation of fixation-based trajectory planning. Finally, the results and work required to improve the system are discussed.","1934-1768","978-9-8815-6380-4","10.23919/CCC52363.2021.9549800","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9549800","3D gaze estimation;Intention estimation;Gaze-based control;Assistive robot","Three-dimensional displays;Trajectory planning;Robot kinematics;Estimation;Grasping;Rehabilitation robotics","","","","","","14","","6 Oct 2021","","","IEEE","IEEE Conferences"
"Tufts Dental Database: A Multimodal Panoramic X-ray Dataset for Benchmarking Diagnostic Systems","K. Panetta; R. Rajendran; A. Ramesh; S. P. Rao; S. Agaian","Department of Electrical and Computer Engineering, Tufts University, Medford, MA.; Department of Electrical and Computer Engineering, Tufts University, Medford, MA.; School of Dental Medicine, Tufts University, Boston, MA. E-mail: Aruna.Ramesh@tufts.edu; Department of Electrical and Computer Engineering, Tufts University, Medford, MA.; Computer Science, City University of New York, 2009 New York, New York, United States, (e-mail: sos.agaian@csi.cuny.edu)","IEEE Journal of Biomedical and Health Informatics","","2021","PP","99","1","1","The application of Artificial Intelligence in dental healthcare has a very promising role due to the abundance of imagery and non-imagery-based clinical data. Expert analysis of dental radiographs can provide crucial information for clinical diagnosis and treatment. In recent years, Convolutional Neural Networks have achieved the highest accuracy in various benchmarks, including analyzing dental X-ray images to improve clinical care quality. The Tufts Dental Database, a new X-ray panoramic radiography image dataset, has been presented in this paper. This dataset consists of 1000 panoramic dental radiography images with expert labeling of abnormalities and teeth. The classification of radiography images was performed based on five different levels: anatomical location, peripheral characteristics, radiodensity, effects on the surrounding structure, and the abnormality category. This first-of-its-kind multimodal dataset also includes the radiologist's expertise captured in the form of eye-tracking and think-aloud protocol. The contributions of this work are 1) publicly available dataset that can help researchers to incorporate human expertise into AI and achieve more robust and accurate abnormality detection; 2) a benchmark performance analysis for various state-of-the-art systems for dental radiograph image enhancement and image segmentation using deep learning; 3) an in-depth review of various panoramic dental image datasets, along with segmentation and detection systems. The release of this dataset aims to propel the development of AI-powered automated abnormality detection and classification in dental panoramic radiographs, enhance tooth segmentation algorithms, and the ability to distill the radiologist's expertise into AI.","2168-2208","","10.1109/JBHI.2021.3117575","National Institute of Justice Graduate Research Fellowship in Science Technology Engineering and Mathematics FY 2019(grant numbers:2019-R2-CX-0036); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9557804","The tufts dental database;dental panoramic radiographs;eye-tracking;expert capturing system;gaze plots;image enhancement;tooth classification;tooth segmentation","Dentistry;Teeth;X-ray imaging;Image segmentation;Artificial intelligence;Feature extraction;Diagnostic radiography","","","","","","","CCBY","4 Oct 2021","","","IEEE","IEEE Early Access Articles"
"Real Time Eye Monitoring System Using CNN for Drowsiness and Attentiveness System","R. Pai; A. Dubey; N. Mangaonkar","Sardar Patel Institute of technology (SPIT),Master of Computer Applications,Mumbai,400047; Sardar Patel Institute of technology (SPIT),Master of Computer Applications,Mumbai,400047; Sardar Patel Institute of technology (SPIT),Mumbai,400047","2021 Asian Conference on Innovation in Technology (ASIANCON)","4 Oct 2021","2021","","","1","4","As computer science is evolving it can be used for a wide variety of things, for the scope of our research we look to track eye movements for the drowsiness and attentive detection of a person. The movements of eyes or feeling drowsiness can have many adverse effects on an individual, and even can lead to catastrophic situations. Eye Monitoring System for Drowsiness and Attentiveness Detection is developed for monitoring the drowsiness and attentiveness of any individual. A lot of drivers suffer from drowsiness while driving the cars, trucks and auto rickshaws. The reason for drowsiness can differ from person to person but feeling drowsy while driving can lead to various safety issues for the driver and other people on the road. A driver feels drowsiness during driving due to lack of sleep, tiredness and due to long travel driving. Drowsiness can also be felt by students in online classes and students tend not to be interested in online classes as there is no faculty keeping an eye on them since they are not in the classrooms physically. As covid-19 impacted education institutions the only way to fill the gap between the students and faculty is online classes, hence online classes are the only solution left. Not only in India but throughout the world online classes are used. A system for checking the attentiveness of students is an active area of research. Our aim is proposing a solution to detect the drowsiness of the driver while he is driving and inattentiveness of students in online class. The main purpose of this research paper is to detect drowsiness and attentiveness using different algorithms and techniques and use the one with best accuracy in our system. The techniques used are Euclidean and CNN. The paper also provides a comparison between these techniques.","","978-1-7281-8402-9","10.1109/ASIANCON51346.2021.9544624","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9544624","Drowsiness Detection;Deep Learning;Euclidean Distance;Neural Network;Face Detection","Technological innovation;Sleep;Tracking;Roads;Gaze tracking;Fatigue;Real-time systems","","","","","","11","","4 Oct 2021","","","IEEE","IEEE Conferences"
"Predicting Secondary Task Performance: A Directly Actionable Metric for Cognitive Overload Detection","P. V. Amadori; T. Fischer; R. Wang; Y. Demiris","Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K. (e-mail: pierluigi.amadori@gmail.com); Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.; Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.; Personal Robotics Lab, Department of Electrical and Electronic Engineering, Imperial College London, SW7 2BT, U.K.","IEEE Transactions on Cognitive and Developmental Systems","","2021","PP","99","1","1","In this paper, we address cognitive overload detection from unobtrusive physiological signals for users in dual-tasking scenarios. Anticipating cognitive overload is a pivotal challenge in interactive cognitive systems and could lead to safer shared-control between users and assistance systems. Our framework builds on the assumption that decision mistakes on the cognitive secondary task of dual-tasking users correspond to cognitive overload events, wherein the cognitive resources required to perform the task exceed the ones available to the users. We propose DecNet, an end-to-end sequence-to-sequence deep learning model that infers in real-time the likelihood of user mistakes on the secondary task, i.e., the practical impact of cognitive overload, from eye-gaze and head-pose data. We train and test DecNet on a dataset collected in a simulated driving setup from a cohort of 20 users on two dual-tasking decision-making scenarios, with either visual or auditory decision stimuli. DecNet anticipates cognitive overload events in both scenarios and can perform in time-constrained scenarios, anticipating cognitive overload events up to 2s before they occur. We show that DecNet’s performance gap between audio and visual scenarios is consistent with user perceived difficulty. This suggests that single modality stimulation induces higher cognitive load on users, hindering their decision-making abilities.","2379-8939","","10.1109/TCDS.2021.3114162","Defence Science and Technology Laboratory(grant numbers:EP/P008461/1); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9542977","Cognitive Workload;User Monitoring;Decision Anticipation;Simulated Driving.","Task analysis;Vehicles;Load modeling;Solid modeling;Visualization;Feature extraction;Data models","","","","","","","Crown","21 Sep 2021","","","IEEE","IEEE Early Access Articles"
"GazeFlow: Gaze Redirection with Normalizing Flows","Y. Wu; H. Liang; X. Hou; L. Shen","Shenzhen University,Computer Vision Institute, College of Computer Science and Software Engineering; Shenzhen University,Computer Vision Institute, College of Computer Science and Software Engineering; Shenzhen University,Computer Vision Institute, College of Computer Science and Software Engineering; Shenzhen University,Computer Vision Institute, College of Computer Science and Software Engineering","2021 International Joint Conference on Neural Networks (IJCNN)","21 Sep 2021","2021","","","1","8","Gaze estimation often requires a large scale datasets with well annotated gaze information to train the estimator. However, such a dataset requires costive annotation and is usually very difficult to collect. Therefore, a number of gaze redirection approaches have been proposed to address such a problem. However, existing methods lack the ability to precisely synthesize images with target gaze and head pose in complex lighting scenes. As a powerful technique to model the distribution of given data, normalizing flows have the ability to generate photo-realistic images and provide flexible latent space manipulation. In this work, we present a novel flow-based generative model, GazeFlow<sup>1</sup><sup>1</sup>The code will be made available at https://github.com/CVI-SZU/GazeFlow, for gaze redirection. The visual results of gaze redirection show that the quality of eye images synthesized by GazeFlow is significantly higher than that of other approaches like Deep Warp and PRGAN. Our approach has also been applied to augment the training data to improve the accuracy of gaze estimators and significant improvement has been achieved for both within dataset and cross dataset experiments.","2161-4407","978-1-6654-3900-8","10.1109/IJCNN52387.2021.9533913","National Natural Science Foundation of China(grant numbers:91959108,U1713214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9533913","","Visualization;Head;Codes;Annotations;Neural networks;Training data;Lighting","","","","","","41","","21 Sep 2021","","","IEEE","IEEE Conferences"
"Using deep learning to increase accuracy of gaze controlled prosthetic arm","T. Kocejko","Gdansk University of Technology,Department of Biomedical Engineering,Gdansk,Poland","2021 14th International Conference on Human System Interaction (HSI)","17 Sep 2021","2021","","","1","5","This paper presents how neural networks can be utilized to improve the accuracy of reach and grab functionality of hybrid prosthetic arm with eye tracing interface. The LSTM based Autoencoder was introduced to overcome the problem of lack of accuracy of the gaze tracking modality in this hybrid interface. The gaze based interaction strongly depends on the eye tracking hardware. In this paper it was presented how the overall the accuracy can be slightly improved by software solution. The cloud of points related to possible final positions of the arm was created to train Autoencoder. The trained model was next used to improve the position provided by the eye tracker. Using the LSTM based Autoencoder resulted in nearly 3% improvement of the overall accuracy.","2158-2254","978-1-6654-4112-4","10.1109/HSI52170.2021.9538710","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9538710","gaze tracking;prosthetic arm;deep filter;HCI;human computer interaction","Deep learning;Neural networks;Gaze tracking;Software;Hardware;Prosthetics","","","","","","15","","17 Sep 2021","","","IEEE","IEEE Conferences"
"Appearance-based Gaze Estimation using Attention and Difference Mechanism","M. L R D; P. Biswas","I3D Lab, CPDM, Indian Institute of Science,Bangalore; I3D Lab, CPDM, Indian Institute of Science,Bangalore","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","1 Sep 2021","2021","","","3137","3146","Appearance-based gaze estimation problem received wide attention over the past few years. Even though model-based approaches existed earlier, availability of large datasets and novel deep learning techniques made appearance-based methods achieve superior accuracy than model-based approaches. In this paper, we proposed two novel techniques to improve gaze estimation accuracy. Our first approach, I2D-Net uses a difference layer to eliminate any common features from left and right eyes of a participant that are not pertinent to gaze estimation task. Our second approach, AGE-Net adapted the idea of attention-mechanism and assigns weights to the features extracted from eye images. I2D-Net performed on par with the existing state-of-the-art approaches while AGE-Net reported state-of-the-art accuracy of 4.09<sup>◦</sup> and 7.44<sup>◦</sup> error on MPI-IGaze and RT-Gene datasets respectively. We performed ablation studies to understand the effectiveness of the proposed approaches followed by analysis of gaze error distribution with respect to various factors of MPIIGaze dataset.","2160-7516","978-1-6654-4899-4","10.1109/CVPRW53098.2021.00351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9523145","","Deep learning;Computer vision;Adaptation models;Conferences;Estimation;Lighting;Feature extraction","","","","","","42","","1 Sep 2021","","","IEEE","IEEE Conferences"
"GOO: A Dataset for Gaze Object Prediction in Retail Environments","H. Tomas; M. Reyes; R. Dionido; M. Ty; J. Mirando; J. Casimiro; R. Atienza; R. Guinto",University of the Philippines; University of the Philippines; University of the Philippines; University of the Philippines; University of the Philippines; University of the Philippines; University of the Philippines; Samsung R&D Institute Philippines,"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","1 Sep 2021","2021","","","3119","3127","One of the most fundamental and information-laden actions humans do is to look at objects. However, a survey of current works reveals that existing gaze-related datasets annotate only the pixel being looked at, and not the boundaries of a specific object of interest. This lack of object an-notation presents an opportunity for further advancing gaze estimation research. To this end, we present a challenging new task called gaze object prediction, where the goal is to predict a bounding box for a person’s gazed-at object. To train and evaluate gaze networks on this task, we present the Gaze On Objects (GOO) dataset. GOO is composed of a large set of synthetic images (GOO-Synth) supplemented by a smaller subset of real images (GOO-Real) of people looking at objects in a retail environment. Our work establishes extensive baselines on GOO by re-implementing and evaluating selected state-of-the-art models on the task of gaze following and domain adaptation. Code is available<sup>1</sup> on github.","2160-7516","978-1-6654-4899-4","10.1109/CVPRW53098.2021.00349","Samsung; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9523161","","Training;Computer vision;Adaptation models;Conferences;Estimation;Computer architecture;Benchmark testing","","","","","","27","","1 Sep 2021","","","IEEE","IEEE Conferences"
"Visual Focus of Attention Estimation in 3D Scene with an Arbitrary Number of Targets","R. Siegfried; J. -M. Odobez","Idiap Research Institute,Martigny,Switzerland; Idiap Research Institute,Martigny,Switzerland","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","1 Sep 2021","2021","","","3147","3155","Visual Focus of Attention (VFOA) estimation in conversation is challenging as it relies on difficult to estimate information (gaze) combined with scene features like target positions and other contextual information (speaking status) allowing to disambiguate situations. Previous VFOA models fusing all these features are usually trained for a specific setup and using a fixed number of interacting people, and should be retrained to be applied to another one, which limits their usability. To address these limitations, we propose a novel deep learning method that encodes all input features as a fixed number of 2D maps, which makes the input more naturally processed by a convolutional neural network, provides scene normalization, and allows to consider an arbitrary number of targets. Experiments performed on two publicly available datasets demonstrate that the proposed method can be trained in a cross-dataset fashion without loss in VFOA accuracy compared to intra-dataset training.","2160-7516","978-1-6654-4899-4","10.1109/CVPRW53098.2021.00352","KT; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9523184","","Deep learning;Training;Visualization;Three-dimensional displays;TV;Estimation;Usability","","","","","","25","","1 Sep 2021","","","IEEE","IEEE Conferences"
"A Lightweight Heatmap-based Eye Tracking System","X. Luan; B. Zhang; D. Liu; X. Liu; X. Tong; K. Li","College of Intelligence and Computing, Tianjin University,Tianjin Key Laboratory of Advanced Networking (TANK),China; College of Intelligence and Computing, Tianjin University,Tianjin Key Laboratory of Advanced Networking (TANK),China; College of Intelligence and Computing, Tianjin University,Tianjin Key Laboratory of Advanced Networking (TANK),China; College of Intelligence and Computing, Tianjin University,Tianjin Key Laboratory of Advanced Networking (TANK),China; College of Intelligence and Computing, Tianjin University,Tianjin Key Laboratory of Advanced Networking (TANK),China; College of Intelligence and Computing, Tianjin University,Tianjin Key Laboratory of Advanced Networking (TANK),China","2021 International Conference on Computer Communications and Networks (ICCCN)","31 Aug 2021","2021","","","1","9","Eye tracking is playing an important role in many applications including human-computer interaction and behavior study. However, the existing approaches have at least one of the following limitations: (i) dedicated devices such as infrared camera and eye-tracker are required; (ii) complex calibration process is involved; (iii) substantial computing resources are consumed; (iv) users suffer from the risk of privacy leakage. To address the above limitations, we propose a H eatmap-based E ye T racking (HETrack) system. One of the key challenges in our system is to design a lightweight model for fine-grained tracking when the computing resources of device is limited. Also, it is necessary to protect user privacy in such a system. To address the above challenging issues, the proposed system consists of the following processes. First, when users randomly look at the screen of the device, HETrack obtains the raw image containing facial information. Then, we design a neural network model and train it with federated learning. The model can map the image to heatmap that implies the possibility of the user’s gaze position on the screen. Finally, HETrack can intercept the real-time video stream into frames, and employ the trained model to generate the heatmap of current frame for gaze estimation. We implement HETrack based on a Commercial-Off-The-Shelf (COTS) camera and conduct extensive experiments to evaluate its performance. Our HETrack system only requires once calibration; whereas, the state-of-the-art work proposed by Google requires 3~5 times calibration on average. Unlike previous approaches that transmit raw image data to a central server, in our HETrack system, only parameters are transmitted, thereby well protecting the user’s privacy. Experimental results demonstrate that the average distance error of estimated gaze point is 3cm, which is compatible with the state-of-the-art methods.","2637-9430","978-1-6654-1278-0","10.1109/ICCCN52240.2021.9522300","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9522300","Eye tracking;Heatmap;Federated learning","Heating systems;Training;Privacy;Computational modeling;Estimation;Gaze tracking;Streaming media","","","","","","28","","31 Aug 2021","","","IEEE","IEEE Conferences"
"Depression Detection by Combining Eye Movement with Image Semantics","Y. Lin; H. Ma; Z. Pan; R. Wang","University of Science and Technology Beijing,School of Computer and Communication Engineering,Beijing,China; University of Science and Technology Beijing,School of Computer and Communication Engineering,Beijing,China; Tsinghua University,Department of Electronic Engineering,Beijing,China; University of Science and Technology Beijing,School of Computer and Communication Engineering,Beijing,China","2021 IEEE International Conference on Image Processing (ICIP)","23 Aug 2021","2021","","","269","273","Depression is a common mental disorder that affects patients’ daily life. Most existing depression detection methods consume a lot of medical resources and exist at risk of subjective judgment. Therefore, we propose an objective and convenient experimental paradigm. Firstly, it selects emotional images as stimuli and records the subjects’ eye movement data. Secondly, we establish a connection between image processing and subjects’ psychological conditions analysis. Rather than some AI-based methods focus on feature engineering of recorded data, we design the saliency difference detection network and semantic segmentation network to explore the images’ deep semantic features and combine them with the subjects’ gaze pattern. Finally, we train a mental state classifier of Support Vector Machine to detect depression. The experimental results demonstrate that it achieves accuracy up to 90.06%, which outperforms previous methods.","2381-8549","978-1-6654-4115-5","10.1109/ICIP42928.2021.9506702","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506702","Depression detection;image semantics;eye movement;saliency difference detection;semantic segmentation","Support vector machines;Deep learning;Image segmentation;Mental disorders;Design methodology;Semantics;Psychology","","","","","","21","","23 Aug 2021","","","IEEE","IEEE Conferences"
"The Multimodal Driver Monitoring Database: A Naturalistic Corpus to Study Driver Attention","S. Jha; M. F. Marzban; T. Hu; M. H. Mahmoud; N. Al-Dhahir; C. Busso","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Richardson, TX 75080 USA (e-mail: busso@utdallas.edu).","IEEE Transactions on Intelligent Transportation Systems","","2021","PP","99","1","17","A smart vehicle should be able to monitor the actions and behaviors of the human driver to provide critical warnings or intervene when necessary. Recent advancements in deep learning and computer vision have shown great promise in monitoring human behavior and activities. While these algorithms work well in a controlled environment, naturalistic driving conditions add new challenges such as illumination variations, occlusions, and extreme head poses. A vast amount of in-domain data is required to train models that provide high performance in predicting driving related tasks to effectively monitor driver actions and behaviors. Toward building the required infrastructure, this paper presents the multimodal driver monitoring (MDM) dataset, which was collected with 59 subjects that were recorded performing various tasks. We use the Fi-Cap device that continuously tracks the head movement of the driver using fiducial markers, providing frame-based annotations to train head pose algorithms in naturalistic driving conditions. We ask the driver to look at predetermined gaze locations to obtain accurate correlation between the driver's facial image and visual attention. We also collect data when the driver performs common secondary activities such as navigation using a smart phone and operating the in-car infotainment system. All of the driver's activities are recorded with high definition RGB cameras and a time-of-flight depth camera. We also record the controller area network-bus (CAN-Bus), extracting important information. These high quality recordings serve as the ideal resource to train various efficient algorithms for monitoring the driver, providing further advancements in the field of in-vehicle safety systems.","1558-0016","","10.1109/TITS.2021.3095462","Semiconductor Research Corporation SRC Texas Analog Center of Excellence TxACE(grant numbers:2810.014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507390","In-vehicle safety;driver visual attention;driver monitoring dataset.","Vehicles;Magnetic heads;Cameras;Head;Monitoring;Annotations;Safety","","","","","","","CCBYNCND","4 Aug 2021","","","IEEE","IEEE Early Access Articles"
"A Framework for Estimating Gaze Point Information for Location-Based Services","J. Masuho; T. Miyazaki; Y. Sugaya; M. Omachi; S. Omachi","Graduate School of Engineering, Tohoku University, Sendai, Japan; Graduate School of Engineering, Tohoku University, Sendai, Japan; Graduate School of Engineering, Tohoku University, Sendai, Japan; National Institute of Technology Sendai College, Natori, Japan; Graduate School of Engineering, Tohoku University, Sendai, Japan","IEEE Transactions on Vehicular Technology","20 Sep 2021","2021","70","9","8468","8477","In this study, a novel framework for estimating a user's gaze point is proposed. If it is possible to detect what a user is looking at, appropriate services can be provided accordingly. Most existing methods for gaze estimation using image processing are classified into two types: those using the third-person viewpoint and those using the first-person viewpoint. However, the former approach lacks accurate estimation and the latter approach can cause privacy issues. In the proposed framework, sensor information from acceleration and gyro sensors installed in mobile devices is utilized instead of the first-person camera. From the images obtained from the third-person camera, a heatmap showing the possibility of objects that the user is looking at is estimated using machine learning techniques. This information is combined with the position of the user's head, which is obtained from the sensor information, to estimate the location of the user's gaze point. Experimental results show that the proposed method achieves a much higher accuracy than existing techniques. Obtaining user gaze information is very helpful in providing advanced location-based services (LBSs). The proposed framework can increase the added value of various types of LBSs.","1939-9359","","10.1109/TVT.2021.3101932","JSPS KAKENHI(grant numbers:18K19772,19K12033,20H04201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9507254","Location-based service;navigation;activity recognition;machine vision","Cameras;Privacy;Estimation;Heating systems;Surveillance;Navigation;Senior citizens","","","","","","40","IEEE","4 Aug 2021","","","IEEE","IEEE Journals"
"Spherical DNNs and Their Applications in 360<formula><tex>$^\circ$</tex></formula> Images and Videos","Y. Xu; Z. Zhang; S. Gao","Institute of High Performance Computing (IHPC), A*STAR, 54759 Singapore, Singapore, Singapore, (e-mail: xuyy2@shanghaitech.edu.cn); School of Information Science and Techology, ShanghaiTech University, 387433 Shanghai, Shanghai, China, (e-mail: zhangzh@shanghaitech.edu.cn); ShanghaiTech University, ShanghaiTech University, Shanghai, Shanghai, China, (e-mail: gaoshh@shanghaitech.edu.cn)","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2021","PP","99","1","1","Spherical images or videos, as typical non-Euclidean data, are usually stored in the form of 2D panoramas obtained through an equirectangular projection, which is neither equal area nor conformal. The distortion caused by the projection limits the performance of vanilla Deep Neural Networks (DNNs) designed for traditional Euclidean data. In this paper, we design a novel Spherical Deep Neural Network (DNN) to deal with the distortion caused by the equirectangular projection. Specifically, we customize a set of components, including a spherical convolution, a spherical pooling, a spherical ConvLSTM cell and a spherical MSE loss, as the replacements of their counterparts in vanilla DNNs for spherical data. The core idea is to change the identical behavior of the conventional operations in vanilla DNNs across different feature patches so that they will be adjusted to the distortion caused by the variance of sampling rate among different feature patches. We demonstrate the effectiveness of our Spherical DNNs for saliency detection and gaze estimation in <formula><tex>$360^\circ$</tex></formula> videos. To facilitate the study of the 360 video saliency detection, we further construct a large-scale <formula><tex>$360^\circ$</tex></formula> video saliency detection dataset. Comprehensive experiments validate the effectiveness of our proposed Spherical DNNs for spherical handwritten digit classification and sport classification, saliency detection and gaze tracking in <formula><tex>$360^\circ$</tex></formula> videos.","1939-3539","","10.1109/TPAMI.2021.3100259","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497715","Spherical Deep Neural Networks;Saliency Detection;Gaze Prediction;360° Videos","Videos;Saliency detection;Distortion;Convolution;Task analysis;Feature extraction;Kernel","","","","","","","IEEE","27 Jul 2021","","","IEEE","IEEE Early Access Articles"
"Enhanced CNN-Based Gaze Estimation on Wireless Capsule Endoscopy Images","P. Gatoula; G. Dimas; D. K. Iakovidis; A. Koulaouzidis","University of Thessaly,Dept. of Computer Science and Biomedical Informatics,Lamia,Greece; University of Thessaly,Dept. of Computer Science and Biomedical Informatics,Lamia,Greece; University of Thessaly,Dept. of Computer Science and Biomedical Informatics,Lamia,Greece; Pomeranian Medical University,Dept. of Social Medicine and Public Health,Szczecin,Poland","2021 IEEE 34th International Symposium on Computer-Based Medical Systems (CBMS)","12 Jul 2021","2021","","","189","195","Wireless capsule endoscopy (WCE) is a modality used for the non-invasive examination of the gastrointestinal (GI) tract. Physicians diagnose pathologies in images derived from Capsule Endoscopy (CE) using specific gaze patterns to observe pathologically related visual cues. Lately, deep learning has advanced in the domain of human eye-fixation estimation in natural images. However, the potentials of predicting the eye related patterns, such as eye fixations, in medical images has not been thoroughly investigated. In this work, we propose a CNN auto-encoder model, that is capable of predicting saliency maps estimating the gaze-patterns, in terms of eye-fixations, of physicians in CE images. The proposed model outperforms other approaches for visual saliency estimation based on physicians' eye fixation by providing an AUC-J of 0.726 among CE images depicting various pathological and normal cases.","2372-9198","978-1-6654-4121-6","10.1109/CBMS52027.2021.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9474669","Wireless Capsule Endoscopy;Deep Learning;Visual Saliency","Wireless communication;Training;Visualization;Pathology;Endoscopes;Estimation;Medical services","biomedical optical imaging;diseases;endoscopes;eye;image colour analysis;learning (artificial intelligence);medical image processing","specific gaze patterns;pathologically related visual cues;deep learning;human eye-fixation estimation;natural images;eye related patterns;eye fixations;medical images;CNN auto-encoder model;gaze-patterns;eye-fixations;CE images;visual saliency estimation;pathological cases;normal cases;enhanced CNN-based gaze estimation;wireless Capsule Endoscopy images;noninvasive examination;gastrointestinal tract;physicians diagnose pathologies","","","","34","","12 Jul 2021","","","IEEE","IEEE Conferences"
"ADMT: Advanced Driver’s Movement Tracking System Using Spatio-Temporal Interest Points and Maneuver Anticipation Using Deep Neural Networks","S. Gite; B. Pradhan; A. Alamri; K. Kotecha","Department of Computer Science and Information Technology, Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India; Faculty of Engineering and I.T., University of Technology Sydney, Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Ultimo, NSW, Australia; Department of Geology and Geophysics, College of Science, King Saud University, Riyadh, Saudi Arabia; Department of Computer Science and Information Technology, Symbiosis Institute of Technology, Symbiosis International (Deemed University), Pune, India","IEEE Access","19 Jul 2021","2021","9","","99312","99326","Assistive driving is a complex engineering problem and is influenced by several factors such as the sporadic nature of the quality of the environment, the response of the driver, and the standard of the roads on which the vehicle is being driven. The authors track the driver’s anticipation based on his head movements using Spatio-Temporal Interest Point (STIP) extraction and enhance the anticipation of action accuracy well before using the RNN-LSTM framework. This research tackles a fundamental problem of lane change assistance by developing a novel model called Advanced Driver’s Movement Tracking (ADMT). ADMT uses customized convolution-based deep learning networks by using Recurrent Convolutional Neural Network (RCNN). STIP with eye gaze extraction and RCNN performed in ADMT on brain4cars dataset for driver movement tracking. Its performance is compared with the traditional machine learning and deep learning models, namely Support Vector Machines (SVM), Hidden Markov Model (HMM), Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and provided an increment of almost 12% in the prediction accuracy and 44% in the anticipation time. Furthermore, ADMT systems outperformed all of the models in terms of both the accuracy of the system and the previously mentioned time of anticipation that is discussed at length in the paper. Thus it assists the driver with additional anticipation time to access the typical reaction time for better preparedness to respond to undesired future behavior. The driver is then assured of a safe and assisted driving experience with the proposed system.","2169-3536","","10.1109/ACCESS.2021.3096032","Centre for Advanced Modelling and Geospatial Information Systems (CAMGIS), Faculty of Engineering and I.T., University of Technology Sydney (UTS); Researchers Supporting Project(grant numbers:RSP-2021/14); King Saud University, Riyadh, Saudi Arabia; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9478887","RCNN;advanced driver movement tracking system;Spatio-temporal interest points;eye gaze tracking;deep neural networks","Vehicles;Tracking;Feature extraction;Deep learning;Video sequences;Hidden Markov models;Support vector machines","","","","1","","52","CCBY","9 Jul 2021","","","IEEE","IEEE Journals"
"Deep Learning based Approach for Facilitating Online Proctoring using Transfer Learning","A. J S; H. S. Kumaran; S. U; K. P. B. V. Rajesh; L. R","Amrita Vishwa Vidyapeetham,Amrita School of Engineering,Department of Electronics and Communication Engineering,Coimbatore,India; Amrita Vishwa Vidyapeetham,Amrita School of Engineering,Department of Electronics and Communication Engineering,Coimbatore,India; Amrita Vishwa Vidyapeetham,Amrita School of Engineering,Department of Electronics and Communication Engineering,Coimbatore,India; Amrita Vishwa Vidyapeetham,Amrita School of Engineering,Department of Electronics and Communication Engineering,Coimbatore,India; Amrita Vishwa Vidyapeetham,Amrita School of Engineering,Department of Electronics and Communication Engineering,Coimbatore,India","2021 5th International Conference on Computer, Communication and Signal Processing (ICCCSP)","29 Jun 2021","2021","","","306","312","This paper aims at developing an algorithm which helps to ensure the reliability of online examinations. The proposed algorithm provides an automated approach to facilitate online proctoring which alleviates the cumbersome nature of its manual counterpart. It makes use of transfer learning to realize deep learning and it combines three models namely- YOLO (for fraudulent object and multi-person detection), MPGazeII (for abnormal gaze detection) and VGG16 (for Face recognition) and combines the results of all the individual anomaly detection algorithms to ultimately predict whether the examinee has been engaging in malpractice so that necessary action could be taken. The existing algorithms make use of huge amounts of processing for localization as well as tedious feature extraction to realize online proctoring. The proposed algorithm used Deep Learning to overcome the drawbacks of existing online Proctoring algorithms such that no hand-crafted feature extraction is involved.","","978-1-6654-3277-1","10.1109/ICCCSP52374.2021.9465530","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465530","Deep Learning;Neural Networks;Remote online proctoring;VGG-16;YOLO;GazeML;Decision fusion","Deep learning;Transfer learning;Signal processing algorithms;Signal processing;Predictive models;Prediction algorithms;Feature extraction","face recognition;feature extraction;learning (artificial intelligence)","fraudulent object;abnormal gaze detection;individual anomaly detection algorithms;online Proctoring algorithms;deep learning;transfer learning;online examinations;automated approach;cumbersome nature","","","","16","","29 Jun 2021","","","IEEE","IEEE Conferences"
"Real-Time Deep Neuro-Vision Embedded Processing System for Saliency-based Car Driving Safety Monitoring","F. Rundo; R. Leotta; S. Battiato","STMicroelectronics, ADG Central R&D,Catania,Italy; STMicroelectronics, ADG Central R&D,Catania,Italy; STMicroelectronics, ADG Central R&D,Catania,Italy","2021 4th International Conference on Circuits, Systems and Simulation (ICCSS)","28 Jun 2021","2021","","","218","224","Recently, much interest has been aroused by scientific community regarding visual saliency-based applications. The proposed approach contributes to the progressive growth of knowledge of video saliency applications in the automotive field. Through the visual saliency detection, the car driver assistance systems (ADAS i.e. Advanced Driver Assistance Systems) are able to process the driving observation scene selectively. Specifically, it has been observed that the drivers along the driving route, focuses his/her gaze on some objects rather than others. This is determined by the perceptual activity of the brain which through the visual saliency determine the focused scene. We propose a driving safety assessment pipeline which combines a near-real time drowsiness car driver monitoring system driven by a visual saliency detection applied to the acquired driving scene. The proposed approach includes ad-hoc 3D pre-trained Semantic Segmentation Deep Network combined with ad-hoc 1D temporal Deep Dilated Convolutional Neural Network. This architecture was developed for the embedded platform based on STA1295 Accordo5 core (ARM A7 Dual-Cores) embedding an hardware graphics accelerator. The proposed system embeds a bio-sensor which will be placed on the steering wheel of the car having the target to collect the driver's Photoplethysmography (PPG) signal and which will result in a control of driver attention level. The so collected PPG time-series will be classified by the mentioned 1D Temporal Deep Convolutional Network which provides an assessment of the driver attention level. A final analyzer block verifies if the car driver attention level is adequate for the saliency-based scene classification. The performed tests confirmed the effectiveness of the overall proposed pipeline.","","978-1-7281-6752-7","10.1109/ICCSS51193.2021.9464177","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464177","Drowsiness;Deep learning;D-CNN;Deep-LSTM;PPG (PhotoPlethySmography)","Visualization;Pipelines;Wheels;Photoplethysmography;Ad hoc networks;Safety;Automobiles","automobiles;computer vision;convolutional neural nets;deep learning (artificial intelligence);driver information systems;embedded systems;feature extraction;image classification;image segmentation;object detection;road safety;semantic networks;sensor fusion;steering systems;time series;video signal processing","scientific community;visual saliency-based applications;steering wheel;bio-sensor;ad-hoc 1D temporal deep dilated convolutional neural network;ad-hoc 3D pre-trained semantic segmentation deep network;near-real time drowsiness car driver monitoring system;saliency-based car driving safety monitoring;real-time deep neuro-vision embedded processing system;saliency-based scene classification;car driver attention level;PPG time-series;STA1295 Accordo5 core;driving safety assessment pipeline;driving observation scene;advanced driver assistance systems;car driver assistance systems;visual saliency detection;video saliency applications","","","","30","","28 Jun 2021","","","IEEE","IEEE Conferences"
"Eye-Gaze Estimation using a Deep Capsule-based Regression Network","V. Bernard; H. Wannous; J. -P. Vandeborre","Univ. Lille, CNRS UMR 9189 - CRIStAL,IMT Lille Douai,Lille,France,F-59000; Univ. Lille, CNRS, Centrale Lille,IMT Lille Douai, UMR 9189 - CRIStAL,Lille,France,F-59000; Univ. Lille, CNRS UMR 9189 - CRIStAL,IMT Lille Douai,Lille,France,F-59000","2021 International Conference on Content-Based Multimedia Indexing (CBMI)","24 Jun 2021","2021","","","1","6","Eye-gaze information is used in a variety of user platforms, such as driver monitoring systems and head-mounted interfaces. In order to estimate human eye-gaze, many solutions have been proposed, using different devices and techniques. However, achieving such estimation using only cheap devices like RGB cameras would enable gaze interactions on mobile devices and therefore generalise this kind of interaction. It could also enable behavior studies based on gaze and made on every day devices. We propose in this paper a new method for eye-gaze estimation using a new deep learning architecture based on the Capsule Neural Network. Capsule Networks have shown great results so far on classification tasks, but only a few works use them for regression tasks.By taking advantage of the Capsule Network architecture and its ability to reconstruct images, we are able to recreate simplified eye images and then estimate human gaze from them. Experiments are performed on two representative datasets for the task of eye-gaze estimation. Encouraging results are obtained for both the estimation and the reconstruction.","1949-3991","978-1-6654-4220-6","10.1109/CBMI50038.2021.9461895","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9461895","","Neural networks;Estimation;Network architecture;Feature extraction;Mobile handsets;Task analysis;Image reconstruction","deep learning (artificial intelligence);eye;gaze tracking;image colour analysis;image reconstruction;neural nets;regression analysis","eye-gaze estimation;deep Capsule-based regression Network;eye-gaze information;human eye-gaze;gaze interactions;Capsule Neural Network;Capsule Network architecture;simplified eye images;human gaze;deep learning architecture","","","","26","","24 Jun 2021","","","IEEE","IEEE Conferences"
"A Human-Robot Interaction System Calculating Visual Focus of Human’s Attention Level","P. Chakraborty; S. Ahmed; M. A. Yousuf; A. Azad; S. A. Alyami; M. A. Moni","Department of Computer Science and Engineering, Comilla University, Cumilla, Bangladesh; Institute of Information Technology, Jahangirnagar University, Savar, Dhaka, Bangladesh; Institute of Information Technology, Jahangirnagar University, Savar, Dhaka, Bangladesh; School of Biotechnology and Biomolecular Sciences, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia; Department of Mathematics and Statistics, Faculty of Science, Imam Mohammad Ibn Saud Islamic University (IMSIU), Riyadh, Saudi Arabia; WHO Collaborating Centre on eHealth, UNSW Digital Health, University of New South Wales Sydney (UNSW Sydney), Sydney, NSW, Australia","IEEE Access","5 Jul 2021","2021","9","","93409","93421","Attention is the mental awareness of human on a particular object or a piece of information. The level of attention indicates how intense the focus is on an object or an instance. In this study, several types of human attention level have been observed. After introducing image segmentation and detection technique for facial features, eyeball movement and gaze estimation were measured. Eye movement were assessed using the video data, and a total of 10197 data instances were manually labelled for the attention level. Then Artificial Neural Network (ANN) and Recurrent Neural Network-Long Short Term Memory (LSTM) based Deep learning (DL) architectures have been proposed for analysing the data. Next, the trained DL model has been implanted into a robotic system that is capable of detecting various features; ultimately leading to the calculation of visual attention for reading, browsing, and writing purposes. This system is capable of checking the attention level of the participants and also can detect if participants are present or not. Based on a certain level of visual focus of attention (VFOA), this system interacts with the person, generates awareness and establishes verbal or visual communication with that person. The proposed ML techniques have achieved almost 99.24% validation accuracy and 99.43% test accuracy. It is also shown in the comparative study that, since the dataset volumes are limited, ANN is more suitable for attention level calculation than RNN-LSTM. We hope that the implemented robotic structure manifests the real-world implication of the proposed method.","2169-3536","","10.1109/ACCESS.2021.3091642","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462086","Human-robot interaction;attention level;visual focus of attention;concentration;ANN;RNN-LSTM","Head;Visualization;Tracking;Writing;Feature extraction;Task analysis;Lighting","deep learning (artificial intelligence);face recognition;gaze tracking;human-robot interaction;image segmentation;object detection;recurrent neural nets;robot vision","human-robot interaction system calculating visual focus;human attention level;image segmentation;detection technique;eyeball movement;gaze estimation;artificial neural network;robotic system;visual attention;attention level calculation;recurrent neural network-long short term memory based deep learning architectures;visual focus of attention;VFOA;RNN-LSTM","","","","45","CCBY","22 Jun 2021","","","IEEE","IEEE Journals"
"Saliency Prediction with External Knowledge","Y. Zhang; M. Jiang; Q. Zhao","University of Minnesota,Twin Cities; University of Minnesota,Twin Cities; University of Minnesota,Twin Cities","2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 Jun 2021","2021","","","484","493","The last decades have seen great progress in saliency prediction, with the success of deep neural networks that are able to encode high-level semantics. Yet, while humans have the innate capability in leveraging their knowledge to decide where to look (e.g. people pay more attention to familiar faces such as celebrities), saliency prediction models have only been trained with large eye-tracking datasets. This work proposes to bridge this gap by explicitly incorporating external knowledge for saliency models as humans do. We develop networks that learn to highlight regions by incorporating prior knowledge of semantic relationships, be it general or domain-specific, depending on the task of interest. At the core of the method is a new Graph Semantic Saliency Network (GraSSNet) that constructs a graph that encodes semantic relationships learned from external knowledge. A Spatial Graph Attention Network is then developed to update saliency features based on the learned graph. Experiments show that the proposed model learns to predict saliency from the external knowledge and outperforms the state-of-the-art on four saliency benchmarks.","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423113","","Knowledge engineering;Bridges;Computer vision;Conferences;Computational modeling;Semantics;Neural networks","computer vision;deep learning (artificial intelligence);feature extraction;gaze tracking;graph theory","saliency prediction;deep neural networks;high-level semantics;eye-tracking datasets;saliency models;semantic relationships;Graph Semantic Saliency Network;Spatial Graph Attention Network;saliency features;graph learning;saliency benchmarks;GraSSNet;visual attention","","","","57","","14 Jun 2021","","","IEEE","IEEE Conferences"
"Subject Guided Eye Image Synthesis with Application to Gaze Redirection","H. Kaur; R. Manduchi","University of California,Santa Cruz; University of California,Santa Cruz","2021 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 Jun 2021","2021","","","11","20","We propose a method for synthesizing eye images from segmentation masks with a desired style. The style encompasses attributes such as skin color, texture, iris color, and personal identity. Our approach generates an eye image that is consistent with a given segmentation mask and has the attributes of the input style image. We apply our method to data augmentation as well as to gaze redirection. The previous techniques of synthesizing real eye images from synthetic eye images for data augmentation lacked control over the generated attributes. We demonstrate the effectiveness of the proposed method in synthesizing realistic eye images with given characteristics corresponding to the synthetic labels for data augmentation, which is further useful for various tasks such as gaze estimation, eye image segmentation, pupil detection, etc. We also show how our approach can be applied to gaze redirection using only synthetic gaze labels, improving the previous state of the art results. The main contributions of our paper are i) a novel approach for Style-Based eye image generation from segmentation mask; ii) the use of this approach for gaze-redirection without the need for gaze annotated real eye images","2642-9381","978-1-6654-0477-8","10.1109/WACV48630.2021.00006","National Eye Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9423373","","Training;Image segmentation;Iris;Image synthesis;Image color analysis;Training data;Estimation","eye;image colour analysis;image motion analysis;image segmentation;image texture","eye image segmentation;gaze redirection;synthetic gaze labels;Style-Based eye image generation;gaze-redirection;subject guided eye image synthesis;given segmentation mask;input style image;data augmentation;synthetic eye images;realistic eye images","","","","40","","14 Jun 2021","","","IEEE","IEEE Conferences"
"Robust Pupil Segmentation using UNET and Morphological Image Processing","S. Gowroju; Aarti; S. Kumar","Lovely Professional University,Computer Science Engineering,Punjab,India; Lovely Professional University,Computer Science Engineering,Punjab,India; Sreyas Institute of Engineering and Technology,Electronics & Communication Engineering,Hyderabad,India","2021 International Mobile, Intelligent, and Ubiquitous Computing Conference (MIUCC)","9 Jun 2021","2021","","","105","109","The current development in image processing towards biometrics systems has opened much research on realtime applications. The deep learning algorithms are added many expectations to the researchers. The main challenges of these applications are vulnerability towards training time, detection accuracy, and accurate segmentation. In addition to this, the visual noise among various biometric systems is the main challenge. In this paper, we deployed the CNN model using modified UNet to perform the segmentation. The proposed method uses noisy images from the MMU (Multi Media University Iris database) dataset. The acquired colored eye images from the dataset exhibit specular reflections, eye gaze, off-angle images with less resolution, and occlusions caused by eyelids and eyelashes. The focus of our work is mainly to perform accurate segmentation in less training time. Compared the existing methods that uses UNet architecture, with the proposed method, we achieved an accuracy of 91.7%.","","978-1-6654-1243-8","10.1109/MIUCC52538.2021.9447658","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9447658","UNET;Pupil Segmentation;Morphological Processing;Accuracy","Training;Location awareness;Image segmentation;Visualization;Image resolution;Media;Ubiquitous computing","eye;feature extraction;gradient methods;image colour analysis;image recognition;image segmentation;iris recognition;learning (artificial intelligence)","robust pupil segmentation;morphological image processing;biometrics systems;realtime applications;deep learning algorithms;vulnerability;training time;detection accuracy;visual noise;biometric systems;CNN model;noisy images;MMU dataset;MultiMedia University Iris database;colored eye images;dataset exhibit specular reflections;eye gaze;off-angle images;UNet architecture","","","","16","","9 Jun 2021","","","IEEE","IEEE Conferences"
"Deep Learning based Vulnerability Detection: Are We There Yet","S. Chakraborty; R. Krishna; Y. Ding; B. Ray","Computer Science, Columbia University, 5798 New York, New York, United States, 10027-6902 (e-mail: saikatc@cs.columbia.edu); Computer Science, Columbia University, 5798 New York, New York, United States, (e-mail: i.m.ralk@gmail.com); Computer Science, Columbia University, 5798 New York, New York, United States, (e-mail: yangruibo.ding@columbia.edu); Computer Science, Columbia University, New York, New York, United States, (e-mail: rayb@cs.columbia.edu)","IEEE Transactions on Software Engineering","","2021","PP","99","1","1","Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning (DL) has resulted in a surge of interest in applying DL for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95% at detecting vulnerabilities. In this paper, we ask, ""how well do the state-of-the-art DL-based techniques perform in a real-world vulnerability prediction scenario"". To our surprise, we find that their performance drops by more than 50%. A systematic investigation of what causes such precipitous performance drop reveals that existing DL-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline up to 33.57% boost in precision and 128.38% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing DL-based vulnerability prediction systems' potential issues and draws a roadmap for future DL-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA.","1939-3520","","10.1109/TSE.2021.3087402","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448435","datasets;neural networks;gaze detection;text tagging","Predictive models;Neural networks;Testing;Data models;Security;Training;Training data","","","","","","","IEEE","8 Jun 2021","","","IEEE","IEEE Early Access Articles"
"A Data-Driven Framework for Intention Prediction via Eye Movement With Applications to Assistive Systems","F. Koochaki; L. Najafizadeh","Department of Electrical and Computer Engineering, Integrated Systems and NeuroImaging Laboratory, Rutgers University, New Brunswick, NJ, USA; Department of Electrical and Computer Engineering, Integrated Systems and NeuroImaging Laboratory, Rutgers University, New Brunswick, NJ, USA","IEEE Transactions on Neural Systems and Rehabilitation Engineering","4 Jun 2021","2021","29","","974","984","Fast and accurate human intention prediction can significantly advance the performance of assistive devices for patients with limited motor or communication abilities. Among available modalities, eye movement can be valuable for inferring the user's intention, as it can be tracked non-invasively. However, existing limited studies in this domain do not provide the level of accuracy required for the reliable operation of assistive systems. By taking a data-driven approach, this paper presents a new framework that utilizes the spatial and temporal patterns of eye movement along with deep learning to predict the user's intention. In the proposed framework, the spatial patterns of gaze are identified by clustering the gaze points based on their density over displayed images in order to find the regions of interest (ROIs). The temporal patterns of gaze are identified via hidden Markov models (HMMs) to find the transition sequence between ROIs. Transfer learning is utilized to identify the objects of interest in the displayed images. Finally, models are developed to predict the user's intention after completing the task as well as at early stages of the task. The proposed framework is evaluated in an experiment involving predicting intended daily-life activities. Results indicate that an average classification accuracy of 97.42% is achieved, which is considerably higher than existing gaze-based intention prediction studies.","1558-0210","","10.1109/TNSRE.2021.3083815","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440978","Eye movement;gaze;eye tracker;predicting intention;assistive technologies;clustering;hidden Markov models (HMMS);gaze;transfer learning;convolutional neural networks (CNN)","Task analysis;Hidden Markov models;Calibration;Feature extraction;Visualization;Tracking;Support vector machines","driver information systems;feature extraction;hidden Markov models;learning (artificial intelligence)","data-driven framework;eye movement;assistive systems;accurate human intention prediction;assistive devices;motor;available modalities;user;data-driven approach;gaze points;displayed images;gaze-based intention prediction studies","Eye Movements;Humans;Intention;Movement;Self-Help Devices","","","41","CCBY","26 May 2021","","","IEEE","IEEE Journals"
"Multi-Modal Learning from Video, Eye Tracking, and Pupillometry for Operator Skill Characterization in Clinical Fetal Ultrasound","H. Sharma; L. Drukker; A. T. Papageorghiou; J. A. Noble","Institute of Biomedical Engineering University of Oxford,Oxford,UK; University of Oxford,Nuffield Department of Women’s and Reproductive Health,Oxford,UK; University of Oxford,Nuffield Department of Women’s and Reproductive Health,Oxford,UK; Institute of Biomedical Engineering University of Oxford,Oxford,UK","2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)","25 May 2021","2021","","","1646","1649","This paper presents a novel multi-modal learning approach for automated skill characterization of obstetric ultrasound operators using heterogeneous spatio-temporal sensory cues, namely, scan video, eye-tracking data, and pupillometric data, acquired in the clinical environment. We address pertinent challenges such as combining heterogeneous, small-scale and variable-length sequential datasets, to learn deep convolutional neural networks in real-world scenarios. We propose spatial encoding for multi-modal analysis using sonography standard plane images, spatial gaze maps, gaze trajectory images, and pupillary response images. We present and compare five multi-modal learning network architectures using late, intermediate, hybrid, and tensor fusion. We build models for the Heart and the Brain scanning tasks, and performance evaluation suggests that multi-modal learning networks outperform uni-modal networks, with the best-performing model achieving accuracies of 82.4% (Brain task) and 76.4% (Heart task) for the operator skill classification problem.","1945-8452","978-1-6654-1246-9","10.1109/ISBI48211.2021.9433863","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9433863","Multi-modal learning;ultrasound;convolutional neural networks;eye tracking;pupillometry","Heart;Performance evaluation;Ultrasonic imaging;Tensors;Biological system modeling;Network architecture;Brain modeling","biomedical ultrasonics;brain;convolutional neural nets;deep learning (artificial intelligence);gaze tracking;image classification;image coding;image fusion;medical image processing;obstetrics;spatiotemporal phenomena","spatial encoding;sonography standard plane images;spatial gaze maps;gaze trajectory images;pupillary response images;multimodal learning network architectures;late fusion;intermediate fusion;hybrid fusion;tensor fusion;unimodal networks;operator skill classification problem;operator skill characterization;multimodal learning approach;automated skill characterization;obstetric ultrasound operators;scan video;eye-tracking data;pupillometric data;clinical environment;variable-length sequential datasets;deep convolutional neural networks;clinical fetal ultrasound;heterogeneous spatiotemporal sensory cues;brain scanning tasks;heart scanning tasks","","","","16","","25 May 2021","","","IEEE","IEEE Conferences"
"A High-Frame-Rate Eye-Tracking Framework for Mobile Devices","Y. Chang; C. He; Y. Zhao; T. Lu; N. Gu","Fudan University,School of Computer Science; Fudan University,School of Computer Science; Fudan University,School of Computer Science; Fudan University,School of Computer Science; Fudan University,School of Computer Science","ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","13 May 2021","2021","","","1445","1449","Gaze-on-screen tracking, an appearance-based eye-tracking task, has drawn significant interest in recent years. While learning-based high-precision eye-tracking methods have been designed in the past, the complex pre-training and high computation in neural network-based deep models restrict their applicability in mobile devices. Moreover, as the display frame rate of mobile devices has steadily increased to 120 fps, high-frame-rate eye tracking becomes increasingly challenging. In this work, we tackle the tracking efficiency challenge and introduce GazeHFR, a biologic-inspired eye-tracking model specialized for mobile devices, offering both high accuracy and efficiency. Specifically, GazeHFR classifies the eye movement into two distinct phases, i.e., saccade and smooth pursuit, and leverages inter-frame motion information combined with lightweight learning models tailored to each movement phase to deliver high-efficient eye tracking without affecting accuracy. Compared to prior art, Gaze-HFR achieves approximately 7x speedup and 15% accuracy improvement on mobile devices.","2379-190X","978-1-7281-7605-5","10.1109/ICASSP39728.2021.9414624","Research and Development; National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9414624","gaze estimation;mobile imaging;biomedical video analysis;applications of machine learning","Target tracking;Biological system modeling;Computational modeling;Motion estimation;Gaze tracking;Machine learning;Signal processing","","","","","","20","","13 May 2021","","","IEEE","IEEE Conferences"
"Temporal Head Pose Estimation From Point Cloud in Naturalistic Driving Conditions","T. Hu; S. Jha; C. Busso","Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA.; Department of Electrical and Computer Engineering, The University of Texas at Dallas, Dallas, TX 75080 USA (e-mail: busso@utdallas.edu).","IEEE Transactions on Intelligent Transportation Systems","","2021","PP","99","1","14","Head pose estimation is an important problem as it facilitates tasks such as gaze estimation and attention modeling. In the automotive context, head pose provides crucial information about the driver's mental state, including drowsiness, distraction and attention. It can also be used for interaction with in-vehicle infotainment systems. While computer vision algorithms using RGB cameras are reliable in controlled environments, head pose estimation is a challenging problem in the car due to sudden illumination changes, occlusions and large head rotations that are common in a vehicle. These issues can be partially alleviated by using depth cameras. Head rotation trajectories are continuous with important temporal dependencies. Our study leverages this observation, proposing a novel temporal deep learning model for head pose estimation from point cloud. The approach extracts discriminative feature representation directly from point cloud data, leveraging the 3D spatial structure of the face. The frame-based representations are then combined with bidirectional long short term memory (BLSTM) layers. We train this model on the newly collected multimodal driver monitoring (MDM) dataset, achieving better results compared to non-temporal algorithms using point cloud data, and state-of-the-art models using RGB images. We further show quantitatively and qualitatively that incorporating temporal information provides large improvements not only in accuracy, but also in the smoothness of the predictions.","1558-0016","","10.1109/TITS.2021.3075350","Semiconductor Research Corporation SRC Texas Analog Center of Excellence TxACE(grant numbers:2810.014); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9425023","Driver head pose estimation;deep learning;point cloud;temporal modeling.","Feature extraction;Three-dimensional displays;Pose estimation;Magnetic heads;Vehicles;Deep learning;Cameras","","","","","","","CCBYNCND","6 May 2021","","","IEEE","IEEE Early Access Articles"
"Estimating Gaze From Head and Hand Pose and Scene Images for Open-Ended Exploration in VR Environments","K. J. Emery; M. Zannoli; L. Xiao; J. Warren; S. S. Talathi","University of Nevada, Reno,Facebook Reality Labs; Facebook Reality Labs; Facebook Reality Labs; Facebook Reality Labs; Facebook Reality Labs","2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","6 May 2021","2021","","","554","555","The widespread utility of eye tracking technology has created a growing demand for more consistent and reliable eye-tracking systems, and there is a need for new and accessible approaches that can enhance the accuracy of eye-tracking data. Previous studies have offered evidence for associations between certain non-eye signals and gaze such as a strong coordination between head motion and gaze shifts. e.g. [3] , hand and eye spatiotemporal statistics, e.g. [7] , and gaze behavior and scene content, e.g. [2] . Previous studies have also shown how various combinations of eye, head, scene, and hand signals can be leveraged for applications such as gaze estimation [5] , [10] , prediction [8] , and classification [6] . Though these previous approaches provide support for the idea that non-eye sensors (i.e. head, hand, and scene) are useful for estimating gaze, they have not yet fully addressed how these signals individually and in combination contribute to gaze estimation.","","978-1-6654-4057-8","10.1109/VRW52623.2021.00159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419188","eye tracking;gaze estimation;virtual reality;non eye sensors","Head;Three-dimensional displays;Conferences;Estimation;Virtual reality;Gaze tracking;User interfaces","eye;gaze tracking;human computer interaction;virtual reality","noneye sensors;gaze estimation;hand pose;scene images;open-ended exploration;VR environments;widespread utility;eye tracking technology;consistent eye-tracking systems;reliable eye-tracking systems;accessible approaches;eye-tracking data;noneye signals;head motion;gaze behavior;scene content;hand signals","","","","10","","6 May 2021","","","IEEE","IEEE Conferences"
"[DC] Eye Fixation Forecasting in Task-Oriented Virtual Reality","Z. Hu",Peking University,"2021 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","6 May 2021","2021","","","707","708","In immersive virtual reality (VR), users' visual attention is crucial for many important applications, including VR content design, gaze-based interaction, and gaze-contingent rendering. Especially, information on users' future eye fixations is key for intelligent user interfaces and has significant relevance for many areas, such as visual attention enhancement, dynamic event triggering, and human-computer interaction. However, previous works typically focused on free-viewing conditions and paid less attention to task-oriented attention. This paper aims at forecasting users' eye fixations in task-oriented virtual reality. To this end, a VR eye tracking dataset that corresponds to different users performing a visual search task in immersive virtual environments is built. A comprehensive analysis of users' eye fixations is performed based on the collected data. The analysis reveals that eye fixations are correlated with users' historical gaze positions, task-related objects, saliency information of the VR content, and head rotation velocities. Based on this analysis, a novel learning-based model is proposed to forecast users' eye fixations in the near future in immersive virtual environments.","","978-1-6654-4057-8","10.1109/VRW52623.2021.00236","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419161","Fixation forecasting—Visual attention—Visual search—Eye tracking; Deep learning—Convolutional neural network—Virtual reality","Visualization;Solid modeling;Three-dimensional displays;Conferences;Virtual environments;Gaze tracking;Predictive models","learning (artificial intelligence);rendering (computer graphics);user interfaces;virtual reality;visual perception","immersive virtual reality;VR content design;gaze-based interaction;gaze-contingent rendering;intelligent user interfaces;visual attention enhancement;human-computer interaction;task-oriented attention;forecasting users;task-oriented virtual reality;VR eye tracking dataset;visual search task;immersive virtual environments;eye fixations;task-related objects;[DC] eye fixation forecasting","","","","12","","6 May 2021","","","IEEE","IEEE Conferences"
"Exposing Deepfake Videos by Tracking Eye Movements","M. Li; B. Liu; Y. Hu; Y. Wang","School of Electronic and Information Engineering, South China University of Technology,Guangzhou,China; School of Electronic and Information Engineering, South China University of Technology,Guangzhou,China; School of Electronic and Information Engineering, South China University of Technology,Guangzhou,China; Sino-Singapore International Joint Research Institute,Guangzhou,China","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","5184","5189","It has recently become a major threat to the public media that fake videos are rapidly spreading over the Internet. The advent of Deepfake, a deep-learning based toolkit, has facilitated a massive abuse of improper synthesized videos, which may influence the media credibility and human rights. A worldwide alert has been set off that finding ways to detect such fake videos is not only crucial but also urgent. This paper reports a novel approach to expose deepfake videos. We found that most fake videos are markedly different from the real ones in the way the eyes move. We are thus motivated to define four features that could well capture such differences. The features are then fed to SVM for classification. It is shown to be a promising approach that without high dimensional features and complicated neural networks, we are able to achieve competitive results on several public datasets. Moreover, the proposed features could well participate with other existing methods in the confrontation with deepfakes.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9413139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413139","","Support vector machines;Tracking;Neural networks;Media;Tools;Feature extraction;Physiology","deep learning (artificial intelligence);gaze tracking;image classification;Internet;support vector machines;video signal processing","Internet;SVM;human rights;media credibility;deep-learning based toolkit;eye movements tracking;Deepfake videos","","","","22","","5 May 2021","","","IEEE","IEEE Conferences"
"FastSal: a Computationally Efficient Network for Visual Saliency Prediction","F. Hu; K. McGuinness","Insight Centre for Data Analytics, Dublin City University; Insight Centre for Data Analytics, Dublin City University","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","9054","9061","This paper focuses on the problem of visual saliency prediction, predicting regions of an image that tend to attract human visual attention, under a constrained computational budget. We modify and test various recent efficient convolutional neural network architectures like EfficientNet and MobileNetV2 and compare them with existing state-of-the-art saliency models such as SalGAN and DeepGaze II both in terms of standard accuracy metrics like Area Under Curve (AUC) and Normalized Scanpath Saliency (NSS), and in terms of the computational complexity and model size. We find that MobileNetV2 makes an excellent backbone for a visual saliency model and can be effective even without a complex decoder. We also show that knowledge transfer from a more computationally expensive model like DeepGaze II can be achieved via pseudo-labelling an unlabelled dataset, and that this approach gives result on-par with many state-of-the-art algorithms with a fraction of the computational cost and model size.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9413057","Science Foundation Ireland (SFI)(grant numbers:SFI/15/SIRG/3283,SFI/12/RC/2289_P2); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413057","","Measurement;Visualization;Computational modeling;Predictive models;Prediction algorithms;Computational efficiency;Pattern recognition","computational complexity;convolutional neural nets;deep learning (artificial intelligence);gaze tracking;human computer interaction;image segmentation;neural net architecture;object tracking","computationally efficient network;visual saliency prediction;human visual attention;convolutional neural network architectures;MobileNetV2;DeepGaze II;normalized scanpath saliency;computational complexity;FastSal;image regions;EfficientNet;SalGAN;area under curve;knowledge transfer;deep neural models;eye tracking;eye fixations","","","","41","","5 May 2021","","","IEEE","IEEE Conferences"
"Detection and Correspondence Matching of Corneal Reflections for Eye Tracking Using Deep Learning","S. Chugh; B. Brousseau; J. Rose; M. Eizenman","University of Toronto,Department of Electrical and Computer Engineering,Toronto,Canada; University of Toronto,Department of Electrical and Computer Engineering,Toronto,Canada; University of Toronto,Department of Electrical and Computer Engineering,Toronto,Canada; Ophthalmology and Vision Sciences, University of Toronto,Department of Electrical and Computer Engineering,Toronto,Canada","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","2210","2217","Eye tracking systems that estimate the point-of-gaze are essential in extended reality (XR) systems as they enable new interaction paradigms and technological improvements. It is important for these systems to maintain accuracy when the headset moves relative to the head (known as device slippage) due to head movements or user adjustment. One of the most accurate eye tracking techniques, which is also insensitive to shifts of the system relative to the head, uses two or more infrared (IR) light emitting diodes to illuminate the eye and an IR camera to capture images of the eye. An essential step in estimating the point-of-gaze in these systems is the precise determination of the location of two or more corneal reflections (virtual images of the IR-LEDs that illuminate the eye) in images of the eye. Eye trackers tend to have multiple light sources to ensure at least one pair of reflections for each gaze position. The use of multiple light sources introduces a difficult problem: the need to match the corneal reflections with the corresponding light source over the range of expected eye movements. Corneal reflection detection and matching often fail in XR systems due to the proximity of camera and steep illumination angles of light sources with respect to the eye. The failures are caused by corneal reflections having varying shape and intensity levels or disappearance due to rotation of the eye, or the presence of spurious reflections. We have developed a fully convolutional neural network, based on the UNET architecture, that solves the detection and matching problem in the presence of spurious and missing reflections. Eye images of 25 people were collected in a virtual reality headset using a binocular eye tracking module consisting of five infrared light sources per eye. A set of 4,000 eye images were manually labelled for each of the corneal reflections, and data augmentation was used to generate a dataset of 40,000 images. The network is able to correctly identify and match 91% of corneal reflections present in the test set. This is comparable to a state-of-the-art deep learning system, but our approach requires 33 times less memory and executes 10 times faster. The proposed algorithm, when used in an eye tracker in a VR system, achieved an average mean absolute gaze error of 1°. This is a significant improvement over the state-of-the-art learning-based XR eye tracking systems that have reported gaze errors of 2-3°.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412066","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412066","Eye tracking;Corneal Reflections;Semantic Segmentation;Virtual Reality;Pattern Recognition","Headphones;Head;Tracking;Memory management;Lighting;Gaze tracking;Reflection","computer vision;eye;gaze tracking;learning (artificial intelligence);light emitting diodes;light sources;neural nets;object detection;optical tracking;virtual reality","corneal reflections;eye tracking systems;point-of-gaze;extended reality systems;accurate eye tracking techniques;eye tracker;multiple light sources;corresponding light source;expected eye movements;XR systems;binocular eye tracking module;infrared light sources;4 eye images;000 eye images;state-of-the-art deep learning system","","","","36","","5 May 2021","","","IEEE","IEEE Conferences"
"Adaptive Feature Fusion Network for Gaze Tracking in Mobile Tablets","Y. Bao; Y. Cheng; Y. Liu; F. Lu","State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University,Beijing,China; State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University,Beijing,China; State Key Laboratory of Virtual Reality Technology and Systems, School of CSE, Beihang University,Beijing,China; State Key Lab. of Virtual Reality Technol. & Syst., Beihang Univ., Beijing, China","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","9936","9943","Recently, many multi-stream gaze estimation methods have been proposed. They estimate gaze from eye and face appearances and achieve reasonable accuracy. However, most of the methods simply concatenate the features extracted from eye and face appearance. The feature fusion process has been ignored. In this paper, we propose a novel Adaptive Feature Fusion Network (AFF-Net), which performs gaze tracking task in mobile tablets. We stack two-eye feature maps and utilize Squeeze-and-Excitation layers to adaptively fuse two-eye features according to their similarity on appearance. Meanwhile, we also propose Adaptive Group Normalization to recalibrate eye features with the guidance of facial feature. Extensive experiments on both GazeCapture and MPIIFaceGaze datasets demonstrate consistently superior performance of the proposed method.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412205","National Natural Science Foundation of China (NSFC)(grant numbers:61972012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412205","","Adaptive systems;Fuses;Navigation;Face recognition;Stacking;Estimation;Gaze tracking","face recognition;feature extraction;gaze tracking;human computer interaction;notebook computers","face appearance;fusion process;gaze tracking task;mobile tablets;two-eye feature maps;Adaptive Group Normalization;facial feature;multistream gaze estimation methods;adaptive feature fusion network;AFF-Net;squeeze-and-excitation layers;GazeCapture dataset;MPIIFaceGaze dataset","","","","42","","5 May 2021","","","IEEE","IEEE Conferences"
"User-Independent Gaze Estimation by Extracting Pupil Parameter and Its Mapping to the Gaze Angle","S. Y. Han; N. I. Cho","INMC, Seoul National Univ.,Dep. of ECE; INMC, Seoul National Univ.,Dep. of ECE","2020 25th International Conference on Pattern Recognition (ICPR)","5 May 2021","2021","","","1993","2000","Since gaze estimation plays a crucial role in recognizing human intentions, it has been researched for a long time, and its accuracy is ever increasing. However, due to the wide variation in eye shapes and focusing abilities between the individuals, accuracies of most algorithms vary depending on each person in the test group, especially when the initial calibration is not well performed. To alleviate the user-dependency, we attempt to derive features that are general for most people and use them as the input to a deep network instead of using the images as the input. Specifically, we use the pupil shape as the core feature because it is directly related to the 3D eyeball rotation, and thus the gaze direction. While existing deep learning methods learn the gaze point by extracting various features from the image, we focus on the mapping function from the eyeball rotation to the gaze point by using the pupil shape as the input. It is shown that the accuracy of gaze point estimation also becomes robust for the uncalibrated points by following the characteristics of the mapping function. Also, our gaze network learns the gaze difference to facilitate the re-calibration process to fix the calibration-drift problem that typically occurs with glass-type or head-mount devices.","1051-4651","978-1-7281-8808-9","10.1109/ICPR48806.2021.9412709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412709","","Training;Three-dimensional displays;Shape;Estimation;Focusing;Multilayer perceptrons;Feature extraction","calibration;eye;gaze tracking;learning (artificial intelligence);object detection;regression analysis","wide variation;eye shapes;focusing abilities;test group;initial calibration;user-dependency;deep network;pupil shape;core feature;eyeball rotation;gaze direction;deep learning methods;mapping function;gaze point estimation;uncalibrated points;gaze network;gaze difference;user-independent gaze estimation;pupil parameter;gaze angle;human intentions","","","","55","","5 May 2021","","","IEEE","IEEE Conferences"
"Hawk-Eye: An AI-Powered Threat Detector for Intelligent Surveillance Cameras","A. A. Ahmed; M. Echi","Department of Computer Science, Prairie View A&M University, Prairie View, TX, USA; Department of Computer Science, Prairie View A&M University, Prairie View, TX, USA","IEEE Access","30 Apr 2021","2021","9","","63283","63293","With recent advances in both AI and IoT capabilities, it is possible than ever to implement surveillance systems that can automatically identify people who might represent a potential security threat to the public in real-time. Imagine a surveillance camera system that can detect various on-body weapons, masked faces, suspicious objects and traffic. This system could transform surveillance cameras from passive sentries into active observers which would help in preventing a possible mass shooting in a school, stadium or mall. In this paper, we present a prototype implementation of such systems, Hawk-Eye, an AI-powered threat detector for smart surveillance cameras. Hawk-Eye can be deployed on centralized servers hosted in the cloud, as well as locally on the surveillance cameras at the network edge. Deploying AI-enabled surveillance applications at the edge enables the initial analysis of the captured images to take place on-site, which reduces the communication overheads and enables swift security actions. At the cloud side, we built a Mask R-CNN model that can detect suspicious objects in an image captured by a camera at the edge. The model can generate a high-quality segmentation mask for each object instance in the image, along with the confidence percentage and classification time. The camera side used a Raspberry Pi 3 device, Intel Neural Compute Stick 2 (NCS 2), and Logitech C920 webcam. At the camera side, we built a CNN model that can consume a stream of images directly from an on-site webcam, classify them, and displays the results to the user via a GUI-friendly interface. A motion detection module is developed to capture images automatically from the video when a new motion is detected. Finally, we evaluated our system using various performance metrics such as classification time and accuracy. Our experimental results showed an average overall prediction accuracy of 94% on our dataset.","2169-3536","","10.1109/ACCESS.2021.3074319","National Science Foundation (NSF)(grant numbers:# 2011330); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408578","Artificial Intelligence (AI);threat detector;surveillance cameras;deep learning;mask Region Based Convolutional Neural Networks (R-CNN);Convolutional Neural Network (CNN);Internet of Things (IoT)","Cameras;Faces;Security;Computational modeling;Weapons;Detectors;Streaming media","cameras;computer vision;convolutional neural nets;gaze tracking;image capture;image colour analysis;image motion analysis;image segmentation;image sensors;Internet;Internet of Things;object detection;video signal processing;video surveillance;weapons","Internet of Things;IoT;centralized servers;Logitech C920 Webcam;Intel Neural Compute Stick 2;Raspberry Pi 3 device;GUI-friendly interface;mass shooting;classification time;confidence percentage;high-quality segmentation mask;mask R-CNN model;swift security actions;image capture;AI-enabled surveillance applications;network edge;smart surveillance cameras;Hawk-Eye;suspicious objects;masked faces;surveillance camera system;security threat;intelligent surveillance cameras;AI-powered threat detector","","","","25","CCBY","20 Apr 2021","","","IEEE","IEEE Journals"
"Machine Learning Based Autism Spectrum Disorder Detection from Videos","C. Wu; S. Liaqat; H. Helvaci; S. -c. S. Chcung; C. -N. Chuah; S. Ozonoff; G. Young","University of California,Department of Computer Science,Davis,CA,US; University of Kentucky,Department of Electrical and Computer Engineering,Lexington,KY,US; University of Kentucky,Department of Electrical and Computer Engineering,Lexington,KY,US; University of California,Department of Electrical and Computer Engineering,Davis,CA,US; University of California,Department of Electrical and Computer Engineering,Davis,CA,US; UC Davis MIND Institute, University of California,Davis,CA,US; UC Davis MIND Institute, University of California,Davis,CA,US","2020 IEEE International Conference on E-health Networking, Application & Services (HEALTHCOM)","14 Apr 2021","2021","","","1","6","Early diagnosis of Autism Spectrum Disorder (ASD) is crucial for best outcomes to interventions. In this paper, we present a machine learning (ML) approach to ASD diagnosis based on identifying specific behaviors from videos of infants of ages 6 through 36 months. The behaviors of interest include directed gaze towards faces or objects of interest, positive affect, and vocalization. The dataset consists of 2000 videos of 3-minute duration with these behaviors manually coded by expert raters. Moreover, the dataset has statistical features including duration and frequency of the above mentioned behaviors in the video collection as well as independent ASD diagnosis by clinicians. We tackle the ML problem in a two-stage approach. Firstly, we develop deep learning models for automatic identification of clinically relevant behaviors exhibited by infants in a one-on-one interaction setting with parents or expert clinicians. We report baseline results of behavior classification using two methods: (1) image based model (2) facial behavior features based model. We achieve 70% accuracy for smile, 68% accuracy for look face, 67% for look object and 53% accuracy for vocalization. Secondly, we focus on ASD diagnosis prediction by applying a feature selection process to identify the most significant statistical behavioral features and a over and under sampling process to mitigate the class imbalance, followed by developing a baseline ML classifier to achieve an accuracy of 82% for ASD diagnosis.","","978-1-7281-6267-6","10.1109/HEALTHCOM49281.2021.9398924","National Institute of Mental Health of the National Institutes of Health(grant numbers:R01MH121344-01); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9398924","Autism Spectrum Disorder;Machine Learning;Human Behavior Detection;Facial Keypoint Detection","Training;Autism;Neural networks;Manuals;Feature extraction;Faces;Videos","biomedical optical imaging;feature extraction;image classification;learning (artificial intelligence);medical disorders;medical image processing;pattern classification","autism spectrum disorder detection;machine learning approach;video collection;independent ASD diagnosis;ML problem;two-stage approach;deep learning models;ASD diagnosis prediction;feature selection process;baseline ML classifier","","","","23","","14 Apr 2021","","","IEEE","IEEE Conferences"
"Gaze Estimation via the Joint Modeling of Multiple Cues","W. Chen; H. Xu; C. Zhu; X. Liu; Y. Lu; C. Zheng; J. Kong","College of Information Sciences and Technology, Northeast Normal University, Changchun 130117, China.; College of Information Sciences and Technology, Northeast Normal University, Changchun 130117, China.; College of Information Sciences and Technology, Northeast Normal University, Changchun 130117, China.; Department of Chemical & Biomolecular Engineering, National University of Singapore, Singapore 117585, Singapore.; Institute for Intelligent Elderly Care, College of Humanities & Sciences of Northeast Normal University, Changchun 130117, China.; College of Information Sciences and Technology, Northeast Normal University, Changchun 130117, China and Key Laboratory of Applied Statistics of MOE, Northeast Normal University, Changchun 130024, China. (e-mail: zhengcx789@nenu.edu.cn); Institute for Intelligent Elderly Care, College of Humanities & Sciences of Northeast Normal University, Changchun 130117, China and Key Laboratory of Applied Statistics of MOE, Northeast Normal University, Changchun 130024, China.","IEEE Transactions on Circuits and Systems for Video Technology","","2021","PP","99","1","1","How to automatically predict people’s gaze has attracted attention in the field of computer vision and machine learning. Previous studies on this topic set many constraints, such as restricted scenarios and strict and complex inputs. To mitigate these constraints to predict the gaze of people in more general scenarios, we propose a three-pathway network (TPNet) to estimate gaze via the joint modeling of multiple cues. Specifically, we first design a human-centric relationship inference (HCRI) module to learn the object-level relationship between the target person and the surrounding persons/objects in a scene. To the best of our knowledge, this is the first time that the object-level relationship is introduced into the gaze estimation task. Then, we construct a novel deep network with three pathways to fuse multiple cues, including scene saliency, object-level relationships and head information, to predict the gaze target. In addition, to extract the multilevel features during network training, we build and embed a micropyramid module in TPNet. The performance of TPNet is evaluated on two gaze estimation datasets: GazeFollow and DLGaze. A large number of quantitative and qualitative experimental results verify that TPNet can obtain robust results and significantly outperform the existing state-of-the-art gaze estimation methods. The code of TPNet will be released later.","1558-2205","","10.1109/TCSVT.2021.3071621","National Natural Science Foundation of China(grant numbers:61672150,61702092,61907007); Fund of the Jilin Provincial Science and Technology Department(grant numbers:20180201089GX,20190201305JC,20200201199JC,20200401081GX,20200401086GX); Fundamental Research Funds for the Central Universities(grant numbers:2412019FZ049,2412020FZ029,2412020FZ031); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9398702","Gaze Estimation;Three-pathway Network;Multiple Cues Fusion;Human-centric Relationship Inference;Image Understanding;Computer Vision","Estimation;Visualization;Task analysis;Feature extraction;Faces;Semantics;Saliency detection","","","","","","","IEEE","7 Apr 2021","","","IEEE","IEEE Early Access Articles"
"Appearance-based gaze estimation using separable convolution neural networks","Y. Zhuang; Y. Zhang; H. Zhao","North China University of Technology,Information Institute,Beijing,China; North China University of Technology,Information Institute,Beijing,China; North China University of Technology,Information Institute,Beijing,China","2021 IEEE 5th Advanced Information Technology, Electronic and Automation Control Conference (IAEAC)","5 Apr 2021","2021","5","","609","612","Gaze estimation is one of the current important research contents of computer vision. For the current situation where the gaze estimation neural network has a large amount of parameters but the accuracy is not greatly improved and the head pose is difficult to handle, this paper proposes a simplified gaze estimation network model SLeNet based on the LeNet neural network. The deep separable convolution in the Xception network is used to reduce the amount of parameters in the convolution part and improve the computational performance of the network model. The method of splicing head posture features is retained, but another branch neural network is designed to learn head posture based on eye image and mouth corner information, and no additional module is required to obtain head posture separately. The improved network model is used to compare experiments with the original network and VGG-16 on the MPIIGaze dataset. The results show that the improved SLeNet network model performs better on the MPIIGaze dataset than LeNet and VGG-16 and has fewer parameters.","2689-6621","978-1-7281-8028-1","10.1109/IAEAC50856.2021.9390807","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9390807","neural network;gaze estimation;depth separable convolution;head pose","Convolution;Computational modeling;Splicing;Neural networks;Estimation;Mouth;Information technology","convolutional neural nets;deep learning (artificial intelligence);eye;feature extraction;gaze tracking;learning (artificial intelligence);pose estimation","appearance-based gaze estimation;separable convolution neural networks;gaze estimation neural network;LeNet neural network;deep separable convolution;Xception network;head posture features;eye image;mouth corner information;head posture;MPIIGaze dataset;improved SLeNet network model","","","","10","","5 Apr 2021","","","IEEE","IEEE Conferences"
"Detecting Expressions with Multimodal Transformers","S. Parthasarathy; S. Sundaram",Amazon; Amazon,"2021 IEEE Spoken Language Technology Workshop (SLT)","25 Mar 2021","2021","","","636","643","Developing machine learning algorithms to understand person-to-person engagement can result in natural user experiences for communal devices such as Amazon Alexa. Among other cues such as voice activity and gaze, a person's audio-visual expression that includes tone of the voice and facial expression serves as an implicit signal of engagement between parties in a dialog. This study investigates deep-learning algorithms for audio-visual detection of user's expression. We first implement an audio-visual baseline model with recurrent layers that shows competitive results compared to current state of the art. Next, we propose the transformer architecture with encoder layers that better integrate audio-visual features for expressions tracking. Performance on the Aff-Wild2 database shows that the proposed methods perform better than baseline architecture with recurrent layers with absolute gains approximately 2% for arousal and valence descriptors. Further, multimodal architectures show significant improvements over models trained on single modalities with gains of up to 3.6%. Ablation studies show the significance of the visual modality for the expression detection on the Aff-Wild2 database.","","978-1-7281-7066-4","10.1109/SLT48900.2021.9383573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383573","expression detection;human-computer interaction;computational paralinguistics","Training;Visualization;Analytical models;Virtual assistants;Data models;Visual databases;Task analysis","audio signal processing;audio-visual systems;face recognition;learning (artificial intelligence);speech recognition","voice activity;facial expression;deep-learning algorithms;audio-visual detection;audio-visual baseline model;recurrent layers;transformer architecture;encoder layers;audio-visual features;expressions tracking;Aff-Wild2 database;baseline architecture;absolute gains approximately 2;multimodal architectures;visual modality;expression detection;detecting expressions;multimodal transformers;person-to-person engagement;natural user;communal devices;Amazon Alexa","","","","50","","25 Mar 2021","","","IEEE","IEEE Conferences"
"Analysis of Multimodal Features for Speaking Proficiency Scoring in an Interview Dialogue","M. Saeki; Y. Matsuyama; S. Kobashikawa; T. Ogawa; T. Kobayashi","Waseda University,Department of Communications and Computer Engineering,Japan; Waseda University,Department of Communications and Computer Engineering,Japan; NTT Corporation,NTT Media Intelligence Laboratories,Japan; Waseda University,Department of Communications and Computer Engineering,Japan; Waseda University,Department of Communications and Computer Engineering,Japan","2021 IEEE Spoken Language Technology Workshop (SLT)","25 Mar 2021","2021","","","629","635","This paper analyzes the effectiveness of different modalities in automated speaking proficiency scoring in an online dialogue task of non-native speakers. Conversational competence of a language learner can be assessed through the use of multimodal behaviors such as speech content, prosody, and visual cues. Although lexical and acoustic features have been widely studied, there has been no study on the usage of visual features, such as facial expressions and eye gaze. To build an automated speaking proficiency scoring system using multi-modal features, we first constructed an online video interview dataset of 210 Japanese English-learners with annotations of their speaking proficiency. We then examined two approaches for incorporating visual features and compared the effectiveness of each modality. Results show the end-to-end approach with deep neural networks achieves a higher correlation with human scoring than one with handcrafted features. Modalities are effective in the order of lexical, acoustic, and visual features.","","978-1-7281-7066-4","10.1109/SLT48900.2021.9383590","Waseda University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9383590","Speaking proficiency assessment;multi-modal machine learning;BERT (Bidirectional Encoder Representations from Transformers)","Visualization;Correlation;Neural networks;Acoustics;Reliability;Interviews;Task analysis","deep learning (artificial intelligence);feature extraction;natural language processing;neural nets;speech recognition","multimodal features;interview dialogue;automated speaking proficiency scoring;online dialogue task;nonnative speakers;conversational competence;language learner;multimodal behaviors;speech content;visual cues;lexical features;acoustic features;facial expressions;eye gaze;online video interview dataset;visual features;end-to-end approach;human scoring;handcrafted features;Japanese English-learners","","","","22","","25 Mar 2021","","","IEEE","IEEE Conferences"
"FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments","Z. Hu; A. Bulling; S. Li; G. Wang","Peking University, China; University of Stuttgart, Germany; Peking University, China; Peking University, China","IEEE Transactions on Visualization and Computer Graphics","15 Apr 2021","2021","27","5","2681","2690","Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments. Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users' eye fixations and other factors, i.e. users' historical gaze positions, task-related objects, saliency information of the VR content, and users' head rotation velocities. Based on this analysis, we propose FixationNet - a novel learning-based model to forecast users' eye fixations in the near future in VR. We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93° to 2.35°) in free-viewing and of 15.1% (from 2.05° to 1.74°) in task-oriented situations. As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.","1941-0506","","10.1109/TVCG.2021.3067779","National Key R&D Program of China(grant numbers:2017YFB0203002,2017YFB1002700); National Natural Science Foundation of China(grant numbers:61632003); European Research Council(grant numbers:801708); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382883","Fixation forecasting;task-oriented attention;visual search;convolutional neural network;deep learning;virtual reality","Visualization;Task analysis;Solid modeling;Predictive models;Virtual environments;Computational modeling;Two dimensional displays","gaze tracking;human computer interaction;learning (artificial intelligence);rendering (computer graphics);virtual reality","task-oriented situations;task-oriented settings;novel learning-based model;VR content;task-related objects;immersive VR environments;visual search task;eye tracking data;free-viewing conditions;gaze-based interaction;gaze-contingent rendering;content design;immersive virtual reality;human visual attention;task-oriented virtual environments;forecasting eye fixations;FixationNet;VR research;guides future work;task-oriented attention","","1","","56","IEEE","22 Mar 2021","","","IEEE","IEEE Journals"
"Vision-based Gaze Estimation: A Review","X. Wang; J. Zhang; H. Zhang; S. Zhao; H. Liu","school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China. (e-mail: 57652761@qq.com); School of Computer Science and Engineering, Tianjin University of Technology, Tianjin 300384, China.; school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China.; School of Computing, University of Portsmouth, Portsmouth PO13HE, U.K.; school of Mechanical and Automation, State Key Lab of Robotics and Systems, Harbin Institute of Technology (Shenzhen), Shenzhen 518055, China.","IEEE Transactions on Cognitive and Developmental Systems","","2021","PP","99","1","1","Eye gaze is an important natural behavior in social interaction as it delivers complex exchanges between observer and observed, by building up the geometric constraints and relation of the exchanges. These inter-person exchanges can be modeled based on gaze direction estimated using computer vision. Despite significant progresses in vision-based gaze estimation in last 10 years, it is still nontrivial since the accuracy of gaze estimation is significantly affected by such intrinsic factors as head pose variance, individual bias between optical axis and visual axis, eye blink, occlusion and image blur, degrade gaze features, lead to inaccurate gaze-involved human social interaction analysis. This paper aims to review and discuss existing methods addressing above-mentioned problems, gaze involved applications and datasets against the state-of-the-arts in vision-based gaze estimation. It also points out future research directions and challenges of gaze estimation in terms of meta learning, causal inference, disentangled representation, and social gaze behaviour for unconstrained gaze estimation.","2379-8939","","10.1109/TCDS.2021.3066465","National Natural Science Foundation of China(grant numbers:61733011,62073279); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380459","3D Gaze Estimation;Head pose;Optical axis;Visual axis;Computer Vision.","Estimation;Feature extraction;Faces;Three-dimensional displays;Solid modeling;Iris;Visualization","","","","","","","IEEE","17 Mar 2021","","","IEEE","IEEE Early Access Articles"
"Robust 3-D Gaze Estimation via Data Optimization and Saliency Aggregation for Mobile Eye-Tracking Systems","M. Liu; Y. Li; H. Liu","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong","IEEE Transactions on Instrumentation and Measurement","24 Mar 2021","2021","70","","1","10","In order to precisely predict 3-D gaze points, calibration is needed for each subject prior to first use the mobile gaze tracking system. However, traditional calibration methods normally expect the user to stare at predefined targets in the scene, which is troublesome and time-consuming. In this study, we proposed a novel method to remove the explicit user calibration and achieve robust 3-D gaze estimation in the room-scale area. Our proposed framework treats salient regions in the scene as possible 3-D locations of gaze points. To improve the efficiency of predicting 3-D gaze from visual saliency, the bag-of-word algorithm is adopted for eliminating redundant scene image data based on their similarities. After the elimination, saliency maps are generated from those scene images, and the geometrical relationship among the scene and eye cameras is obtained through aggregating 3-D salient targets with eye visual directions. Finally, we calculate the 3-D point of regard (PoR) by utilizing 3-D structures of the scene. The experimental results indicate that our method enhances the reliability of saliency maps and achieves promising performances on 3-D gaze estimation with different subjects.","1557-9662","","10.1109/TIM.2021.3065437","National Natural Science Foundation of China(grant numbers:61873220,62011530436,61875068); Research Grants Council of Hong Kong(grant numbers:CityU 11203619); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380691","3-D gaze;mobile gaze tracking system;room-scale area;visual saliency","Three-dimensional displays;Calibration;Estimation;Cameras;Visualization;Pupils;Solid modeling","calibration;cameras;eye;feature extraction;gaze tracking;image sensors;object detection","scene images;saliency maps;redundant scene image data;visual saliency;explicit user calibration;traditional calibration methods;mobile gaze tracking system;gaze points;mobile eye-tracking systems;saliency aggregation;robust 3-D gaze estimation","","","","36","IEEE","17 Mar 2021","","","IEEE","IEEE Journals"
"MERASTC: Micro-expression Recognition using Effective Feature Encodings and 2D Convolutional Neural network","P. Gupta","Computer Science and Engineering, IIT Indore, 226957 Indore, Madhya Pradesh, India, 452017 (e-mail: puneet@iiti.ac.in)","IEEE Transactions on Affective Computing","","2021","PP","99","1","1","Facial micro-expression (ME) can disclose genuine and concealed human feelings. It makes MEs extensively useful in real-world applications pertaining to affective computing and psychology. Unfortunately, they are induced by subtle facial movements for a short duration of time, which makes the ME recognition, a highly challenging problem even for human beings. In automatic ME recognition, the well-known features encode either incomplete or redundant information, and there is a lack of sufficient training data. The proposed method, Micro-Expression Recognition by Analysing Spatial and Temporal Characteristics, MERASTC mitigates these issues for improving the ME recognition. It compactly encodes the subtle deformations using action units (AUs), landmarks, gaze, and appearance features of all the video frames while preserving most of the relevant ME information. Furthermore, it improves the efficacy by introducing a novel neutral face normalization for ME and initiating the utilization of gaze features in deep learning based ME recognition. The features are provided to the 2D convolutional neural network that jointly analyses the spatial and temporal behavior for correct ME classification. Experimental results on the publicly available datasets indicate that the proposed method exhibits better performance than the well-known methods.","1949-3045","","10.1109/TAFFC.2021.3061967","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363624","Micro-expression Recognition;Action units;Gaze feature;Deep Learning;Spatiotemporal CNN","Face recognition;Feature extraction;Encoding;Deep learning;Optical network units;Training data;Strain","","","","","","","IEEE","25 Feb 2021","","","IEEE","IEEE Early Access Articles"
"Gaze-Based Dual Resolution Deep Imitation Learning for High-Precision Dexterous Robot Manipulation","H. Kim; Y. Ohmura; Y. Kuniyoshi","Laboratory for Intelligent Systems and Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Laboratory for Intelligent Systems and Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan; Laboratory for Intelligent Systems and Informatics, Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan","IEEE Robotics and Automation Letters","8 Mar 2021","2021","6","2","1630","1637","A high-precision manipulation task, such as needle threading, is challenging. Physiological studies have proposed connecting low-resolution peripheral vision and fast movement to transport the hand into the vicinity of an object, and using high-resolution foveated vision to achieve the accurate homing of the hand to the object. The results of this study demonstrate that a deep imitation learning based method, inspired by the gaze-based dual resolution visuomotor control system in humans, can solve the needle threading task. First, we recorded the gaze movements of a human operator who was teleoperating a robot. Then, we used only a high-resolution image around the gaze to precisely control the thread position when it was close to the target. We used a low-resolution peripheral image to reach the vicinity of the target. The experimental results obtained in this study demonstrate that the proposed method enables precise manipulation tasks using a general-purpose robot manipulator and improves computational efficiency.","2377-3766","","10.1109/LRA.2021.3059619","Grant-in-Aid for Scientific Research (A)(grant numbers:JP18H04108); New Energy and Industrial Technology Development Organization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354902","Imitation learning;deep learning in grasping and manipulation;bioinspired robot learning;telerobotics and teleoperation;failure detection and recovery","Robots;Needles;Task analysis;Yarn;Visualization;Robot kinematics;Cameras","deep learning (artificial intelligence);dexterous manipulators;gaze tracking;image resolution;position control;robot vision;telerobotics","deep imitation learning based method;gaze-based dual resolution visuomotor control system;needle threading task;gaze movements;high-resolution image;low-resolution peripheral image;general-purpose robot manipulator;gaze-based dual resolution deep imitation learning;high-precision dexterous robot manipulation;thread position control;teleoperation;high-resolution foveated vision;fast movement;low-resolution peripheral vision;physiological studies;high-precision manipulation task","","","","37","CCBYNCND","16 Feb 2021","","","IEEE","IEEE Journals"
"Massive-Scale Aerial Photo Categorization by Cross-Resolution Visual Perception Enhancement","L. Zhang; X. Zhang; M. Xu; L. Shao","College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China.; College of Computer Science and Artificial Intelligence, Wenzhou University, Wenzhou 325035, China (e-mail: zhangxiaoqinnan@gmail.com); Center for Interdisciplinary Information Science Research, Zhengzhou University, Zhengzhou 450000, China.; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE.","IEEE Transactions on Neural Networks and Learning Systems","","2021","PP","99","1","14","Categorizing aerial photographs with varied weather/lighting conditions and sophisticated geomorphic factors is a key module in autonomous navigation, environmental evaluation, and so on. Previous image recognizers cannot fulfill this task due to three challenges: 1) localizing visually/semantically salient regions within each aerial photograph in a weakly annotated context due to the unaffordable human resources required for pixel-level annotation; 2) aerial photographs are generally with multiple informative attributes (e.g., clarity and reflectivity), and we have to encode them for better aerial photograph modeling; and 3) designing a cross-domain knowledge transferal module to enhance aerial photograph perception since multiresolution aerial photographs are taken asynchronistically and are mutually complementary. To handle the above problems, we propose to optimize aerial photograph's feature learning by leveraging the low-resolution spatial composition to enhance the deep learning of perceptual features with a high resolution. More specifically, we first extract many BING-based object patches (Cheng et al., 2014) from each aerial photograph. A weakly supervised ranking algorithm selects a few semantically salient ones by seamlessly incorporating multiple aerial photograph attributes. Toward an interpretable aerial photograph recognizer indicative to human visual perception, we construct a gaze shifting path (GSP) by linking the top-ranking object patches and, subsequently, derive the deep GSP feature. Finally, a cross-domain multilabel SVM is formulated to categorize each aerial photograph. It leverages the global feature from low-resolution counterparts to optimize the deep GSP feature from a high-resolution aerial photograph. Comparative results on our compiled million-scale aerial photograph set have demonstrated the competitiveness of our approach. Besides, the eye-tracking experiment has shown that our ranking-based GSPs are over 92% consistent with the real human gaze shifting sequences.","2162-2388","","10.1109/TNNLS.2021.3055548","National Natural Science Foundation of China(grant numbers:61922064,U2033210); Zhejiang Provincial Natural Science Foundation(grant numbers:LR17F030001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354503","Aerial photograph;cross domain;machine learning;perception enhancement;ranking;transfer learning","Visualization;Semantics;Training;Support vector machines;Kernel;Image recognition;Visual perception","","","","","","","IEEE","15 Feb 2021","","","IEEE","IEEE Early Access Articles"
"HammerDrive: A Task-Aware Driving Visual Attention Model","P. V. Amadori; T. Fischer; Y. Demiris","Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K. (e-mail: p.amadori@imperial.ac.uk); Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K..; Personal Robotics Laboratory, Department of Electrical and Electronic Engineering, Imperial College London, London SW7 2BU, U.K..","IEEE Transactions on Intelligent Transportation Systems","","2021","PP","99","1","13","We introduce HammerDrive, a novel architecture for task-aware visual attention prediction in driving. The proposed architecture is learnable from data and can reliably infer the current focus of attention of the driver in real-time, while only requiring limited and easy-to-access telemetry data from the vehicle. We build the proposed architecture on two core concepts: 1) driving can be modeled as a collection of sub-tasks (maneuvers), and 2) each sub-task affects the way a driver allocates visual attention resources, i.e., their eye gaze fixation. HammerDrive comprises two networks: a hierarchical monitoring network of forward-inverse model pairs for sub-task recognition and an ensemble network of task-dependent convolutional neural network modules for visual attention modeling. We assess the ability of HammerDrive to infer driver visual attention on data we collected from 20 experienced drivers in a virtual reality-based driving simulator experiment. We evaluate the accuracy of our monitoring network for sub-task recognition and show that it is an effective and light-weight network for reliable real-time tracking of driving maneuvers with above 90% accuracy. Our results show that HammerDrive outperforms a comparable state-of-the-art deep learning model for visual attention prediction on numerous metrics with ~13% improvement for both Kullback-Leibler divergence and similarity, and demonstrate that task-awareness is beneficial for driver visual attention prediction.","1558-0016","","10.1109/TITS.2021.3055120","U.K. Defense Science and Technology Laboratory DSTL Engineering and Physical Sciences Research Council EPSRC(grant numbers:EP/P008461/1); Royal Academy of Engineering Chair in Emerging Technologies to Yiannis Demiris; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351808","Advanced driver-assistance systems;visual attention;task recognition;simulated driving;HAMMER.","Visualization;Vehicles;Task analysis;Predictive models;Computational modeling;Real-time systems;Computer architecture","","","","","","","","9 Feb 2021","","","IEEE","IEEE Early Access Articles"
"In the Eye of the Beholder: Gaze and Actions in First Person Video","Y. Li; M. Liu; J. Rehg","Biostatistics / Computer Sciences, University of Wisconsin-Madison, 5228 Madison, Wisconsin, United States, 53706 (e-mail: yin.li@wisc.edu); IRIM, Georgia Institute of Technology, 1372 Atlanta, Georgia, United States, 30332-0002 (e-mail: mliu328@gatech.edu); School of Interactive Computing, Georgia Institute of Technology, 1372 Atlanta, Georgia, United States, (e-mail: rehg@gatech.edu)","IEEE Transactions on Pattern Analysis and Machine Intelligence","","2021","PP","99","1","1","We address the task of jointly determining what a person is doing and where they are looking based on the analysis of video captured by a headworn camera. To facilitate our research, we first introduce the EGTEA Gaze+ dataset. Our dataset comes with videos, gaze tracking data, hand masks and action annotations, thereby providing the most comprehensive benchmark for First Person Vision (FPV). Moving beyond the dataset, we propose a novel deep model for joint gaze estimation and action recognition in FPV. Our method describes the participant's gaze as a probabilistic variable and models its distribution using stochastic units in a deep network. We further sample from these stochastic units, generating an attention map to guide the aggregation of visual features for action recognition. Our method is evaluated on our EGTEA Gaze+ dataset and achieves a performance level that exceeds the state-of-the-art by a significant margin. More importantly, we demonstrate that our model can be applied to larger scale FPV dataset---EPIC-Kitchens even without using gaze, offering new state-of-the-art results on FPV action recognition.","1939-3539","","10.1109/TPAMI.2021.3051319","National Institute of Biomedical Imaging and Bioengineering(grant numbers:54EB020404); Intel Corporation(grant numbers:Gift from ISTC-PC); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325929","Action recognition;deep probabilistic models;first person vision;gaze estimation;video analysis","Three-dimensional displays;Convolution;Visualization;Cameras;Benchmark testing;Stochastic processes;Gaze tracking","","","","","","","IEEE","15 Jan 2021","","","IEEE","IEEE Early Access Articles"
"Head2Head++: Deep Facial Attributes Re-Targeting","M. C. Doukas; M. R. Koujan; V. Sharmanska; A. Roussos; S. Zafeiriou","Department of Computing, Imperial College London, London, U.K.; College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.; Department of Computing, Imperial College London, London, U.K.; College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.; Department of Computing, Imperial College London, London, U.K.","IEEE Transactions on Biometrics, Behavior, and Identity Science","11 Feb 2021","2021","3","1","31","43","Facial video re-targeting is a challenging problem aiming to modify the facial attributes of a target subject in a seamless manner by a driving monocular sequence. We leverage the 3D geometry of faces and Generative Adversarial Networks (GANs) to design a novel deep learning architecture for the task of facial and head reenactment. Our method is different to purely 3D model-based approaches, or recent image-based methods that use Deep Convolutional Neural Networks (DCNNs) to generate individual frames. We manage to capture the complex non-rigid facial motion from the driving monocular performances and synthesise temporally consistent videos, with the aid of a sequential Generator and an ad-hoc Dynamics Discriminator network. We conduct a comprehensive set of quantitative and qualitative tests and demonstrate experimentally that our proposed method can successfully transfer facial expressions, head pose and eye gaze from a source video to a target subject, in a photo-realistic and faithful fashion, better than other state-of-the-art methods. Most importantly, our system performs end-to-end reenactment in nearly real-time speed (18 fps).","2637-6407","","10.1109/TBIOM.2021.3049576","Engineering and Physical Sciences Research Council(grant numbers:(EP/S010203/1)); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9319722","Face reenactment;full head reenactment;neural rendering;video renderer;3DMM;3-D reconstruction;gaze tracking;temporal discriminator;facial flow","Three-dimensional displays;Faces;Solid modeling;Shape;Rendering (computer graphics);Geometry;Task analysis","convolutional neural nets;deep learning (artificial intelligence);face recognition;image sequences;stereo image processing;video signal processing","facial video re-targeting;target subject;driving monocular sequence;generative adversarial networks;deep learning;model-based approaches;image-based methods;deep convolutional neural networks;nonrigid facial motion;synthesise temporally consistent videos;sequential Generator;facial expressions;source video;end-to-end reenactment;Head2Head++;deep facial attributes retargeting;3D geometry;GANs;ad-hoc dynamics discriminator network;eye gaze;head pose","","","","58","IEEE","11 Jan 2021","","","IEEE","IEEE Journals"
"Vision Based Detection of Driver Cell Phone Usage and Food Consumption","B. Wagner; F. Taffner; S. Karaca; L. Karge","ARRK Engineering, 80807 Munich, Germany (e-mail: benjamin.wagner@arrk-engineering.com); ARRK Engineering, 80807 Munich, Germany.; ARRK Engineering, 80807 Munich, Germany.; ARRK Engineering, 80807 Munich, Germany.","IEEE Transactions on Intelligent Transportation Systems","","2021","PP","99","1","10","Distracted driving is a problem which yearly causes a large amount of road traffic crashes with high rates of fatalities and injured persons. Recently, car manufacturers started to integrate driver monitoring systems to detect visual distraction. This paper proposes a method to extend such systems by driver posture classification to detect driver cell phone usage and food consumption. Such an extension can be beneficial since systems that focus on the detection of visual distraction mainly rely on head pose and gaze information. Thus, distraction caused by cell phone usage or food consumption can not be detected by these systems when the driver is looking to the road ahead. To robustly detect those types of manual and cognitive distraction, different deep learning models were trained and evaluated based on a new image dataset which was captured by two infrared cameras to ensure that a large range of head angles can be covered by the system. Separate Convolutional Neural Networks (CNNs) were trained and evaluated for the dataset of the left and the right camera to optimize the classification accuracy. The trained CNNs revealed a competitive test accuracy of 92.88% and 90.36% for the left and the right camera, respectively. In inference mode, the models achieve a frame rate of 44Hz and 28Hz for the left and the right camera, respectively. The combination of the classification output of both networks revealed a test accuracy of 92.54%.","1558-0016","","10.1109/TITS.2020.3043145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9318557","Driver monitoring;deep learning;driver posture classification.","Vehicles;Cameras;Cellular phones;Head;Roads;Visualization;Monitoring","","","","","","","IEEE","8 Jan 2021","","","IEEE","IEEE Early Access Articles"
"3D Model-Based Gaze Tracking Via Iris Features With a Single Camera and a Single Light Source","J. Liu; J. Chi; W. Hu; Z. Wang","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Transactions on Human-Machine Systems","15 Mar 2021","2021","51","2","75","86","Traditional 3D gaze estimation methods are usually based on the models of pupil refraction and corneal reflection. These methods typically rely on multiple light sources. The 3D gaze can be estimated using single-camera-single-light-source systems only when certain user-dependent eye parameters are available a priori, which is rarely the case. This article proposes a 3D gaze estimation method which works based on iris features using a single camera and a single light source. User-dependent eye parameters involving the iris radius and the cornea radius are user-calibrated. The 3D line-of-sight is estimated from the optical axis and the positional relationship between the optical axis and the visual axis, and then optimized using a binocular stereo vision model. The feasibility and robustness of the proposed method are assessed by simulations and practical experiments. The system configuration required by the method is simpler than that required by the state-of-the-art methods, which shows significant potential value, especially with regard to mobile device applications.","2168-2305","","10.1109/THMS.2020.3035176","National Key Research and Development Program of China(grant numbers:2017YFB1002804); Natural Science Foundation of Beijing Municipality(grant numbers:4172040); Fundamental Research Funds for the Central Universities(grant numbers:FRF-GF-20-04 A); Scientific and Technological Innovation Foundation; Ministry of Transport of the People's Republic of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9270574","3D gaze estimation;iris radius;kappa angle;single-camera-single-light-source","Three-dimensional displays;Cornea;Pupils;Iris;Estimation;Cameras;Solid modeling","calibration;cameras;eye;feature extraction;gaze tracking;human computer interaction;light sources;stereo image processing","single light source;traditional 3D gaze estimation methods;multiple light sources;single-camera-single-light-source systems;user-dependent eye parameters;3D gaze estimation method;iris features;single camera;iris radius;user-calibrated;3D line-of-sight;optical axis;binocular stereo vision model;3D model-based gaze tracking","","","","46","IEEE","25 Nov 2020","","","IEEE","IEEE Journals"
"Understanding More About Human and Machine Attention in Deep Neural Networks","Q. Lai; S. Khan; Y. Nie; H. Sun; J. Shen; L. Shao","Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Mohamed bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; University of Electronic Science and Technology of China, Chengdu, China; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; Inception Institute of Artificial Intelligence, Abu Dhabi, UAE","IEEE Transactions on Multimedia","25 Jun 2021","2021","23","","2086","2099","Human visual system can selectively attend to parts of a scene for quick perception, a biological mechanism known as Human attention. Inspired by this, recent deep learning models encode attention mechanisms to focus on the most task-relevant parts of the input signal for further processing, which is called Machine/Neural/Artificial attention. Understanding the relation between human and machine attention is important for interpreting and designing neural networks. Many works claim that the attention mechanism offers an extra dimension of interpretability by explaining where the neural networks look. However, recent studies demonstrate that artificial attention maps do not always coincide with common intuition. In view of these conflicting evidence, here we make a systematic study on using artificial attention and human attention in neural network design. With three example computer vision tasks (i.e., salient object segmentation, video action recognition, and fine-grained image classification), diverse representative backbones (i.e., AlexNet, VGGNet, ResNet) and famous architectures (i.e., Two-stream, FCN), corresponding real human gaze data, and systematically conducted large-scale quantitative studies, we quantify the consistency between artificial attention and human visual attention and offer novel insights into existing artificial attention mechanisms by giving preliminary answers to several key questions related to human and artificial attention mechanisms. Overall results demonstrate that human attention can benchmark the meaningful `ground-truth' in attention-driven tasks, where the more the artificial attention is close to human attention, the better the performance; for higher-level vision tasks, it is case-by-case. It would be advisable for attention-driven tasks to explicitly force a better alignment between artificial and human attention to boost the performance; such alignment would also improve the network explainability for higher-level computer vision tasks.","1941-0077","","10.1109/TMM.2020.3007321","Natural Science Foundation of Guangdong Province(grant numbers:2019A1515010860); Fundamental Research Funds for the Central Universities(grant numbers:D2190670); Science and Technology Project of Guangzhou City(grant numbers:201707010140); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9133499","Attention mechanism;human attention;artificial attention;deep learning","Task analysis;Visualization;Computer vision;Neural networks;Object segmentation;Image recognition;Reliability","computer vision;image classification;image recognition;image segmentation;learning (artificial intelligence);neural nets;video signal processing","deep Neural networks;Human visual system;human attention;recent deep learning models encode attention mechanisms;human machine attention;interpreting;designing neural networks;attention mechanism;artificial attention maps;neural network design;human visual attention;offer novel insights;artificial attention mechanisms;attention-driven tasks","","1","","91","IEEE","6 Jul 2020","","","IEEE","IEEE Journals"
"Using Eye Gaze to Enhance Generalization of Imitation Networks to Unseen Environments","C. Liu; Y. Chen; M. Liu; B. E. Shi","Department of Electrical and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Electrical and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Electrical and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong; Department of Electrical and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong","IEEE Transactions on Neural Networks and Learning Systems","3 May 2021","2021","32","5","2066","2074","Vision-based autonomous driving through imitation learning mimics the behavior of human drivers by mapping driver view images to driving actions. This article shows that performance can be enhanced via the use of eye gaze. Previous research has shown that observing an expert's gaze patterns can be beneficial for novice human learners. We show here that neural networks can also benefit. We trained a conditional generative adversarial network to estimate human gaze maps accurately from driver-view images. We describe two approaches to integrating gaze information into imitation networks: eye gaze as an additional input and gaze modulated dropout. Both significantly enhance generalization to unseen environments in comparison with a baseline vanilla network without gaze, but gaze-modulated dropout performs better. We evaluated performance quantitatively on both single images and in closed-loop tests, showing that gaze modulated dropout yields the lowest prediction error, the highest success rate in overtaking cars, the longest distance between infractions, lowest epistemic uncertainty, and improved data efficiency. Using Grad-CAM, we show that gaze modulated dropout enables the network to concentrate on task-relevant areas of the image.","2162-2388","","10.1109/TNNLS.2020.2996386","National Natural Science Foundation of China(grant numbers:U1713211); Shenzhen Science, Technology and Innovation Commission(grant numbers:JCYJ20160428154842603); Research Grant Council of Hong Kong SAR Government, China(grant numbers:11210017,21202816,16211015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9104858","Autonomous driving;eye gaze;generalization;human attention;imitation learning (IL)","Uncertainty;Autonomous vehicles;Task analysis;Automobiles;Visualization;Neural networks","automobiles;collision avoidance;control engineering computing;deep learning (artificial intelligence);eye;gaze tracking;mobile robots;robot vision;traffic engineering computing","eye gaze modulated dropout;image task-relevant areas;Grad-CAM;data efficiency;epistemic uncertainty;overtaking cars;closed-loop test;human gaze map estimation;expert gaze pattern;driving actions;driver-view image mapping;vision-based autonomous driving;imitation network generalization enhancement;gaze information integration;conditional generative adversarial network;neural networks;human learners;human driver behavior;imitation learning;baseline vanilla network;unseen environments","","2","","28","IEEE","1 Jun 2020","","","IEEE","IEEE Journals"
"Deep Convolutional Neural Networks and Transfer Learning for Measuring Cognitive Impairment Using Eye-Tracking in a Distributed Tablet-Based Environment","R. U. Haque; A. L. Pongos; C. M. Manzanares; J. J. Lah; A. I. Levey; G. D. Clifford","Department of NeurologyEmory School of Medicine; Department of NeurologyEmory School of Medicine; Department of NeurologyEmory School of Medicine; Department of NeurologyEmory School of Medicine; Department of NeurologyEmory School of Medicine; Department of Biomedical Informatics, Emory School of Medicine, Atlanta, GA, USA","IEEE Transactions on Biomedical Engineering","21 Dec 2020","2021","68","1","11","18","Objective: Alzheimer's disease (AD) is a neurodegenerative disorder that initially presents with memory loss in the presence of underlying neurofibrillary tangle and amyloid plaque pathology. Mild cognitive impairment is the initial symptomatic stage, which is an early window for detecting cognitive impairment prior to progressive decline and dementia. We recently developed the Visuospatial Memory Eye-Tracking Test (VisMET), a passive task capable of classifying cognitive impairment in AD in under five minutes. Here we describe the development of a mobile version of VisMET to enable efficient and widespread administration of the task. Methods: We delivered VisMET on iPad devices and used a transfer learning approach to train a deep neural network to track eye gaze. Eye movements were used to extract memory features to assess cognitive status in a population of 250 individuals. Results: Mild to severe cognitive impairment was identifiable with a test accuracy of 70%. By enforcing a minimal eye tracking calibration error of 2 cm, we achieved an accuracy of 76% which is equivalent to the accuracy obtained using commercial hardware for eye-tracking. Conclusion: This work demonstrates a mobile version of VisMET capable of estimating the presence of cognitive impairment. Significance: Given the ubiquity of tablet devices, our approach has the potential to scale globally.","1558-2531","","10.1109/TBME.2020.2990734","Goizueta Foundation; Goizueta Alzheimer Disease Research Center at Emory University(grant numbers:P50 AG025688); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079571","Alzheimer's disease;convolutional neural networks;eye tracking;memory;mild cognitive Impairment","Task analysis;Tablet computers;Calibration;Agriculture;Feature extraction;Estimation;Convolutional neural networks","brain;calibration;cognition;deep learning (artificial intelligence);diseases;neurophysiology;notebook computers","VisMET;transfer learning;eye gaze;eye movements;cognitive status;severe cognitive impairment;minimal eye tracking calibration error;deep convolutional neural networks;cognitive impairment;distributed tablet-based environment;memory loss;amyloid plaque pathology;mild cognitive impairment;initial symptomatic stage;visuospatial memory eye-tracking test;size 2.0 cm","Alzheimer Disease;Cognitive Dysfunction;Eye-Tracking Technology;Humans;Machine Learning;Neural Networks, Computer","3","","18","IEEE","27 Apr 2020","","","IEEE","IEEE Journals"
"GazMon: Eye Gazing Enabled Driving Behavior Monitoring and Prediction","X. Fan; F. Wang; D. Song; Y. Lu; J. Liu","College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; Department of Computer and Information Science, University of Mississippi, University, MS, USA; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; School of Computing Science, Simon Fraser University, Burnaby, BC, Canada; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China","IEEE Transactions on Mobile Computing","4 Mar 2021","2021","20","4","1420","1433","Automobiles have become one of the necessities of modern life, but also introduced numerous traffic accidents that threaten drivers and other road users. Most state-of-the-art safety systems are passively triggered, reacting to dangerous road conditions or driving maneuvers only after they happen and are observed, which greatly limits the last chances for collision avoidances. Timely tracking and predicting the driving maneuvers calls for a more direct interface beyond the traditional steering wheel/brake/gas pedal. In this paper, we argue that a driver's eyes are the interface, as it is the first and the essential window that gathers external information during driving. Our experiments suggest that a driver's gaze patterns appear prior to and correlate with the driving maneuvers for driving maneuver prediction. We accordingly present GazMon, an active driving maneuver monitoring and prediction framework for driving assistance applications. GazMon extracts the gaze information through a front-camera and analyzes the facial features, including facial landmarks, head pose, and iris centers, through a carefully constructed deep learning architecture. Both our on-road experiments and driving simulator based evaluations demonstrate the superiority of our GazMon on predicting driving maneuvers as well as other distracted behaviors. It is readily deployable using RGB cameras and allows reuse of existing smartphones towards more safely driving.","1558-0660","","10.1109/TMC.2019.2962764","NSERC Postdoctoral Fellowship; NSERC Discovery Grant; National Natural Science Foundation of China(grant numbers:61602214); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944165","Gaze;driving assistant;mobile computing;deep learning","Vehicles;Wheels;Monitoring;Smart phones;Accidents;Roads;Safety","automobiles;cameras;collision avoidance;deep learning (artificial intelligence);driver information systems;face recognition;gaze tracking;image colour analysis;iris recognition;road safety;traffic engineering computing;user interfaces","deep learning architecture;smartphones;RGB cameras;automobiles;iris centers;head pose;facial landmarks;front-camera;gaze information;driver gaze patterns;driving simulator based evaluations;driving assistance applications;active driving maneuver monitoring;driving maneuver prediction;dangerous road conditions;safety systems;driving behavior monitoring;eye gazing;GazMon","","","","31","IEEE","27 Dec 2019","","","IEEE","IEEE Journals"
"Screening Early Children With Autism Spectrum Disorder via Response-to-Name Protocol","Z. Wang; J. Liu; K. He; Q. Xu; X. Xu; H. Liu","State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Child Health Care, Children's Hospital of Fudan University, Shanghai, China; Department of Child Health Care, Children's Hospital of Fudan University, Shanghai, China; State Key Laboratory of Mechanical System and Vibration, School of Mechanical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Industrial Informatics","29 Oct 2020","2021","17","1","587","595","Incidence of children with autism spectrum disorder (ASD) has increased with an average rate of 1% worldwide. Clinical ASD screening, especially for children screening is a laborious and skilled task; however, there is no objective and effective method automating ASD children screening. Analyzing children ASD characteristics in predefined motion behavior protocols is attempted to provide automatic solutions to children ASD screening. A novel protocol, response to name (RTN), is proposed in this article for ASD clinical validation and diagnosis. The RTN method is jointly designed with clinical partners, and novel gaze estimation is developed for validating ASD characteristic behavior. Seventeen subjects including ten adults and seven children (five ASD subjects and two healthy subjects) have participated the experiment. The experiment results show that the proposed RTN system achieves an average classification score of 92.7% fully demonstrating that the principle of motion protocol based ASD screening has the potential to have early ASD screening automated.","1941-0050","","10.1109/TII.2019.2958106","National Natural Science Foundation of China(grant numbers:61733011,51575338); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8928567","Autism spectrum disorder (ASD);early screening;gaze estimation;multisensor fusion;response to name (RTN)","Autism;Protocols;Estimation;Computer vision;Pediatrics;Head;Cameras","gaze tracking;medical disorders;medical image processing;neurophysiology;paediatrics","autism spectrum disorder;response-to-name protocol;clinical ASD screening;children screening;predefined motion behavior protocols;RTN method;clinical partners;ASD subjects;ASD characteristic behavior;children ASD characteristics;ASD clinical validation;ASD clinical diagnosis;gaze estimation","","1","","29","IEEE","8 Dec 2019","","","IEEE","IEEE Journals"
"A Differential Approach for Gaze Estimation","G. Liu; Y. Yu; K. A. F. Mora; J. -M. Odobez","Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland; Eyeware Tech SA, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland","IEEE Transactions on Pattern Analysis and Machine Intelligence","4 Feb 2021","2021","43","3","1092","1099","Most non-invasive gaze estimation methods regress gaze directions directly from a single face or eye image. However, due to important variabilities in eye shapes and inner eye structures amongst individuals, universal models obtain limited accuracies and their output usually exhibit high variance as well as subject dependent biases. Thus, increasing accuracy is usually done through calibration, allowing gaze predictions for a subject to be mapped to her actual gaze. In this article, we introduce a novel approach, which works by directly training a differential convolutional neural network to predict gaze differences between two eye input images of the same subject. Then, given a set of subject specific calibration images, we can use the inferred differences to predict the gaze direction of a novel eye sample. The assumption is that by comparing eye images of the same user, annoyance factors (alignment, eyelid closing, illumination perturbations) which usually plague single image prediction methods can be much reduced, allowing better prediction altogether. Furthermore, the differential network itself can be adapted via finetuning to make predictions consistent with the available user reference pairs. Experiments on 3 public datasets validate our approach which constantly outperforms state-of-the-art methods even when using only one calibration sample or those relying on subject specific gaze adaptation.","1939-3539","","10.1109/TPAMI.2019.2957373","Swiss Innovation Agency; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8920005","Gaze estimation;differential network;gaze calibration","Estimation;Head;Training;Artificial neural networks;Calibration;Robustness;Feature extraction","calibration;eye;face recognition;feature extraction;gaze tracking;human computer interaction;learning (artificial intelligence);neural nets;pose estimation;regression analysis","eye images;single image prediction methods;differential network;subject specific gaze adaptation;differential approach;noninvasive gaze estimation methods regress gaze directions;single face;eye shapes;inner eye structures;subject dependent biases;increasing accuracy;gaze predictions;actual gaze;differential convolutional neural network;gaze differences;eye input images;subject specific calibration images;gaze direction;eye sample","","4","","46","IEEE","3 Dec 2019","","","IEEE","IEEE Journals"
"Forecasting People Trajectories and Head Poses by Jointly Reasoning on Tracklets and Vislets","I. Hasan; F. Setti; T. Tsesmelis; V. Belagiannis; S. Amin; A. Del Bue; M. Cristani; F. Galasso","Inception Institute of Artificial Intelligence, Abu Dhabi, UAE; Department of Computer Science, University of Verona, Verona, Italy; Italian Institute of Technology, Genova, Italy; Ulm University, Ulm, Germany; Osram GmbH, Munich, Germany; Italian Institute of Technology, Genova, Italy; Department of Computer Science, University of Verona, Verona, Italy; Osram GmbH, Munich, Germany","IEEE Transactions on Pattern Analysis and Machine Intelligence","4 Mar 2021","2021","43","4","1267","1278","In this article, we explore the correlation between people trajectories and their head orientations. We argue that people trajectory and head pose forecasting can be modelled as a joint problem. Recent approaches on trajectory forecasting leverage short-term trajectories (aka tracklets) of pedestrians to predict their future paths. In addition, sociological cues, such as expected destination or pedestrian interaction, are often combined with tracklets. In this article, we propose MiXing-LSTM (MX-LSTM) to capture the interplay between positions and head orientations (vislets) thanks to a joint unconstrained optimization of full covariance matrices during the LSTM backpropagation. We additionally exploit the head orientations as a proxy for the visual attention, when modeling social interactions. MX-LSTM predicts future pedestrians location and head pose, increasing the standard capabilities of the current approaches on long-term trajectory forecasting. Compared to the state-of-the-art, our approach shows better performances on an extensive set of public benchmarks. MX-LSTM is particularly effective when people move slowly, i.e., the most challenging scenario for all other models. The proposed approach also allows for accurate predictions on a longer time horizon.","1939-3539","","10.1109/TPAMI.2019.2949414","Marie Sklodowska-Curie(grant numbers:676455); Italian Ministry of Education, Universities and Research; PORFESR 2014-2020 Work Program(grant numbers:10066183); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8883081","LSTM;trajectory forecasting;RNN;head pose estimation;visual attention;gaze estimation","Magnetic heads;Forecasting;Trajectory;Correlation;Head;Predictive models;Visualization","backpropagation;covariance matrices;data visualisation;inference mechanisms;object tracking;optimisation;pedestrians;pose estimation;recurrent neural nets;traffic engineering computing","MX-LSTM;jointly reasoning;head orientations;aka tracklets;expected destination;joint unconstrained optimization;LSTM backpropagation;long-term trajectory forecasting;people trajectories forecasting;mixing-LSTM;pedestrians location;visual attention;social interactions;head pose forecasting;full covariance matrices","","6","","87","IEEE","25 Oct 2019","","","IEEE","IEEE Journals"
"Real-time Object Detection of Retail Products for Eye Tracking","W. Fang; K. Zhang","Suzhou Institute of Trade and Commerce,Information Technology Department,Suzhou,China; Suzhou Institute of Trade and Commerce,Information Technology Department,Suzhou,China","2020 8th International Conference on Orange Technology (ICOT)","7 Jul 2021","2020","","","1","4","Object detection is one important task in automatically analyzing eye tracking video data. This paper presents a real-time object detection method of retail products based on deep learning for eye tracking system. In the proposed approach, an eye tracking based Convolutional Neural Networks is constructed to obtain the feature of original images. Then, a weighted bounding box selection strategy based on gaze location is used for object detection. Besides, parameters are adjusted according to gaze location of eye tracking. Experimental results show that our method can achieve better accuracy for eye tracking than other existing methods in the detection of retail products.","","978-1-6654-1852-2","10.1109/ICOT51877.2020.9468806","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468806","Object Detection;Eye Tracking;Convolutional Neural Networks;Gaze Location","Deep learning;Gaze tracking;Object detection;Detectors;Streaming media;Benchmark testing;Real-time systems","learning (artificial intelligence);neural nets;object detection;video signal processing","retail products;eye tracking system;real-time object detection method","","","","20","","7 Jul 2021","","","IEEE","IEEE Conferences"
"Drowsiness Detection using Facial Emotions and Eye Aspect Ratios","S. Cheamanunkul; S. Chawla","Mahidol University International College,Nakhon Pathom,Thailand,73170; Mahidol University International College,Nakhon Pathom,Thailand,73170","2020 24th International Computer Science and Engineering Conference (ICSEC)","15 Mar 2021","2020","","","1","4","Drowsy drivers are a major cause of many road accidents around the world. Facial emotions are known to be one of the visual cues for detecting drowsiness. In this paper, we propose a machine learning approach to drowsiness detection based on using a combination of facial emotion features extracted by using deep convolutional neural networks (CNN) and eye-aspect-ratio (EAR) features. The combined feature vectors are then used for training a classifier. From our experiments, we obtain a classification accuracy of 81.7% when we use the combined features with a support vector machines (SVM) classifier.","","978-1-6654-0339-9","10.1109/ICSEC51790.2020.9375240","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9375240","","Support vector machines;Visualization;Machine learning;Feature extraction;Classification algorithms;Convolutional neural networks;Vehicles","convolutional neural nets;deep learning (artificial intelligence);emotion recognition;face recognition;feature extraction;gaze tracking;image classification;road accidents;support vector machines;traffic engineering computing","eye-aspect-ratio features;facial emotion feature extraction;machine learning approach;CNN;EAR;support vector machines classifier;SVM;road accidents;drowsy drivers;combined feature vectors;deep convolutional neural networks;drowsiness detection","","","","12","","15 Mar 2021","","","IEEE","IEEE Conferences"
"Development of a Sign Language for Total Paralysis and Interpretation using Deep Learning","Z. Z. Sarowar Dhrubo; M. A. Islam Hridoy; L. Jamal; S. Sarker; M. Shidujaman","University of Dhaka,Department of Robotics and Mechatronics Engineering,Dhaka,Bangladesh; University of Dhaka,Department of Robotics and Mechatronics Engineering,Dhaka,Bangladesh; University of Dhaka,Department of Robotics and Mechatronics Engineering,Dhaka,Bangladesh; University of Dhaka,Department of Robotics and Mechatronics Engineering,Dhaka,Bangladesh; Tsinghua University,Academy of Arts and Design,X-Studio, Dept. of Information Art and Design,Beijing,China","2020 International Conference on Image Processing and Robotics (ICIP)","5 Mar 2021","2020","","","1","6","People with total paralysis suffer not just from physical disability but also from the agony of the inability to communicate and express their feelings. Researches have long tried to solve this problem with various techniques such as collecting the patient's message directly from the brain using Brain-Computer interface and determining the gaze of the patient on a screen that contains letters and symbols. The first approach proved to be expensive and less accurate. The second one needs very fine calculation of the pupil center and even if done correctly the drifting of the human eye makes it hard to get a better accuracy. Again, all of these methods are dependent on electronic devices. In this paper, we proposed a solution that does not require the help of any electronic devices by developing a sign language for eye movements. We also proposed an approach to automatically interpret the patient's sign using Convolutional Neural Network and achieved great accuracy on the training data. This suggests that our network can be used to provide a better communication method for paralyzed patients.","","978-1-7281-6541-7","10.1109/ICIP48927.2020.9367362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9367362","Quadriplegia;Convolutional Neural Network;Eye Sign;Sign Language;Deep Learning","Training;Assistive technology;Training data;Gesture recognition;Paralysis;Pupils;Videos","brain-computer interfaces;convolutional neural nets;deep learning (artificial intelligence);feature extraction;gaze tracking;handicapped aids;image classification;medical disorders;natural language processing;sign language recognition","convolutional neural network;automatic patient sign interpretation;paralyzed patients;communication method;eye movements;deep learning;sign language","","","","23","","5 Mar 2021","","","IEEE","IEEE Conferences"
"3D Gaze Estimation for Head-Mounted Devices based on Visual Saliency","M. Liu; Y. Fu Li; H. Liu","City University of Hong Kong,Department of Mechanical Engineering,Kowloon,Hong Kong; City University of Hong Kong,Department of Mechanical Engineering,Kowloon,Hong Kong; Central China Normal University,National Engineering Research Center for E-Learning,Wuhan,China","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","10611","10616","Compared with the maturity of 2D gaze tracking technology, 3D gaze tracking has gradually become a research hotspot in recent years. The head-mounted gaze tracker has shown great potential for gaze estimation in 3D space due to its appealing flexibility and portability. The general challenge for 3D gaze tracking algorithms is that calibration is necessary before the usage, and calibration targets cannot be easily applied in some situations or might be blocked by moving human and objects. Besides, the accuracy on depth direction has always come to be a crucial problem. Regarding the issues mentioned above, a 3D gaze estimation with auto-calibration method is proposed in this study. We use an RGBD camera as the scene camera to acquire the accurate 3D structure of the environment. The automatic calibration is achieved by uniting gaze vectors with saliency maps of the scene which aligned depth information. Finally, we determine the 3D gaze point through a point cloud generated from the RGBD camera. The experiment result demonstrates that our proposed method achieves 4.34° of average angle error in the field from 0.5m to 3m and the average depth error is 23.22mm, which is sufficient for 3D gaze estimation in the real scene.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9341755","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9341755","","Visualization;Three-dimensional displays;Target tracking;Estimation;Gaze tracking;Cameras;Calibration","calibration;cameras;gaze tracking;human computer interaction;object tracking;stereo image processing","gaze vectors;3D gaze point;3D gaze estimation;head-mounted devices;head-mounted gaze tracker;3D gaze tracking algorithms;3D structure;visual saliency","","","","25","","10 Feb 2021","","","IEEE","IEEE Conferences"
"Distilling Location Proposals of Unknown Objects through Gaze Information for Human-Robot Interaction","D. Weber; T. Santini; A. Zell; E. Kasneci","University of Tübingen,Tübingen,Germany,72076; University of Tübingen,Tübingen,Germany,72076; University of Tübingen,Tübingen,Germany,72076; University of Tübingen,Tübingen,Germany,72076","2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","10 Feb 2021","2020","","","11086","11093","Successful and meaningful human-robot interaction requires robots to have knowledge about the interaction context - e.g., which objects should be interacted with. Unfortunately, the corpora of interactive objects is - for all practical purposes - infinite. This fact hinders the deployment of robots with pre-trained object-detection neural networks other than in pre-defined scenarios. A more flexible alternative to pre-training is to let a human teach the robot about new objects after deployment. However, doing so manually presents significant usability issues as the user must manipulate the object and communicate the object's boundaries to the robot. In this work, we propose streamlining this process by using automatic object location proposal methods in combination with human gaze to distill pertinent object location proposals. Experiments show that the proposed method 1) increased the precision by a factor of approximately 21 compared to location proposal alone, 2) is able to locate objects sufficiently similar to a state-of-the-art pre-trained deep-learning method (FCOS) without any training, and 3) detected objects that were completely missed by FCOS. Furthermore, the method is able to locate objects for which FCOS was not trained on, which are undetectable for FCOS by definition.","2153-0866","978-1-7281-6212-6","10.1109/IROS45743.2020.9340893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340893","","Training;Neural networks;Human-robot interaction;Object detection;Proposals;Usability;Robots","deep learning (artificial intelligence);gaze tracking;human-robot interaction;neural nets;object detection","gaze information;human-robot interaction;interactive objects;pre-trained object-detection neural networks;automatic object location proposal methods;human gaze;pertinent object location proposals;distilling location proposals;pre-trained deep-learning method;interaction context;FCOS","","","","51","","10 Feb 2021","","","IEEE","IEEE Conferences"
"Eyes speak out Mind: Deep models for Gaze-based Analysis of Bilingual And Monolingual Reading","A. Prava Roy; S. Kumar Koley; U. Garain","National Inst. of Technology,Dept. of ECE,Durgapur,India; Indian Statistical Institute,Comp. Vis. & Patt. Recog. (CVPR) Unit,Kolkata,India; Indian Statistical Institute,CVPR Unit and Centre for AIML (CAIML),Kolkata,India","2020 IEEE 17th India Council International Conference (INDICON)","5 Feb 2021","2020","","","1","7","This paper presents an approach that attempts to explore effects of bilingualism in reading by analysis of eye-gaze data. As readers are more comfortable in reading first language (L1) compared to second language (L2), their eyes move differently in these two cases. Being motivated by this hypothesis, a deep learning model is developed to predict whether a reader is reading first or second language by analyzing the reading behavior given by the eye-tracking data. Exposure to different languages simultaneously may influence the proficiency in native language reading and this matter is also investigated by using deep learning model which can differentiate monolingual readers from bilingual readers analyzing their eye movement during reading in their respective native language. Experiments are conducted on GECO Corpus which provides eye-tracking data of 19 readers reading a novel in L1 and L2 along with eye-tracking data of several monolingual readers reading same text in their native language. Experimental results show that the proposed models are quite efficient in capturing the differences in eye gaze pattern for reading L1 and L2 for a reader as well as in identifying monolingual and bilingual readers by processing eye-gaze data.","2325-9418","978-1-7281-6916-3","10.1109/INDICON49873.2020.9342182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9342182","Eye-tracking Data;Reading Behavior;First Language;Second Language;Monolingual;Bilingual;Deep Learning;Feature attribution","Deep learning;Analytical models;Psychology;Predictive models;Feature extraction;Data models;Data mining","deep learning (artificial intelligence);eye;gaze tracking;human factors","deep models;gaze-based analysis;eye-gaze data;eye gaze pattern;respective native language;eye movement;bilingual readers;monolingual readers;native language reading;eye-tracking data;reading behavior;deep learning model","","","","21","","5 Feb 2021","","","IEEE","IEEE Conferences"
"Scanpaths Generation for Target Search Based on Deep Learning","J. Wu; M. Lu; Y. Lin; X. Zhang","National Engineering Laboratory for Visual Information Processing and Applications,Xi’an,Shaanxi,China,710049; National Engineering Laboratory for Visual Information Processing and Applications,Xi’an,Shaanxi,China,710049; Xi'an Jiaotong University,School of Foreign Studies,Xi’an,Shaanxi,710049; National Engineering Laboratory for Visual Information Processing and Applications,Xi’an,Shaanxi,China,710049","2020 Chinese Automation Congress (CAC)","29 Jan 2021","2020","","","1443","1448","Scanpath is a sequence of gaze fixations changing with time when browsing something, which records eyes' movement dynamically. Accurate prediction of scanpaths can help computers better predict which area human pay attention to and drive the development of next-generation systems that can understand human behavior and needs. Therefore, this paper introduces an algorithm of scanpaths generation for target search. Based on one-shot learning network, the algorithm extracts the target map with task information, and then imports it into a visual model of the superior colliculus to predict the task-drive scanpaths. The results are similar to the gaze behavior of human eyes in shape, direction and length. At the same time, the algorithm also has the ability to predict the scanpaths based on unseen categories.","2688-0938","978-1-7281-7687-1","10.1109/CAC51589.2020.9327358","Research and Development; Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9327358","Scanpaths Generation;Deep Learning;Target Search;Computer Vision","Task analysis;Visualization;Predictive models;Search problems;Proposals;Prediction algorithms;Clocks","behavioural sciences computing;deep learning (artificial intelligence);gaze tracking;neurophysiology;search problems;visual perception","scanpaths generation;target search;deep learning;gaze fixations;next-generation systems;human behavior;one-shot learning network;target map;task-drive scanpaths;gaze behavior;human eyes;human pay attention","","","","24","","29 Jan 2021","","","IEEE","IEEE Conferences"
"DeeSCo: Deep heterogeneous ensemble with Stochastic Combinatory loss for gaze estimation","E. Yvinec; A. Dapogny; K. Bailly","Datakalab, 114 Boulevard Malesherbes,Paris,France,75017; Datakalab, 114 Boulevard Malesherbes,Paris,France,75017; Datakalab, 114 Boulevard Malesherbes,Paris,France,75017","2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)","18 Jan 2021","2020","","","146","152","From medical research to gaming applications, gaze estimation is becoming a valuable tool. While there exists a number of hardware-based solutions, recent deep learning-based approaches, coupled with the availability of large-scale databases, have allowed to provide a precise gaze estimate using only consumer sensors. However, there remains a number of questions, regarding the problem formulation, architectural choices and learning paradigms for designing gaze estimation systems in order to bridge the gap between geometry-based systems involving specific hardware and approaches using consumer sensors only. In this paper, we introduce a deep, end-to-end trainable ensemble of heatmap-based weak predictors for 2D/3D gaze estimation. We show that, through heterogeneous architectural design of these weak predictors, we can improve the decorrelation between the latter predictors to design more robust deep ensemble models. Furthermore, we propose a stochastic combinatory loss that consists in randomly sampling combinations of weak predictors at train time. This allows to train better individual weak predictors, with lower correlation between them. This, in turns, allows to significantly enhance the performance of the deep ensemble. We show that our Deep heterogeneous ensemble with Stochastic Combinatory loss (DeeSCo) outperforms state-of-the-art approaches for 2D/3D gaze estimation on multiple datasets.","","978-1-7281-3079-8","10.1109/FG47880.2020.00039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320196","Gaze estimation;deep learning;Ensemble methods","Estimation;Heating systems;Stochastic processes;Predictive models;Agriculture;Two dimensional displays;Decorrelation","decorrelation;deep learning (artificial intelligence);gaze tracking;stereo image processing;stochastic processes","decorrelation;3D gaze estimation;2D gaze estimation;large-scale databases;deep heterogeneous ensemble;DeeSCo;deep ensemble models;heterogeneous architectural design;heatmap-based weak predictors;end-to-end trainable ensemble;geometry-based systems;gaze estimation systems;consumer sensors;deep learning;hardware-based solutions;stochastic combinatory loss","","","","37","","18 Jan 2021","","","IEEE","IEEE Conferences"
"Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep Visual Speech Recognition","Y. Zhang; S. Yang; J. Xiao; S. Shan; X. Chen","Institute of Computing Technology, CAS,Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),Beijing,China,100190; Institute of Computing Technology, CAS,Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),Beijing,China,100190; Institute of Computing Technology, CAS,Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),Beijing,China,100190; Institute of Computing Technology, CAS,Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),Beijing,China,100190; Institute of Computing Technology, CAS,Key Laboratory of Intelligent Information Processing of Chinese Academy of Sciences (CAS),Beijing,China,100190","2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)","18 Jan 2021","2020","","","356","363","Recent advances in deep learning have heightened interest among researchers in the field of visual speech recognition (VSR). Currently, most existing methods equate VSR with automatic lip reading, which attempts to recognise speech by analysing lip motion. However, human experience and psychological studies suggest that we do not always fix our gaze at each other's lips during a face-to-face conversation, but rather scan the whole face repetitively. This inspires us to revisit a fundamental yet somehow overlooked problem: can VSR models benefit from reading extraoral facial regions, i.e. beyond the lips? In this paper, we perform a comprehensive study on the evaluation of the effects of different facial regions with state-of-the-art VSR models, including the mouth, the whole face, the upper face, and even the cheeks. Experiments are conducted on both word-level and sentence-level benchmarks with different characteristics. We find that despite the complex variations of the data, incorporating information from extraoral facial regions, even the upper face, consistently benefits VSR performance. Furthermore, we introduce a simple yet effective method based on Cutout to learn more discriminative features for face-based VSR, hoping to maximise the utility of information encoded in different facial regions. Our experiments show obvious improvements over existing state-of-the-art methods that use only the lip region as inputs, a result we believe would probably provide the VSR community with some new and exciting insights.","","978-1-7281-3079-8","10.1109/FG47880.2020.00134","Chinese Academy of Sciences; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320240","visual speech recognition","Face recognition;Lips;Speech recognition;Mouth;Visualization;Task analysis;Agriculture","audio-visual systems;face recognition;feature extraction;human computer interaction;learning (artificial intelligence);psychology;speech recognition;word processing","read speech;lips;RoI selection;deep visual speech recognition;deep learning;existing methods equate VSR;automatic lip reading;lip motion;human experience;psychological studies;face-to-face conversation;fundamental yet somehow overlooked problem;extraoral facial regions;different facial regions;state-of-the-art VSR models;upper face;word-level;sentence-level benchmarks;VSR performance;simple yet effective method;face-based VSR;lip region;VSR community","","","","39","","18 Jan 2021","","","IEEE","IEEE Conferences"
"Student and Lecturer Performance Enhancement System using Artificial Intelligence","I. K. Seneviratne; B. A. S. D. Perera; R. S. D. Fernando; L. K. B. Siriwardana; U. U. S. K. Rajapaksha","Sri Lanka Institute of Information Technology,Dept of Software Engineering,Malabe,Sri Lanka; Sri Lanka Institute of Information Technology,Dept of Software Engineering,Malabe,Sri Lanka; Sri Lanka Institute of Information Technology,Dept of Software Engineering,Malabe,Sri Lanka; Sri Lanka Institute of Information Technology,Dept of Software Engineering,Malabe,Sri Lanka; Sri Lanka Institute of Information Technology,Dept of Software Engineering,Malabe,Sri Lanka","2020 3rd International Conference on Intelligent Sustainable Systems (ICISS)","18 Jan 2021","2020","","","88","93","The proposed research work develops a system to enhance the performance of university students and lecturers by providing an excellent statistical insight. Already existing research works have attempted to solve independent classroom challenges that are related to measuring the student attention and marking student attendance but the existing research works have not combined theimportant aspects into one system. Hence, the proposed research wor has been carried out on various main aspects such as attendance register, monitoring student behavior as well as lecturer performance and lecture summarization. The system will incorporate tools and technologies in the different domains of artificial intelligence, machine learning, and natural language processing. After implementing and testing the proposed method it has been concluded that the student activity recognition process has been performed much better than the other emotion and gaze components by providing 94.5% results. The proposed system can determine the lecturer's physical activities and the quality of the lecture content with a reasonable accuracy. The summarized lecture has showed 70% similarity to actual lecture content and student attendance by using Face Recognition was marked with 83% accuracy. This research concludes that the automation of major classroom activities will impact the students and lecturers positively. Also, this system yields valuable results and increases the productivity of higher education institutions in the future.","","978-1-7281-7089-3","10.1109/ICISS49785.2020.9315981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9315981","Artificial intelligence;computer vision;deep learning;natural language processing","Face recognition;Monitoring;Videos;Deep learning;Artificial intelligence;Activity recognition;Face detection","computer aided instruction;educational institutions;face recognition;further education;learning (artificial intelligence);natural language processing;statistical analysis","attendance register;student behavior;lecture summarization;artificial intelligence;natural language processing;student activity recognition process;lecturer performance enhancement system;University students;statistical insight;student attention;student attendance;student performance enhancement system;machine learning;gaze components;emotion components;lecturers physical activities;face recognition;higher education institutions","","","","17","","18 Jan 2021","","","IEEE","IEEE Conferences"
"Appearance-Based Gaze Tracking Through Supervised Machine Learning","D. Melesse; M. Khalil; E. Kagabo; T. Ning; K. Huang","Trinity College,Department of Engineering,Hartford,Connecticut,USA; Trinity College,Department of Engineering,Hartford,Connecticut,USA; Trinity College,Department of Engineering,Hartford,Connecticut,USA; Trinity College,Department of Engineering,Hartford,Connecticut,USA; Trinity College,Department of Engineering,Hartford,Connecticut,USA","2020 15th IEEE International Conference on Signal Processing (ICSP)","18 Jan 2021","2020","1","","467","471","Applications that use human gaze have become increasingly more popular in the domain of human-computer interfaces, and advances in eye gaze tracking technology over the past few decades have led to the development of promising gaze estimation techniques. In this paper, a low-cost, in-house video camera-based gaze tracking system was developed, trained and evaluated. Seminal gaze detection methods constrained the application space to indoor conditions, and in most cases techniques required intrusive hardware. More modern gaze detection techniques try to eliminate the use of any additional hardware to reduce monetary cost as well as undue burden to the user, all the while maintaining accuracy of detection. In this work, image acquisition was achieved using a low-cost USB web camera mounted at a fixed position on the viewing screen or laptop. In order to determine the point of gaze, the Viola Jones face detection algorithm is used to extract facial features from the image frame. The gaze is then calculated using image processing techniques to extract gaze features, namely related to the image position of the pupil. Thousands of images are classified and labeled to form an in-house database. A multi-class Support Vector Machine (SVM) was trained and tested on this data set to distinguish point of gaze from input face image. Cross validation was used to train the model. Confusion matrices, accuracy, precision, and recall are used to evaluate the performance of the classification model. Evaluation of the proposed appearance-based technique using two different kernel functions is also assessed in detail.","2164-5221","978-1-7281-4480-1","10.1109/ICSP48669.2020.9321075","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321075","human computer interface;automatic gaze tracking;support vector machine;face detection","Support vector machines;Kernel;Feature extraction;Pupils;Gaze tracking;Faces;Cameras","cameras;face recognition;feature extraction;gaze tracking;learning (artificial intelligence);object detection;support vector machines;video cameras","eye gaze;promising gaze estimation techniques;in-house video camera-based gaze tracking system;seminal gaze detection methods;application space;cases techniques;modern gaze detection techniques;monetary cost;image acquisition;low-cost USB web camera;Viola Jones face detection algorithm;image frame;gaze features;image position;multiclass Support Vector Machine;input face image;appearance-based technique;appearance-based gaze tracking;human gaze;human-computer interfaces","","","","24","","18 Jan 2021","","","IEEE","IEEE Conferences"
"RNN-based Pedestrian Crossing Prediction using Activity and Pose-related Features","J. Lorenzo; I. Parra; F. Wirth; C. Stiller; D. F. Llorca; M. A. Sotelo","Universidad de Alcalá,Department of Computer Engineering,Madrid,Spain; Universidad de Alcalá,Department of Computer Engineering,Madrid,Spain; Institut für Mess- und Regelungstechnik, Karlsruher Institut für Technologie,Karlsruhe,Germany; Institut für Mess- und Regelungstechnik, Karlsruher Institut für Technologie,Karlsruhe,Germany; Universidad de Alcalá,Department of Computer Engineering,Madrid,Spain; Universidad de Alcalá,Department of Computer Engineering,Madrid,Spain","2020 IEEE Intelligent Vehicles Symposium (IV)","8 Jan 2021","2020","","","1801","1806","Pedestrian crossing prediction is a crucial task for autonomous driving. Numerous studies show that an early estimation of the pedestrian's intention can decrease or even avoid a high percentage of accidents. In this paper, different variations of a deep learning system are proposed to attempt to solve this problem. The proposed models are composed of two parts: a CNN-based feature extractor and an RNN module. All the models were trained and tested on the JAAD dataset. The results obtained indicate that the choice of the features extraction method, the inclusion of additional variables such as pedestrian gaze direction and discrete orientation, and the chosen RNN type have a significant impact on the final performance.","2642-7214","978-1-7281-6673-5","10.1109/IV47402.2020.9304652","Universidad de Alcalá; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9304652","","Feature extraction;Training;Data mining;Computer architecture;Roads;Deep learning;Vehicle dynamics","convolutional neural nets;feature extraction;gaze tracking;learning (artificial intelligence);pedestrians;recurrent neural nets","pedestrian gaze direction;RNN type;RNN-based pedestrian crossing prediction;autonomous driving;early estimation;deep learning system;CNN-based feature extractor;RNN module;features extraction method;JAAD dataset","","","","28","","8 Jan 2021","","","IEEE","IEEE Conferences"
"Driver Gaze Estimation in the Real World: Overcoming the Eyeglass Challenge","A. Rangesh; B. Zhang; M. M. Trivedi","Laboratory for Intelligent and Safe Automobiles at the University of California,San Diego,CA,USA,92092; Laboratory for Intelligent and Safe Automobiles at the University of California,San Diego,CA,USA,92092; Laboratory for Intelligent and Safe Automobiles at the University of California,San Diego,CA,USA,92092","2020 IEEE Intelligent Vehicles Symposium (IV)","8 Jan 2021","2020","","","1054","1059","A driver's gaze is critical for determining the driver's attention level, state, situational awareness, and readiness to take over control from partially and fully automated vehicles. Tracking both the head and eyes (pupils) can provide a reliable estimation of a driver's gaze using face images under ideal conditions. However, the vehicular environment introduces a variety of challenges that are usually unaccounted for - harsh illumination, nighttime conditions, and reflective/dark eyeglasses. Unfortunately, relying on head pose alone under such conditions can prove to be unreliable owing to significant eye movements. In this study, we offer solutions to address these problems encountered in the real world. To solve issues with lighting, we demonstrate that using an infrared camera with suitable equalization and normalization usually suffices. To handle eyeglasses and their corresponding artifacts, we adopt the idea of image-to-image translation using generative adversarial networks (GANs) to pre-process images prior to gaze estimation. To this end, we propose the Gaze Preserving CycleGAN (GPCycleGAN). This network preserves the driver's gaze while removing potential eyeglasses from infrared face images. Our approach exhibits improved performance and robustness on challenging real-world data spanning 13 subjects and a variety of driving conditions.","2642-7214","978-1-7281-6673-5","10.1109/IV47402.2020.9304573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9304573","","Estimation;Cameras;Data models;Lighting;Training;Faces;Vehicles","cameras;driver information systems;eye;face recognition;gaze tracking;infrared imaging;neural nets;pose estimation","infrared face image;GPCycleGAN;gaze preserving CycleGAN;generative adversarial networks;infrared camera;head pose;reflective/dark eyeglasses;face image;driver gaze estimation;eyeglass challenge;image-to-image translation","","","","30","","8 Jan 2021","","","IEEE","IEEE Conferences"
"On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR Applications","F. Boutros; N. Damer; K. Raja; R. Ramachandra; F. Kirchbuchner; A. Kuijper","Mathematical and Applied Visual Computing, TU Darmstadt,Darmstadt,Germany; Mathematical and Applied Visual Computing, TU Darmstadt,Darmstadt,Germany; Norwegian Biometrics Laboratory, NTNU,Gjovik,Norway; Norwegian Biometrics Laboratory, NTNU,Gjovik,Norway; Mathematical and Applied Visual Computing, TU Darmstadt,Darmstadt,Germany; Mathematical and Applied Visual Computing, TU Darmstadt,Darmstadt,Germany","2020 IEEE International Joint Conference on Biometrics (IJCB)","6 Jan 2021","2020","","","1","10","Augmented and virtual reality is being deployed in different fields of applications. Such applications might involve accessing or processing critical and sensitive information, which requires strict and continuous access control. Given that Head-Mounted Displays (HMD) developed for such applications commonly contains internal cameras for gaze tracking purposes, we evaluate the suitability of such setup for verifying the users through iris recognition. In this work, we first evaluate a set of iris recognition algorithms suitable for HMD devices by investigating three well-established handcrafted feature extraction approaches, and to complement it, we also present the analysis using four deep learning models. While taking into consideration the minimalistic hardware requirements of stand-alone HMD, we employ and adapt a recently developed miniature segmentation model (EyeMMS) for segmenting the iris. Further, to account for non-ideal and non-collaborative capture of iris, we define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to quantify the iris recognition performance. Motivated by the performance of iris recognition, we also propose the continuous authentication of users in a non-collaborative capture setting in HMD. Through the experiments on a publicly available OpenEDS dataset, we show that performance with EER = 5% can be achieved using deep learning methods in a general setting, along with high accuracy for continuous user authentication.","2474-9699","978-1-7281-9186-7","10.1109/IJCB48548.2020.9304919","German Federal Ministry of Education and Research and the Hessen State Ministry for Higher Education, Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9304919","","Iris recognition;Resists;Feature extraction;Image segmentation;Cameras;Pupils;Authentication","authorisation;biometrics (access control);feature extraction;helmet mounted displays;image recognition;iris recognition;learning (artificial intelligence);virtual reality","recently developed miniature segmentation model;minimalistic hardware requirements;deep learning models;HMD devices;iris recognition algorithms;gaze tracking purposes;internal cameras;Head-Mounted Displays;continuous access control;strict access control;sensitive information;critical information;virtual reality;augmented reality;Head-mounted display;benchmarking Iris recognition;continuous user authentication;noncollaborative capture setting;iris recognition performance;Iris Mask Ratio;iris quality","","","","50","","6 Jan 2021","","","IEEE","IEEE Conferences"
"Deep Learning based Eye Gaze Tracking for Automotive Applications: An Auto-Keras Approach","A. Bublea; C. D. Căleanu","Politehnica University Timișoara,Department of Applied Electronics Faculty of Electronics, Telecommunications and Information Technologies,Timisoara,Romania; Politehnica University Timișoara,Department of Applied Electronics Faculty of Electronics, Telecommunications and Information Technologies,Timisoara,Romania","2020 International Symposium on Electronics and Telecommunications (ISETC)","1 Jan 2021","2020","","","1","4","We propose a deep neural network-based gaze sensing method in which the design of the neural architecture is performed automatically, through a network architecture search algorithm called Auto-Keras. First, the neural model is generated using the Columbia Gaze Data Set. Then, the performance of the solution is estimated on an online scenario and proves the generalization ability of our model. In comparison to a geometrical approach, which uses dlib facial landmarks, filtering and morphological operators for gaze estimation, the proposed method provides superior results and certain advantages.","2475-7861","978-1-7281-9513-1","10.1109/ISETC50328.2020.9301091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301091","eye gaze tracking;deep learning;automotive","Estimation;Vehicles;Cameras;Computer architecture;Gaze tracking;Automotive engineering;Visualization","automotive engineering;deep learning (artificial intelligence);eye;gaze tracking;generalisation (artificial intelligence);neural net architecture","neural architecture;network architecture search algorithm;neural model;Columbia Gaze Data Set;geometrical approach;gaze estimation;eye gaze tracking;automotive applications;Auto-Keras approach;deep neural network;gaze sensing;generalization ability","","","","23","","1 Jan 2021","","","IEEE","IEEE Conferences"
"Decision Anticipation for Driving Assistance Systems","P. V. Amadori; T. Fischer; R. Wang; Y. Demiris","Imperial College London,Personal Robotics Laboratory,Department of Electrical and Electronic Engineering,SW7 2BT,U.K.; Imperial College London,Personal Robotics Laboratory,Department of Electrical and Electronic Engineering,SW7 2BT,U.K.; Imperial College London,Personal Robotics Laboratory,Department of Electrical and Electronic Engineering,SW7 2BT,U.K.; Imperial College London,Personal Robotics Laboratory,Department of Electrical and Electronic Engineering,SW7 2BT,U.K.","2020 IEEE 23rd International Conference on Intelligent Transportation Systems (ITSC)","24 Dec 2020","2020","","","1","7","Anticipating the correctness of imminent driver decisions is a crucial challenge in advanced driving assistance systems and has the potential to lead to more reliable and safer human-robot interactions. In this paper, we address the task of decision correctness prediction in a driver-in-the-loop simulated environment using unobtrusive physiological signals, namely, eye gaze and head pose. We introduce a sequence-to-sequence based deep learning model to infer the driver's likelihood of making correct/wrong decisions based on the corresponding cognitive state. We provide extensive experimental studies over multiple baseline classification models on an eye gaze pattern and head pose dataset collected from simulated driving. Our results show strong correlates between the physiological data and decision correctness, and that the proposed sequential model reliably predicts decision correctness from the driver with 80% precision and 72% recall. We also demonstrate that our sequential model performs well in scenarios where early anticipation of correctness is critical, with accurate predictions up to two seconds before a decision is performed.","","978-1-7281-4149-7","10.1109/ITSC45102.2020.9294216","Royal Academy of Engineering; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9294216","","Task analysis;Biomedical monitoring;Feature extraction;Monitoring;Vehicles;Predictive models;Load modeling","cognition;deep learning (artificial intelligence);driver information systems;human-robot interaction;pattern classification","multiple baseline classification models;eye gaze pattern;simulated driving;sequential model performs;decision anticipation;driver decisions;advanced driving assistance systems;reliable human-robot interactions;safer human-robot interactions;decision correctness prediction;driver-in-the-loop simulated environment;unobtrusive physiological signals;sequence-to-sequence based deep learning model;head pose dataset","","","","29","","24 Dec 2020","","","IEEE","IEEE Conferences"
"Robust Deep Learning Technique: U-Net Architecture for Pupil Segmentation","S. Gowroju; Aarti; S. Kumar","Lovely Professional University,Computer Science Engineering,Punjab,India; Lovely Professional University,Computer Science Engineering,Punjab,India; Electronics & Communication Engineering, Sreyas Institute of Engineering and Technology,Hyderabad,India","2020 11th IEEE Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON)","22 Dec 2020","2020","","","0609","0613","In many of the iris biometric applications plays a major role in tracking the gaze, detecting fatigue, and predicting the age of a person, etc. that were built for human-computer interaction and security applications such as border control applications or criminal tracking applications. In this paper, we proposed a novel CNN U-Net based model to perform the accurate segmentation of pupil. We experimented on the CASIA database and generated an accuracy of 90% in segmentation. We considered various parameters such as Accuracy, Loss, and Mean Square Error (MSE) to predict the efficiency of the model. The proposed system performed the segmentation of pupil from 512×512 images with MSE of 1.24.","2644-3163","978-1-7281-8416-6","10.1109/IEMCON51383.2020.9284947","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284947","CNN;U-Net;Segmentation;Val_Loss;Val_Accuracy;Pupil","Image segmentation;Databases;Mean square error methods;Predictive models;Mobile communication;Security;Pupils","biometrics (access control);convolutional neural nets;deep learning (artificial intelligence);image segmentation;mean square error methods","border control applications;criminal tracking applications;CASIA database;mean square error methods;robust deep learning technique;pupil segmentation;iris biometric applications;human-computer interaction;security applications;CNN U-Net architecture;MSE methods;image segmentation","","1","","35","","22 Dec 2020","","","IEEE","IEEE Conferences"
"Automatic Generation of Eye Gaze Corrected Video Using Recursive Conditional Generative Adversarial Networks","K. Otsu; M. Seo; T. Kitajima; Y. -W. Chen","Ritsumeikan University,Graduate School of Information and Engineering,Shiga,Japan; Osaka Institute of Technology,Osaka,Japan; Samsung Japan Research Institute,Kanagawa,Japan; Ritsumeikan University,Graduate School of Information and Engineering,Shiga,Japan","2020 IEEE 9th Global Conference on Consumer Electronics (GCCE)","21 Dec 2020","2020","","","674","677","Eye contact plays an important role in conversations. However, it is difficult to maintain eye contact while using current popular video calling systems due to the different positions of the camera and display. To solve this problem, we introduce convolutional long short-term memory, which captures the features of frames up to the previous instant, into a deep learning generation model-conditional generative adversarial networks (GANs), which is recursive GANs-to generate eye gaze corrected video. By extending this network, the generator generates an image that takes the previous frame into account and the discriminator identifies the image by considering the previous frame. Thus, we aimed to achieve a consistent video conversion.","2378-8143","978-1-7281-9802-6","10.1109/GCCE50665.2020.9291784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9291784","deep learning;generative adversarial networks;Convolutional LSTM;Video;image transformation;face","Deep learning;Conferences;Network architecture;Generative adversarial networks;Cameras;Generators;Consumer electronics","convolutional neural nets;deep learning (artificial intelligence);gaze tracking;recurrent neural nets;video signal processing","automatic generation;eye gaze corrected video;recursive conditional generative adversarial networks;eye contact;convolutional long short-term memory;deep learning generation model;video conversion;recursive GANs;video calling systems","","","","8","","21 Dec 2020","","","IEEE","IEEE Conferences"
"Robust Physician Gaze Prediction Using a Deep Learning Approach","T. Tan; E. Montague; J. Furst; D. Raicu","DePaul University,College of Computing and Digital Media,Chicago,USA; DePaul University,College of Computing and Digital Media,Chicago,USA; DePaul University,College of Computing and Digital Media,Chicago,USA; DePaul University,College of Computing and Digital Media,Chicago,USA","2020 IEEE 20th International Conference on Bioinformatics and Bioengineering (BIBE)","16 Dec 2020","2020","","","993","998","The patient-physician relationship is an integral part of primary care visits. To build a better relationship, understanding the communication between patient and physician is the key. This study focused on analyzing the gaze, one of the most important non-verbal behaviors found to influence patient outcomes. Gaze analysis often needs a manual rating process which might be time-consuming, costly, and unreliable. This research aimed to support automated analysis of physician-patient interaction using a deep convolutional neural network with transfer learning to a build robust model for physician gaze prediction. Utilizing only 3 minutes of 15 videos capturing 3 physicians interacting with different patients in a clinical setting, the model achieved over 98% accuracy for train, test, and validation sets. By visualizing the convolutional layers and comparing sample frames from different interactions, results highlighted several patterns shared across frames predicted correctly from both seen and unseen video sequences. The proposed work has the potential to informed the future design of technologies used to capture the clinical interaction and provide real-time feedback for physicians, which will contribute to the improvement of care quality.","2471-7819","978-1-7281-9574-2","10.1109/BIBE50027.2020.00168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288056","primary care visits;deep learning;automatic labeling;physician gaze","Deep learning;Annotations;Video sequences;Medical services;Predictive models;Real-time systems;Videos","convolutional neural nets;deep learning (artificial intelligence);health care;medical computing;medical information systems;video signal processing","deep learning;patient-physician relationship;primary care visits;patient physician;nonverbal behaviors;patient outcomes;gaze analysis;automated analysis;physician-patient interaction;deep convolutional neural network;transfer learning;build robust model;convolutional layers;sample frames;clinical interaction;robust physician gaze prediction;time 3.0 min","","","","29","","16 Dec 2020","","","IEEE","IEEE Conferences"
"A Robust Driver’s Gaze Zone Classification using a Single Camera for Self-occlusions and Non-aligned Head and Eyes Direction Driving Situations","C. Lollett; H. Hayashi; M. Kamezaki; S. Sugano","Waseda University,Graduate School of Creative Science and Engineering,Department of Modern Mechanical Engineering,Tokyo,Japan,162-0044; Waseda University,Graduate School of Creative Science and Engineering,Department of Modern Mechanical Engineering,Tokyo,Japan,162-0044; Waseda University,Research Institute for Science and Engineering (RISE),Tokyo,Japan,162-0044; Waseda University,Department of Modern Mechanical Engineering,Tokyo,Japan,169-8555","2020 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","14 Dec 2020","2020","","","4302","4308","Distracted driving is one of the most common causes of traffic accidents around the world. Recognizing the driver's gaze direction during a maneuver could be an essential step for avoiding the matter mentioned above. Thus, we propose a gaze zone classification system that serves as a base of supporting systems for driver's situation awareness. However, the challenge is to estimate the driver's gaze inside not ideal scenarios, specifically in this work, scenarios where may occur self-occlusions or non-aligned head and eyes direction of the driver. Firstly, towards solving miss classifications during self-occlusions scenarios, we designed a novel protocol where a 3D full facial geometry reconstruction of the driver from a single 2D image is made using the state-of-the-art method PRNet. To solve the miss classification when the driver's head and eyes direction are not aligned, eyes and head information are extracted. After this, based on a mix of different data pre-processing and deep learning methods, we achieved a robust classifier in situations where self-occlusions or non-aligned head and eyes direction of the driver occur. Our results from the experiments explicitly measure and show that the proposed method can make an accurate classification for the two before-mentioned problems. Moreover, we demonstrate that our model generalizes new drivers while being a portable and extensible system, making it easy-adaptable for various automobiles.","2577-1655","978-1-7281-8526-2","10.1109/SMC42975.2020.9283470","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283470","","Geometry;Head;Three-dimensional displays;Protocols;Two dimensional displays;Image reconstruction;Vehicles","deep learning (artificial intelligence);gaze tracking;image classification;road accidents;road safety;road traffic;traffic engineering computing","distracted driving;gaze zone classification system;self-occlusions scenarios;single 2D image;PRNet;robust driver gaze zone classification;non-aligned head and eyes direction driving situations;traffic accidents;3D full facial geometry reconstruction;deep learning methods","","","","26","","14 Dec 2020","","","IEEE","IEEE Conferences"
"Eye Gaze Analysis of Students in Educational Systems","P. -E. Aivaliotis; F. Grivokostopoulou; I. Perikos; I. Daramouskas; I. Hatziligeroudis","University of Patras,Department of Computer Engineering & Informatics,Patras,Greece; University of Patras,Department of Computer Engineering & Informatics,Patras,Greece; University of Patras,Department of Computer Engineering & Informatics,Patras,Greece; University of Patras,Department of Computer Engineering & Informatics,Patras,Greece; University of Patras,Department of Computer Engineering & Informatics,Patras,Greece","2020 11th International Conference on Information, Intelligence, Systems and Applications (IISA","11 Dec 2020","2020","","","1","6","Eye gaze provides indicative information about the status and the behavior of a person and can be very assistive in human-computer interaction. Eye-gaze analysis is very helpful in a variety of applications in order to understand the interest of the users, their behavior or even to unveil distractions. However, the accurate eye-gaze estimation is a very challenging process. In this paper, we present an eye gaze estimation work that relies on convolutional neural networks which imitate the LeNet's architecture. They analyze eye gaze and provide a 2D vector that concerns the coordinates of the specific pixel inside the 2D screen's space, in which the user is looking at. Also, a system capable of working under various real-world conditions such as light, angle and distance differentiations was designed and developed. An evaluation study was performed and the results are quite promising pointing out that the system is scalable and accurate in estimating the eye gaze of the users.","","978-1-6654-2228-4","10.1109/IISA50023.2020.9284374","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284374","Human-Computer Interaction;Eye Gaze estimation;Convolutional Neural Networks;Deep Learning","Human computer interaction;Two dimensional displays;Estimation;Computer architecture;Convolutional neural networks;Faces","computer aided instruction;eye;feature extraction;gaze tracking;human computer interaction;neural nets;video signal processing","eye gaze estimation work;eye-gaze analysis;educational systems;human-computer interaction;LeNet architecture","","","","16","","11 Dec 2020","","","IEEE","IEEE Conferences"
"An Analysis ofDriver Cognitive Distraction","D. Jia","Sichuan University,College of Computer Science,Chengdu,China","2020 International Conference on Computing and Data Science (CDS)","9 Dec 2020","2020","","","334","337","Driver distraction detection is a very important problem today. There are a large number of accidents occur every year. In this paper, we give some example of a variety of driver distraction detection methods and present an organized review. Our discussion focuses on the characteristics of driver distraction detection methods and the comparison of advantages and disadvantages, so that readers can quickly understand the mainstream methods in this field. We also propose some opinions on the field of distraction detection.","","978-1-7281-7106-7","10.1109/CDS49703.2020.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9275937","driver distraction detection;head pose;visual focus;gaze estimation","6G mobile communication;Data science","cognition;driver information systems;object detection","driver distraction detection methods;driver cognitive distraction analysis","","","","25","","9 Dec 2020","","","IEEE","IEEE Conferences"
"Vision Based Communicator","A. Kabra; C. Agrawal; H. Pallab Jyoti Dutta; M. K. Bhuyan; R. H. Laskar","IIT Guwahati,Department of Electronics and Electrical Engineering,Assam,India,781039; IIT Guwahati,Department of Electronics and Electrical Engineering,Assam,India,781039; IIT Guwahati,Department of Electronics and Electrical Engineering,Assam,India,781039; IIT Guwahati,Department of Electronics and Electrical Engineering,Assam,India,781039; NIT Silchar,Department of Electronics and Communication Engineering,Assam,India,788010","2020 IEEE Applied Signal Processing Conference (ASPCON)","7 Dec 2020","2020","","","293","297","This work provides a vision-based human-computer interface. The interface senses and interprets eye-blinks as controls along with gaze detection methods. We used image processing methods like haar-like features for automatic face detection, followed by eye tracking and blink detection based on landmarks. The gaze detection mechanism helps in easy controlling of the mouse pointer which can help people with disabilities to communicate effectively. A heatmap is also generated so that in future developments, this data can be used to design a more efficient layout for the controls. Given the necessity to move towards online solutions, such data can cater to the needs of a variety of sectors.","","978-1-7281-6882-1","10.1109/ASPCON49795.2020.9276664","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9276664","Communicator;Computer Vision;Gaze estimation;Gaze Heatmap","Faces;Feature extraction;Training;Gaze tracking;Paralysis;Iris;Estimation","computer vision;eye;face recognition;handicapped aids;human computer interaction;object detection","interface senses;interprets eye-blinks;gaze detection methods;automatic face detection;eye tracking;blink detection;gaze detection mechanism;easy controlling;mouse pointer;vision based communicator;vision-based human-computer interface","","","","14","","7 Dec 2020","","","IEEE","IEEE Conferences"
"Accuracy Improvement of Object Selection in Gaze Gesture Application using Deep Learning","M. Alfaroby E.; S. Wibirama; I. Ardiyanto","Universitas Gadjah Mada,Department of Electrical and Information Engineering,Yogyakarta,Indonesia,55281; Universitas Gadjah Mada,Department of Electrical and Information Engineering,Yogyakarta,Indonesia,55281; Universitas Gadjah Mada,Department of Electrical and Information Engineering,Yogyakarta,Indonesia,55281","2020 12th International Conference on Information Technology and Electrical Engineering (ICITEE)","1 Dec 2020","2020","","","307","311","Gaze-based interaction is a crucial research area. Gaze gesture provides faster interaction between a user and a computer application because people naturally look at the object of interest before taking any other actions. Spontaneous gaze-gesture-based application uses gaze-gesture as an input modality without performing any calibration. The conventional eye tracking systems have a problem with low accuracy. In general, data captured by eye tracker contains errors and noise within gaze position signal. The errors and noise affect the performance of object selection in gaze gesture based application that controls digital contents on the display using smooth-pursuit eye movement. The conventional object selection method suffers from low accuracy (<; 80%). In this paper, we addressed this accuracy problem with a novel approach using deep learning. We exploited deep learning power to recognize the pattern of eye-gaze data. Long Short Term Memory (LSTM) is a deep learning architecture based on recurrent neural network (RNN). We used LSTM to perform object selection task. The dataset consisted of 34 participants taken from previous study of object selection technique of gaze gesture-based application. Our experimental results show that the proposed method achieved 96.17% of accuracy. In future, our result may be used as a guidance for developing gaze gesture application.","","978-1-7281-1097-4","10.1109/ICITEE49829.2020.9271771","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271771","Eye Tracking;Gaze Gesture;Deep Learning;LSTM","Deep learning;Gaze tracking;Training;Computer architecture;Tracking;Logic gates;Testing","eye;gaze tracking;gesture recognition;human computer interaction;learning (artificial intelligence);recurrent neural nets","deep learning power;eye-gaze data;deep learning architecture;object selection task;object selection technique;gaze gesture-based application;gaze gesture application;gaze-based interaction;computer application;spontaneous gaze-gesture-based application;conventional eye tracking systems;gaze position signal;smooth-pursuit eye movement;conventional object selection method;recurrent neural network;long short term memory;LSTM","","","","18","","1 Dec 2020","","","IEEE","IEEE Conferences"
"Multiscale and Nonlinearility Convolutional Regression for Locating the Eye’s Pupil Center","P. Lertsiravarameth; P. Taeprasartsit","Silpakorn University,Faculty of Science,Department of Computing,Nakhon Pathom,Thailand; Silpakorn University,Faculty of Science,Department of Computing,Nakhon Pathom,Thailand","2020 17th International Joint Conference on Computer Science and Software Engineering (JCSSE)","30 Nov 2020","2020","","","47","52","Pupil detection has been an active research topic in recent years due to its usability in many areas including interaction design and medical diagnosis. Many existing methods are accurate in pupil localization; however, many of them were designed and tested under specialized lighting conditions. Furthermore, there are challenges to cope with images whose color of the iris and the pupil are both dark. In this work, we proposed deep regression models to locate the eye's pupil center in target groups whose iris and pupil are both dark in color and images were acquired in a typical lighting condition. The proposed neural network model concurrently utilizes features from multiple convolutional layers for final regression. In other words, features from different layers which contain different degrees of nonlinearities are concatenated so that a neural network model can explicitly employ these features together for predicting the location of the eye's pupil. Ablation analysis indicated that these model characteristics were essential for robustness. All experimented models were trained and validated against 2,500 eye images from the MPIIGaze dataset. Additional 2,515 test images were obtained from the MMU2 and BioID datasets to evaluate robustness of the method. Our experiments showed that robust and accurate localization of the pupil center could be achieved over datasets whose image were acquired under non-specialized lighting conditions. Our best model had average pixel error of 1.11 pixels, and had error within two pixels for 95.5 percent of test samples.","2642-6579","978-1-7281-6167-9","10.1109/JCSSE49651.2020.9268375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268375","pupil localization;deep learning;data augmentation;convolutional neural network","Pupils;Monitoring;Image color analysis;Iris;Measurement;Lighting;Statistics","computer vision;convolution;convolutional neural nets;eye;gaze tracking;regression analysis","specialized lighting conditions;iris;deep regression models;lighting condition;neural network model;multiple convolutional layers;model characteristics;eye images;test images;robust localization;pupil center;nonspecialized lighting conditions;pupil detection;interaction design;medical diagnosis;pupil localization;efficiency 95.5 percent","","","","17","","30 Nov 2020","","","IEEE","IEEE Conferences"
"Classification of Motor Imagery EEG Signals Using Machine Learning","A. Abdeltawab; A. Ahmad","School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia (UTM),Johor,Malaysia; School of Electrical Engineering, Faculty of Engineering, Universiti Teknologi Malaysia (UTM),Johor,Malaysia","2020 IEEE 10th International Conference on System Engineering and Technology (ICSET)","27 Nov 2020","2020","","","196","201","Brain Computer Interface (BCI) is a term that was first introduced by Jacques Vidal in the 1970s when he created a system that can determine the human eye gaze direction, making the system able to determine the direction a person want to go or move something to using scalp-recorded visual evoked potential (VEP) over the visual cortex. Ever since that time, many researchers where captivated by the huge potential and list of possibilities that can be achieved if simply a digital machine can interpret human thoughts. In this work, we explore electroencephalography (EEG) signal classification, specifically for motor imagery (MI) tasks. Classification of MI tasks can be carried out by using machine learning and deep learning models, yet there is a trade between accuracy and computation time that needs to be maintained. The objective is to create a machine learning model that can be optimized for real-time classification while having a relatively acceptable classification accuracy. The proposed model relies on common spatial patter (CSP) for feature extraction as well as linear discriminant analysis (LDA) for classification. With simple pre-processing stage and a proper selection of data for training the model proved to have a balanced accuracy of +80% while maintaining small run-time (milliseconds) that is opted for real-time classifications.","2470-640X","978-1-7281-9910-8","10.1109/ICSET51301.2020.9265364","Ministry of Higher Education of Malaysia(grant numbers:Q.J130000.2451.04G96); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9265364","Motor Imagery Classification;EEG;Machine learning;Linear Discriminant Analysis (LDA);Common Spatial Pattern (CSP)","Electroencephalography;Feature extraction;Brain modeling;Task analysis;Support vector machines;Data models;Training","brain-computer interfaces;electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing;signal classification;visual evoked potentials","motor imagery EEG signals;brain computer interface;BCI;human eye gaze direction;scalp-recorded visual evoked potential;visual cortex;motor imagery tasks;deep learning models;machine learning model;electroencephalography;EEG signal classification;common spatial patter;feature extraction;linear discriminant analysis","","","","18","","27 Nov 2020","","","IEEE","IEEE Conferences"
"Estimation of Person-Specific Visual Attention via Selection of Similar Persons","Y. Moroto; K. Maeda; T. Ogawa; M. Haseyama","Graduate School of Information Science and Technology, Hokkaido University,Sapporo,Hokkaido,Japan,060-0814; Office of Institutional Research, Hokkaido University,Sapporo,Hokkaido,Japan,060-0808; Hokkaido University,Faculty of Information Science and Technology,Sapporo,Hokkaido,Japan,060-0814; Hokkaido University,Faculty of Information Science and Technology,Sapporo,Hokkaido,Japan,060-0814","2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)","23 Nov 2020","2020","","","1","2","This paper presents a method for estimation of person-specific visual attention based on estimated similar persons' visual attention. For improving the estimation performance of person-specific visual attention, the proposed method uses the dataset including the large number of images and corresponding gaze data of many persons not including the target person and trains an estimation model based on deep learning. By using the estimated visual attention of similar persons for the target image, the proposed method estimates the visual attention of the target person with the small amount of gaze data. Experimental results show that the proposed method is effective for estimation of person-specific visual attention.","2575-8284","978-1-7281-7399-3","10.1109/ICCE-Taiwan49838.2020.9258340","MIC/SCOPE(grant numbers:181601001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9258340","","Visualization;Estimation;Pattern analysis;Consumer electronics;Training data;Training;Information science","feature selection;gaze tracking;learning (artificial intelligence);neural nets;pose estimation;visual perception","target person;person-specific visual attention estimation;similar person estimation;similar person selection;gaze data;deep learning","","","","11","","23 Nov 2020","","","IEEE","IEEE Conferences"
"The Eye Tracking and Gaze Estimation System by Low Cost Wearable Devices","K. F. Lee; Y. L. Chen; C. W. Yu; C. H. Wu","National Taipei University of Technology,Taipei,Taiwan; National Taipei University of Technology,Taipei,Taiwan; National Taipei University of Technology,Taipei,Taiwan; National Taipei University of Technology,Taipei,Taiwan","2020 IEEE International Conference on Consumer Electronics - Taiwan (ICCE-Taiwan)","23 Nov 2020","2020","","","1","2","This study develop a wearable eye tracking and gaze estimation low cost devices. This devices use infrared camera and design by integration of elastic mechanism adaptable. The use of cheap endoscope camera for mobile and 3D print technique for building up the devices, which results in be a low cost solution. This device can effectively extract and estimate pupil ellipse from few camera-captured samples of an eye and compute the corresponding 3D eye model. And this device use multiple point's calibration method to solve the related polynomial formula for future angle-to-gaze mapping. This wearable device is a low-cost which can be used for virtual reality and auxiliary equipment.","2575-8284","978-1-7281-7399-3","10.1109/ICCE-Taiwan49838.2020.9258009","Ministry of Science and Technology(grant numbers:108-2218-E-027-017,108-2221-E-027-066); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9258009","","Tracking;Solid modeling;Pupils;Iris recognition;Light sources;Iris;Image color analysis","cameras;endoscopes;eye;gaze tracking;image capture;infrared imaging;mobile computing;polynomials;shape recognition","angle-to-gaze mapping;low cost wearable devices;wearable eye tracking devices;gaze estimation;infrared camera;elastic mechanism;endoscope camera;3D print technique;camera-captured samples;pupil ellipse estimation;3D eye model;multiple points calibration method;polynomial formula","","","","7","","23 Nov 2020","","","IEEE","IEEE Conferences"
"Personalized Course Recommendation Based on Eye-Tracking Technology and Deep Learning","Q. Chen; X. Yu; N. Liu; X. Yuan; Z. Wang","Shandong Normal University,School of Information Science and Engineering,Jinan,China; Shandong Normal University,School of Information Science and Engineering,Jinan,China; Shandong Normal University,School of Information Science and Engineering,Jinan,China; Shandong Normal University,School of Information Science and Engineering,Jinan,China; Qufu Normal University,School of Information Science and Engineering,Qufu,China","2020 IEEE 7th International Conference on Data Science and Advanced Analytics (DSAA)","20 Nov 2020","2020","","","692","968","With the rapid development of online courses, the requirements of personalized course recommendation have been increasing. The traditional collaborative filtering algorithm confronts with the challenge of cold start, which is difficult to settle on online course recommendation effectively. In this paper, we propose a novel click through rate (CTR) model for personalized online course recommendation, with discriminative user features, item features and cross features. The feature representation ability of the CTR model is improved and the serious challenge of cold start is alleviated. Furthermore, transfer learning is introduced to deal with the problem of insufficient data in models training. More specially, eye tracking technology is applied to capture the users' cognitive styles, which are visualized with the heat map and fixation point trajectory. Finally, the recommendation interface sent to the learners, according to the user's cognitive style. The experiments show that the novel CTR model improves the performance of the personalized online course recommendation.","","978-1-7281-8206-3","10.1109/DSAA49011.2020.00079","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9260031","personalization;course recommendation;deep transfer learning;CTR prediction","Frequency modulation;Data models;Training;Adaptation models;Feature extraction;Correlation;Sparse matrices","collaborative filtering;computer aided instruction;educational courses;gaze tracking;learning (artificial intelligence);recommender systems","novel CTR model;personalized online course recommendation;eye-tracking technology;traditional collaborative filtering algorithm;cold start;rate model;discriminative user features;item features;cross features;feature representation ability;recommendation interface;click through rate model;transfer learning;model training;user cognitive styles;heat map;fixation point trajectory","","","","20","","20 Nov 2020","","","IEEE","IEEE Conferences"
"Improving Driver Gaze Prediction with Reinforced Attention","K. Lv; H. Sheng; Z. Xiong; W. Li; L. Zheng","school of computer science and engineering, Beihang University, 12633 Beijing, China, 100083 (e-mail: lvkai@buaa.edu.cn); School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, 100083 (e-mail: shenghao@buaa.edu.cn); School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, (e-mail: xiongz@buaa.edu.cn); School of Computer Science and Engineering, Beihang University, 12633 Beijing, China, (e-mail: liwei@nlsde.buaa.edu.cn); Research School of Computer Science, Australian National University, Canberra, Australia, (e-mail: liang.zheng@anu.edu.au)","IEEE Transactions on Multimedia","","2020","PP","99","1","1","We consider the task of driver gaze prediction: estimating where the location of the focus of a driver should be, based on a raw video of the outside environment. In practice, we output a probability map that gives the normalized probability of each point in a given scene being the object of the driver attention. Most existing methods (i.e., Coarse-to-Fine and Multi-branch) take an image or a video as input and directly output the fixation map. While successful, these methods can often produce highly scattered predictions, rendering them unreliable for real-world usage. Motivated by this observation, we propose the reinforced attention (RA) model as a regulatory mechanism to increase prediction density. Our method is built directly on top of existing methods, making it complementary to current approaches. Specifically, we first use Multi-branch to obtain an initial fixation map. Then, RA is trained using deep reinforcement learning to learn a location prediction policy, producing a reinforced attention. Finally, in order to obtain the final gaze prediction result, we combine the fixation map and the reinforced attention by a mask-guided multiplication. Experimental results show that our framework improves the accuracy of gaze prediction, and provides state-of-the-art performance on the DR(eye)VE dataset.","1941-0077","","10.1109/TMM.2020.3038311","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9262067","gaze prediction;driver attention;reinforcement learning;video processing;deep learning","Task analysis;Computational modeling;Vehicles;Predictive models;Hidden Markov models;Semantics;Computer architecture","","","","1","","","CCBY","17 Nov 2020","","","IEEE","IEEE Early Access Articles"
"Individual Eye Gaze Prediction with the Effect of Image Enhancement Using Deep Neural Networks","O. P. Olawale; K. Dimililer","Near East University,Faculty of Engineering, Research Center for AI and IoT,Software Engineering Department,Nicosia Cyprus via Mersin 10,Turkey; Near East University,Faculty of Engineering, Applied Artificial Intelligence Research Center,Electrical and Electronic Engineering Department,Nicosia Cyprus via Mersin 10,Turkey","2020 4th International Symposium on Multidisciplinary Studies and Innovative Technologies (ISMSIT)","17 Nov 2020","2020","","","1","7","The prediction of individual eye gaze is a research topic that has gained the interest of researchers with its wide range of applications because neural networks majorly increase the rate of accuracy of individual gaze. In this research work, MPIIGaze dataset has been employed for the prediction of individual gaze and the direction of individual gaze was grouped into down view, left view, right view and lastly centre view. A CNN model was used to train and validate a random selection of images. Firstly, the ordinary images were trained and validated, after which image enhancement processing technique was applied. With the image brightness enhancement technique, a higher rate of gaze prediction accuracy was achieved. Hence, it can be deduced that image enhancement has proved its purpose by providing image interpretation with better quality.","","978-1-7281-9090-7","10.1109/ISMSIT50672.2020.9254786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9254786","Gaze detection;gaze direction;individual eyes;image enhancement;deep learning","Deep learning;Computational modeling;Neural networks;Brightness;Predictive models;Prediction algorithms;Image enhancement","computer vision;convolutional neural nets;eye;feature extraction;gaze tracking;image enhancement;learning (artificial intelligence);motion estimation","image brightness enhancement;image interpretation;individual eye gaze prediction;deep neural networks;MPIIGaze dataset;CNN model;image selection;deep learning;eye movement","","1","","39","","17 Nov 2020","","","IEEE","IEEE Conferences"
"Interpretation of Depression Detection Models via Feature Selection Methods","S. M. Alghowinem; T. Gedeon; R. Goecke; J. Cohn; G. Parker","Media Lab, Massachusetts Institute of Technology, 2167 Cambridge, Massachusetts, United States, (e-mail: sharifah@mit.edu); College of Engineering and Computer Science, Australian National University, 2219 Canberra, Australian Capital Territory, Australia, (e-mail: tom.gedeon@anu.edu.au); Faculty of Education, Science, Technology and Mathematics, University of Canberra, 2234 Canberra, Australian Capital Territory, Australia, (e-mail: roland.goecke@ieee.org); Department of Psychology, University of Pittsburgh, 6614 Pittsburgh, Pennsylvania, United States, (e-mail: jeffcohn@pitt.edu); School of Psychiatry, University of New South Wales, 7800 Sydney, New South Wales, Australia, (e-mail: g.parker@unsw.edu.au)","IEEE Transactions on Affective Computing","","2020","PP","99","1","1","Given the prevalence of depression worldwide and its major impact on society, several studies employed artificial intelligence modelling to automatically detect and assess depression. However, interpretation of these models and cues are rarely discussed in detail in the AI community, but have received increased attention lately. In this study, we aim to analyse the commonly selected features using a proposed framework of several feature selection methods and their effect on the classification results, which will provide an interpretation of the depression detection model. The developed framework aggregates and selects the most promising features for modelling depression detection from 38 feature selection algorithms of different categories. Using three real-world depression datasets, 902 behavioural cues were extracted from speech behaviour, speech prosody, eye movement and head pose. To verify the generalisability of the proposed framework, we applied the entire process to depression datasets individually and when combined. The results from the proposed framework showed that speech behaviour features (e.g. pauses) are the most distinctive features of the depression detection model. From the speech prosody modality, the strongest feature groups were F0, HNR, formants, and MFCC, while for the eye activity modality they were left-right eye movement and gaze direction, and for the head modality it was yaw head movement. Modelling depression detection using the selected features (even though there are only 9 features) outperformed using all features in all the individual and combined datasets. Our feature selection framework did not only provide an interpretation of the model, but was also able to produce a higher accuracy of depression detection with a small number of features in varied datasets. This could help to reduce the processing time needed to extract features and creating the model.","1949-3045","","10.1109/TAFFC.2020.3035535","National Institutes of Health(grant numbers:MH51435, MH65376, and MH096951); Australian Research Councils Discovery Projects(grant numbers:DP190101294); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253541","depression detection;multimodal analysis;feature selection;datasets generalisation","Feature extraction;Depression;Biological system modeling;Deep learning;Analytical models;Australia;Stability analysis","","","","","","","IEEE","10 Nov 2020","","","IEEE","IEEE Early Access Articles"
"Towards Robust Auto-calibration for Head-mounted Gaze Tracking Systems","M. Liu; Y. F. Li; H. Liu","City University of Hong Kong,Department of Mechanical Engineering,Kowloon,Hong Kong; City University of Hong Kong,Department of Mechanical Engineering,Kowloon,Hong Kong; Central China Normal University,National Engineering Research Center for E-Learning,Wuhan,Hubei Province,China","2020 IEEE International Conference on Mechatronics and Automation (ICMA)","26 Oct 2020","2020","","","588","593","Removing explicit user calibration is indeed an appealing goal for gaze tracking systems. In this paper, a novel auto-calibration method is proposed to achieve the 3D point of regard (PoR) prediction for the head-mounted gaze tracker. Our method chooses an RGBD sensor as the scene camera to capture 3D structures of the environment and treats salient regions as possible 3D calibration targets. In order to improve efficiency, the bag of words (BoW) algorithm is applied to calculate scene images' similarity and eliminate redundant maps. After elimination, the translation relationship between eye cameras and the scene camera can be determined by uniting calibration targets with gaze vectors, and 3D gaze points are obtained by transformed gaze vectors and the point cloud of environment. The experiment results indicate that our method achieves effective performance on 3D gaze estimation for head-mounted gaze trackers, which can promote engineering applications of human-computer interaction technology in many areas.","2152-744X","978-1-7281-6416-8","10.1109/ICMA49215.2020.9233571","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233571","head-mounted gaze tracker;auto-calibration;3D gaze estimation;human-computer interaction","Three-dimensional displays;Target tracking;Estimation;Gaze tracking;Cameras;Prediction algorithms;Calibration","calibration;cameras;gaze tracking;image colour analysis;image sensors;vectors","head-mounted gaze tracking systems;explicit user calibration;autocalibration method;head-mounted gaze tracker;scene camera;calibration targets;eye cameras;3D gaze point estimation;gaze vector transformation;3D point of regard prediction;3D PoR prediction;RGBD sensor;bag of words algorithm;BoW algorithm;human-computer interaction technology","","","","20","","26 Oct 2020","","","IEEE","IEEE Conferences"
"Estimating a Driver's Gaze Point by a Remote Spherical Camera","Z. Zhao; S. Li; T. Kosaki","Hiroshima City Universityd,Department of Systems Engineering,Hiroshima,JAPAN; Hiroshima City Universityd,Department of Systems Engineering,Hiroshima,JAPAN; Hiroshima City Universityd,Department of Systems Engineering,Hiroshima,JAPAN","2020 IEEE International Conference on Mechatronics and Automation (ICMA)","26 Oct 2020","2020","","","599","604","In this paper we propose a novel method of detecting a driver's gaze point by a single remote spherical camera. A spherical camera is fixed at the upside of the windshield of a vehicle so that it can observes outside scenes and an inside driver simultaneously. We propose a simplified model of a driver's gaze point detection under this setting-up by approximating the gaze direction vector as that starting from the viewpoint of the spherical camera by omitting the distance of the driver from the spherical camera. Furthermore, two algorithms are proposed and evaluated: one is gaze point estimation based on the geometric model using OpenFace [1], and the other is to use a neural network, which uses the gaze vector and head pose parameters obtained from OpenFace as input of the neural network, to estimate the gaze point in the spherical image. As shown in the experimental results, using a neural network to compensate the measuring errors of OpenFace can achieves a much better result.","2152-744X","978-1-7281-6416-8","10.1109/ICMA49215.2020.9233793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9233793","Gaze Estimation;Spherical Camera","","cameras;driver information systems;gaze tracking;neural nets;pose estimation","neural network;gaze vector;spherical image;driver;single remote spherical camera;gaze direction vector;gaze point estimation;vehicle windshield;head pose parameters;OpenFace","","","","12","","26 Oct 2020","","","IEEE","IEEE Conferences"
"A Simple Model of Reading Eye Movement Based on Deep Learning","Y. Wang; X. Wang; Y. Wu","Graduate School, Xi’an International Studies University, Xi’an, China; Graduate School, Xi’an International Studies University, Xi’an, China; Graduate School, Xi’an International Studies University, Xi’an, China","IEEE Access","2 Nov 2020","2020","8","","193757","193767","At present, an increasing amount of research is being conducted on human cognitive behaviour, and reading eye-movement modelling is a research hotspot in cognitive linguistics. However, existing reading eye-movement models are complicated and require a large number of hand-crafted features. To address these issues, this paper improves upon the fixation granularity processing mode and the regression processing mode of the traditional reading eye-movement models and proposes a reading eye-movement fixation sequence labelling method to construct a simpler model. The proposed model is based on a multi-input deep-learning neural network, which takes advantage of deep learning to reduce the number of required hand-crafted features and integrates knowledge from the field of cognitive psychology to increase its accuracy. To meet the data-size requirements of the deep-learning model, this paper also proposes a reading eye-movement data augmentation method. The experimental results show that the proposed method can describe the actual process of reading eye-movement intuitively and that the simple reading eye-movement models based on this method can obtain a similar accuracy with existing models by using fewer hand-crafted features.","2169-3536","","10.1109/ACCESS.2020.3033382","Social Science Foundation of Shaanxi Province(grant numbers:2019M001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9238389","Eye-movement model;deep learning;sequence labelling;cognitive computing","Hidden Markov models;Computational modeling;Predictive models;Data models;Labeling;Task analysis;Psychology","cognition;eye;gaze tracking;learning (artificial intelligence);neural nets;psychology","data-size requirements;reading eye-movement data augmentation method;eye movement;deep learning;human cognitive behaviour;cognitive linguistics;fixation granularity processing mode;regression processing mode;reading eye-movement fixation sequence labelling method;deep-learning neural network;cognitive psychology","","","","36","CCBY","23 Oct 2020","","","IEEE","IEEE Journals"
"NeuroIV: Neuromorphic Vision Meets Intelligent Vehicle Towards Safe Driving With a New Database and Baseline Evaluations","G. Chen; F. Wang; W. Li; L. Hong; J. Conradt; J. Chen; Z. Zhang; Y. Lu; A. Knoll","School of Automotive Studies, Tongji University, Shanghai 200092, China, also with the State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body, Changsha 410012, China, and also with the Department of Informatics, Technical University of Munich, 80333 Munich, Germany (e-mail: tj_autodrive@hotmail.com); School of Automotive Studies, Tongji University, Shanghai 200092, China.; School of Automotive Studies, Tongji University, Shanghai 200092, China.; School of Transportation, Shandong University of Science and Technology, Qingdao 266510, China.; Department of Computational Science and Technology, KTH Royal Institute of Technology, 114 28 Stockholm, Sweden.; School of Automotive Studies, Tongji University, Shanghai 200092, China.; School of Automotive Studies, Tongji University, Shanghai 200092, China.; School of Automotive Studies, Tongji University, Shanghai 200092, China.; Department of Informatics, Technical University of Munich, 80333 Munich, Germany.","IEEE Transactions on Intelligent Transportation Systems","","2020","PP","99","1","13","Neuromorphic vision sensors such as the Dynamic and Active-pixel Vision Sensor (DAVIS) using silicon retina are inspired by biological vision, they generate streams of asynchronous events to indicate local log-intensity brightness changes. Their properties of high temporal resolution, low-bandwidth, lightweight computation, and low-latency make them a good fit for many applications of motion perception in the intelligent vehicle. However, as a younger and smaller research field compared to classical computer vision, neuromorphic vision is rarely connected with the intelligent vehicle. For this purpose, we present three novel datasets recorded with DAVIS sensors and depth sensor for the distracted driving research and focus on driver drowsiness detection, driver gaze-zone recognition, and driver hand-gesture recognition. To facilitate the comparison with classical computer vision, we record the RGB, depth and infrared data with a depth sensor simultaneously. The total volume of this dataset has 27360 samples. To unlock the potential of neuromorphic vision on the intelligent vehicle, we utilize three popular event-encoding methods to convert asynchronous event slices to event-frames and adapt state-of-the-art convolutional architectures to extensively evaluate their performances on this dataset. Together with qualitative and quantitative results, this work provides a new database and baseline evaluations named NeuroIV in cross-cutting areas of neuromorphic vision and intelligent vehicle.","1558-0016","","10.1109/TITS.2020.3022921","National Natural Science Foundation of China(grant numbers:61906138); State Key Laboratory of Advanced Design and Manufacturing for Vehicle Body Open Project(grant numbers:31815005); European Unions Horizon 2020 Framework Programme for Research and Innovation Human Brain Project SGA3(grant numbers:945539); Shanghai AI Innovation Development Program 2018; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234108","Neuromorphic vision;distracted driving;advanced driver assistance system;database and baseline evaluations;event encoding;deep learning.","Neuromorphics;Vision sensors;Intelligent sensors;Intelligent vehicles;Cameras","","","","1","","","IEEE","20 Oct 2020","","","IEEE","IEEE Early Access Articles"
"Do You Move Unconsciously? Accuracy of Social Distance and Line of Sight Between a Virtual Robot Head and Humans","T. Minegishi; H. Osawa",University of Tsukuba; University of Tsukuba,"2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)","14 Oct 2020","2020","","","503","508","In this paper, we examine the effectiveness of the social distance between a virtual agent and users, and the gaze instruction using a display that can be viewed stereoscopically without using any wearable devices. An actual robot cannot always maintain an appropriate interpersonal distance, through nonverbal gestures owing to its limited range of motion. Because large movements may harm humans, the nonverbal gestures of robots are limited in the real world. In this work, 14 participants were asked how far they wanted to move from a robot posing as a museum guide agent. They were also asked to identify the point at which they felt the agent was gazing. There was a significant distance between the initial position and the position to which the participants moved in the first task under two-dimensional (2D) and three-dimensional (3D) scenarios. The participants moved a significant distance in the first task. In the gaze estimation task, however, the error between the 3D and 2D evaluations was significantly lesser, at a point far from the agent.","1944-9437","978-1-7281-6075-7","10.1109/RO-MAN47096.2020.9223506","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9223506","","","gesture recognition;human computer interaction;human-robot interaction;mobile robots;museums;virtual reality","gaze estimation task;museum guide agent;nonverbal gestures;appropriate interpersonal distance;actual robot;wearable devices;gaze instruction;virtual agent;virtual robot head;social distance","","","","13","","14 Oct 2020","","","IEEE","IEEE Conferences"
"Euclidean Distance based Loss Function for Eye-Gaze Estimation","B. Sung Lee; R. Phattharaphon; S. Yean; J. Liu; M. Shakya","Nanyang Technological University,School of Computer Science and Engineering,Singapore; Kasetsart University,Department of Computer Engineering,Thailand; Nanyang Technological University,School of Computer Science and Engineering,Singapore; Nanyang Technological University,School of Computer Science and Engineering,Singapore; Nanyang Technological University,School of Computer Science and Engineering,Singapore","2020 IEEE Sensors Applications Symposium (SAS)","12 Oct 2020","2020","","","1","5","The following topics are dealt with: Internet of Things; wireless sensor networks; sensor fusion; microsensors; diseases; vibrations; microfabrication; cameras; learning (artificial intelligence); and convolutional neural nets.","","978-1-7281-4842-7","10.1109/SAS48726.2020.9220051","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9220051","Eye Gaze;Loss function;Cross-entropy;Euclidean distance;Convolutional Neural Network","","diseases;Internet of Things;microsensors;sensor fusion;wireless sensor networks","Internet of Things;wireless sensor networks;sensor fusion;microsensors;diseases;vibrations;microfabrication;cameras;learning (artificial intelligence);convolutional neural nets","","1","","17","","12 Oct 2020","","","IEEE","IEEE Conferences"
"Cornea Radius Calibration for Remote 3D Gaze Tracking Systems","J. Chi; D. Wang; N. Lu; Z. Wang","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Access","21 Oct 2020","2020","8","","187634","187647","Cornea radius estimation is a key technique for 3D gaze estimation in the single-camera 3D gaze tracking system. Traditional methods with one-camera-one-light-source systems or one-camera-two-light-source systems cannot achieve 3D gaze estimation. The 3D line-of-sight can be estimated only when the cornea radius is pre-calibrated by the user. A cornea radius calibration method based on the iris radius is proposed in this paper for 3D gaze estimation in remote one-camera-two-light-source systems. We first calibrate the iris radius based on the binocular strategy, estimate the spatial iris center using the calibrated iris radius, and then calibrate the cornea radius by a set of non-linear equations under the constraint of equivalent distances from the cornea center to the iris edge points. The calibrated cornea radius is verified by binocular optimization constraints. Simulations and physical experiments validate the effectiveness of the proposed method. The iris-based cornea radius calibration approach is novel; it can be used to obtain the cornea radius and 3D gaze using remote one-camera-one-light-source or one-camera-multi-light-source systems.","2169-3536","","10.1109/ACCESS.2020.3029300","Key Research and Development project (Institute of Psychology) of the Ministry of Science and Technology(grant numbers:2018YFC2001700); Fundamental Research Funds for the Central Universities(grant numbers:FRF-GF-20-04A); Scientific and Technological Innovation Foundation of Shunde Graduate School, USTB; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9215978","Cornea radius;corneal reflections;optimization constraints;3D line-of-sight estimation;gaze tracking;user calibration","Cornea;Three-dimensional displays;Gaze tracking;Estimation;Calibration;Cameras;Light sources","calibration;cameras;edge detection;eye;feature extraction;gaze tracking;iris recognition;optimisation;stereo image processing","remote 3D gaze tracking systems;cornea radius estimation;3D gaze estimation;single-camera 3D gaze tracking system;one-camera-one-light-source systems;one-camera-two-light-source systems;3D line-of-sight;cornea radius calibration method;calibrated iris radius;cornea center;calibrated cornea radius;iris-based cornea radius calibration approach;one-camera-multilight-source systems","","","","28","CCBY","7 Oct 2020","","","IEEE","IEEE Journals"
"End-to-end Off-angle Iris Recognition Using CNN Based Iris Segmentation","E. Jalilian; M. Karakaya; A. Uhl","University of Salzburg,Department of Computer Science,Salzburg,Austria; Kennesaw State University,Department of Computer Science,Georgia,USA; University of Salzburg,Department of Computer Science,Salzburg,Austria","2020 International Conference of the Biometrics Special Interest Group (BIOSIG)","2 Oct 2020","2020","","","1","7","While deep learning techniques are increasingly becoming a tool of choice for iris segmentation, yet there is no comprehensive recognition framework dedicated for off-angle iris recognition using such modules. In this work, we investigate the effect of different gaze-angles on the CNN based off-angle iris segmentations, and their recognition performance, introducing an improvement scheme to compensate for some segmentation degradations caused by the off-angle distortions. Also, we propose an off-angle parameterization algorithm to re-project the off-angle images back to frontal view. Taking benefit of these, we further investigate if: (i) improving the segmentation outputs and/or correcting the iris images before or after the segmentation, can compensate for off-angle distortions, or (ii) the generalization capability of the network can be improved, by training it on iris images of different gaze-angles. In each experimental step, segmentation accuracy and the recognition performance are evaluated, and the results are analyzed and compared.","1617-5468","978-3-88579-700-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9210983","Off-angle iris segmentation;Off-angle iris recognition;Iris parameterization;Convolutional neural network;CNN","Iris recognition;Image segmentation;Iris;Distortion;Training;Feature extraction;Task analysis","feature extraction;image recognition;image segmentation;iris recognition;neural nets","CNN based iris segmentation;deep learning techniques;comprehensive recognition framework;different gaze-angles;off-angle iris segmentations;recognition performance;segmentation degradations;off-angle distortions;off-angle parameterization algorithm;off-angle images;segmentation outputs;iris images;segmentation accuracy;end-to-end off-angle iris recognition","","","","16","","2 Oct 2020","","","IEEE","IEEE Conferences"
"A Generative Self-Ensemble Approach To Simulated+Unsupervised Learning","Y. Mitsuzum; G. Irie; A. Kimura; A. Nakazawa",NTT Corporation; NTT Corporation; NTT Corporation; Kyoto University,"2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","2151","2155","In this paper, we consider Simulated and Unsupervised (S+U) learning which is a problem of learning from labeled synthetic and unlabeled real images. After translating the synthetic images to real ones, existing S+U learning methods use only the labeled synthetic images for training a predictor (e.g., a regression function) and ignore the target real images, which may result in unsatisfactory prediction performance. Our approach utilizes both synthetic and real images to train the predictor. The main idea of ours is to involve a self-ensemble learning framework into S+U learning. More specifically, we require the prediction results for an unlabeled real image to be consistent between “teacher” and “student” predictors, even after some perturbations are added to the image. Furthermore, aiming at generating diverse perturbations along the underlying data manifold, we introduce one-to-many image translation between synthetic and real images. Evaluation experiments on an appearance-based gaze estimation task demonstrate that the proposed ideas can improve the prediction accuracy and our full method can outperform existing S+U learning methods.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9191100","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191100","Simulated+Unsupervised Learning;SemiSupervised Learning;Image-to-Image Translation","Gallium nitride;Training;Task analysis;Estimation;Bidirectional control;Standards;Learning systems","image processing;regression analysis;unsupervised learning","generative self-ensemble approach;labeled synthetic images;unlabeled real images;target real images;student predictors;S+U learning;simulated+unsupervised learning;regression function;self-ensemble learning;teacher predictor;diverse perturbations;data manifold;one-to-many image translation;appearance-based gaze estimation","","","","15","","30 Sep 2020","","","IEEE","IEEE Conferences"
"Identifying Children with Autism Spectrum Disorder Based on Gaze-Following","Y. Fang; H. Duan; F. Shi; X. Min; G. Zhai","Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University,Shanghai,China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University,Shanghai,China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University,Shanghai,China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University,Shanghai,China; Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University,Shanghai,China","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","423","427","This paper presents a novel method to identify children with Autism Spectrum Disorder (ASD) based on the stimuli with gaze-following. Individuals with ASD are characterized by having atypical visual attention patterns, especially in social scenes. Gaze-following is considered to be a key element in understanding social scenarios, and it is reasonable to use stimuli with gaze-following to identify the children with ASD. Thus in this paper, we first construct a dataset of eye movements in gaze-following scenes for children with ASD (i.e., GazeFollow4ASD dataset), including 300 images with gaze-following information inside them and the corresponding eye movement data collected from 8 children with ASD and 10 healthy controls. We propose a novel deep neural network (DNN) model to extract discriminative features and classify children with ASD and healthy controls on single images. The proposed model shows the best performance among all compared methods on all datasets.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9190831","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9190831","Autism Spectrum Disorder identifying;visual attention;gaze-following;deep neural network","Pediatrics;Autism;Visualization;Neural networks;Psychology;Predictive models;Feature extraction","deep learning (artificial intelligence);eye;feature extraction;gaze tracking;image classification;medical disorders;medical image processing;neurophysiology;paediatrics","gaze following information;children with ASD;deep neural network;feature extraction;neurodevelopmental disorder;ASD diagnostic process;children classification;eye movement data;GazeFollow4ASD dataset;atypical visual attention patterns;autism spectrum disorder","","","","21","","30 Sep 2020","","","IEEE","IEEE Conferences"
"Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization","L. A. Zanlorensi; H. Proença; D. Menotti","Federal University of Paraná,Department of Informatics,Curitiba,Brazil; University of Beira Interior,Department of Informatics,Covilhã,Portugal; Federal University of Paraná,Department of Informatics,Curitiba,Brazil","2020 IEEE International Conference on Image Processing (ICIP)","30 Sep 2020","2020","","","1361","1365","Ocular biometric systems working in unconstrained environments usually face the problem of small within-class compactness caused by the multiple factors that jointly degrade the quality of the obtained data. In this work, we propose an attribute normalization strategy based on deep learning generative frameworks, that reduces the variability of the samples used in pairwise comparisons, without reducing their discriminability. The proposed method can be seen as a preprocessing step that contributes for data regularization and improves the recognition accuracy, being fully agnostic to the recognition strategy used. As proof of concept, we consider the “eyeglasses” and “gaze” factors, comparing the levels of performance of five different recognition methods with/without using the proposed normalization strategy. Also, we introduce a new dataset for unconstrained periocular recognition, composed of images acquired by mobile devices, particularly suited to perceive the impact of “wearing eyeglasses” in recognition effectiveness. Our experiments were performed in two different datasets, and support the usefulness of our attribute normalization scheme to improve the recognition performance.","2381-8549","978-1-7281-6395-6","10.1109/ICIP40778.2020.9191251","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191251","Periocular recognition;Biometrics;Attribute editing;Image normalization","Performance evaluation;Deep learning;Image recognition;Face recognition;Conferences;Mobile handsets","eye;face recognition;feature extraction;learning (artificial intelligence)","attribute normalization scheme;recognition performance;recognition effectiveness;different recognition methods;recognition strategy;recognition accuracy;data regularization;pairwise comparisons;deep learning generative frameworks;attribute normalization strategy;within-class compactness;unconstrained environments;ocular biometric systems;generative deep learning frameworks;unconstrained periocular recognition","","","","31","","30 Sep 2020","","","IEEE","IEEE Conferences"
"Predicting Human Errors from Gaze and Cursor Movements","R. Riad Saboundji; R. Adrian Rill","Eotvos Lorand University,Faculty of Informatics,Budapest,Hungary; Babes-Bolyai University,Faculty of Mathematics and Computer Science,Cluj-Napoca,Romania","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","8","Intelligent interfaces are increasingly integrated into diverse technological areas. In complex high-risk environments, where humans represent a crucial part of the system and their attention is often divided between simultaneous activities, imminent human errors may have serious consequences. Enhancing interfaces with predictive capabilities promotes the safe and reliable operation of such systems. In this work, we employ a data-driven approach to predict human errors in a special divided attention task involving timing constraints and requiring focused concentration and frequent shifts of attention. We performed a longitudinal study with 10 subjects, and constructed time series from the experimental data using gaze movement and mouse cursor motion features in order to classify successful and failed actions. We evaluate classical machine learning algorithms, compare them with a more traditional temporal modeling approach and a deep learning based LSTM model. Employing a leave-one- subject-out cross-validation procedure we achieve a classification accuracy of up to 86%, with LSTM presenting the highest performance. Furthermore, we investigate the trade-off between evaluation metrics and anticipation window, i.e. the time remaining until the correct action can still be performed. We conclude that prediction is feasible and accuracy and F1-score increases, despite the training dataset becoming greatly imbalanced. Investigating the anticipation window allows to understand how far in advance human errors need to be predicted in order to initiate preventive measures. Our efforts have implications for the design of predictive interfaces involving decision making under time pressure in dynamic divided attention environments.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207189","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207189","human error;time series;LSTM;gaze tracking;anticipation window;predictive interface","Task analysis;Mice;Microsoft Windows;Time series analysis;History;Predictive models;Machine learning algorithms","decision making;learning (artificial intelligence);pattern classification;recurrent neural nets;time series;user interfaces","data-driven approach;special divided attention task;timing constraints;time series;gaze movement;mouse cursor motion features;traditional temporal modeling approach;deep learning;LSTM model;classification accuracy;evaluation metrics;anticipation window;F1-score;predictive interfaces;time pressure;dynamic divided attention environments;leave-one- subject-out cross-validation procedure","","","","47","","28 Sep 2020","","","IEEE","IEEE Conferences"
"A Neural Network-Based Driver Gaze Classification System with Vehicle Signals","S. Dari; N. Kadrileev; E. Hüllermeier","BMW Goup,Research and Development,Munich,Germany; Technical University Munich,Department of Electrical Engineering,Munich,Germany; Technical University Munich,Department of Electrical Engineering,Munich,Germany","2020 International Joint Conference on Neural Networks (IJCNN)","28 Sep 2020","2020","","","1","7","Driver monitoring can play an essential part in avoiding accidents by warning the driver and shifting the driver's attention to the traffic scenery in time during critical situations. This may apply for the different levels of automated driving, for take-over requests as well as for driving in manual mode. A great proxy for this purpose has always been the driver's gazing direction. The aim of this work is to introduce a robust gaze detection system. In this regard, we make several contributions that are novel in the area of gaze detection systems. In particular, we propose a deep learning approach to predict gaze regions, which is based on informative features such as eye landmarks and head pose angles of the driver. Moreover, we introduce different post-processing techniques that improve the accuracy by exploiting temporal information from videos and the availability of other vehicle signals. Last but not least, we confirm our method with a leave-one-driver-out cross-validation. Unlike previous studies, we do not use gazes to predict maneuver changes, but we consider the human-computer-interaction aspect and use vehicle signals to improve the performance of the estimation. The proposed system is able to achieve an accuracy of 92.3% outperforming earlier landmark-based gaze estimators.","2161-4407","978-1-7281-6926-2","10.1109/IJCNN48605.2020.9207709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207709","Driver Monitoring;Driver Gaze Estimation;Driver State Recognition;Driver distraction;Autonomous Driving;Naturalistic Driving Study","Vehicles;Faces;Videos;Training;Mirrors;Feature extraction","computer vision;driver information systems;eye;feature extraction;gaze tracking;learning (artificial intelligence);neural nets;pose estimation;road vehicles","neural network-based driver gaze classification system;driver monitoring;traffic scenery;critical situations;automated driving;robust gaze detection system;gaze detection systems;deep learning approach;gaze regions;post-processing techniques;leave-one-driver-out cross-validation;vehicle signals;landmark-based gaze estimators","","1","","20","","28 Sep 2020","","","IEEE","IEEE Conferences"
"Early Screening of Autism in Toddlers via Response-To-Instructions Protocol","J. Liu; Z. Wang; K. Xu; B. Ji; G. Zhang; Y. Wang; J. Deng; Q. Xu; X. Xu; H. Liu","State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai 200240, China, and also with the State Key Laboratory of Robotics and Systems, Harbin Institute of Technology Shenzhen, Shenzhen 518055, China.; State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai 200240, China, and also with the State Key Laboratory of Robotics and Systems, Harbin Institute of Technology Shenzhen, Shenzhen 518055, China.; State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai 200240, China.; State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai 200240, China.; School of Computing, University of Portsmouth, Portsmouth PO1 3HE, U.K.; Department of Child Health Care, Children's Hospital of Fudan University, Shanghai 201102, China.; Department of Child Health Care, Children's Hospital of Fudan University, Shanghai 201102, China.; Department of Child Health Care, Children's Hospital of Fudan University, Shanghai 201102, China.; Department of Child Health Care, Children's Hospital of Fudan University, Shanghai 201102, China (e-mail: xuxiu@fudan.edu.cn).; State Key Laboratory of Robotics and Systems, Harbin Institute of Technology Shenzhen, Shenzhen, China (e-mail: honghai.liu@icloud.com).","IEEE Transactions on Cybernetics","","2020","PP","99","1","11","Early screening of autism spectrum disorder (ASD) is crucial since early intervention evidently confirms significant improvement of functional social behavior in toddlers. This article attempts to bootstrap the response-to-instructions (RTIs) protocol with vision-based solutions in order to assist professional clinicians with an automatic autism diagnosis. The correlation between detected objects and toddler's emotional features, such as gaze, is constructed to analyze their autistic symptoms. Twenty toddlers between 16-32 months of age, 15 of whom diagnosed with ASD, participated in this study. The RTI method is validated against human codings, and group differences between ASD and typically developing (TD) toddlers are analyzed. The results suggest that the agreement between clinical diagnosis and the RTI method achieves 95% for all 20 subjects, which indicates vision-based solutions are highly feasible for automatic autistic diagnosis.","2168-2275","","10.1109/TCYB.2020.3017866","National Natural Science Foundation of China(grant numbers:61733011,51575338); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204807","Autistic early screening;gaze estimation;social behavior disorder","Autism;Pediatrics;Protocols;Toy manufacturing industry;Computer vision;Cameras;Estimation","","","","","","","IEEE","23 Sep 2020","","","IEEE","IEEE Early Access Articles"
"Towards an Efficient Segmentation Algorithm for Near-Infrared Eyes Images","A. Valenzuela; C. Arellano; J. E. Tapia","R+D CORFO Center SR-226, TOC Biometric, Santiago, Chile; R+D Visiometrica, Santiago, Chile; R+D CORFO Center SR-226, TOC Biometric, Santiago, Chile","IEEE Access","28 Sep 2020","2020","8","","171598","171607","Semantic segmentation has been widely used for several applications, including the detection of eye structures. This is used in tasks such as eye-tracking and gaze estimation, which are useful techniques for human-computer interfaces, salience detection, and Virtual reality (VR), amongst others. Most of the state of the art techniques achieve high accuracy but with a considerable number of parameters. This article explores alternatives to improve the efficiency of the state of the art method, namely DenseNet Tiramisu, when applied to NIR image segmentation. This task is not trivial; the reduction of block and layers also affects the number of feature maps. The growth rate (k) of the feature maps regulates how much new information each layer contributes to the global state, therefore the trade-off amongst grown rate (k), IOU, and the number of layers needs to be carefully studied. The main goal is to achieve a light-weight and efficient network with fewer parameters than traditional architectures in order to be used for mobile device applications. As a result, a DenseNet with only three blocks and ten layers is proposed (DenseNet10). Experiments show that this network achieved higher IOU rates when comparing with Encoder-Decoder, DensetNet56-67-103, MaskRCNN, and DeeplabV3+ models in the Facebook database. Furthermore, this method reached 8th place in The Facebook semantic segmentation challenge with 0.94293 mean IOU and 202.084 parameters with a final score of 0.97147. This score is only 0,001 lower than the first place in the competition. The sclera was identified as the more challenging structure to be segmented.","2169-3536","","10.1109/ACCESS.2020.3025195","Fondecyt Iniciacion(grant numbers:11170189); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200989","Biometrics;semantic segmentation;deep learning","Image segmentation;Semantics;Computer architecture;Solid modeling;Task analysis;Virtual reality;Facebook","eye;gaze tracking;image segmentation;object detection;virtual reality","eye structures;eye-tracking;gaze estimation;human-computer interfaces;salience detection;DenseNet Tiramisu;NIR image segmentation;feature maps;information each layer;global state;trade-off amongst grown rate;mobile device applications;IOU rates;DensetNet56-67-103;DeeplabV3+ models;Facebook semantic segmentation challenge","","2","","29","CCBY","21 Sep 2020","","","IEEE","IEEE Journals"
"Accurate Regression-Based 3D Gaze Estimation Using Multiple Mapping Surfaces","Z. Wan; C. Xiong; Q. Li; W. Chen; K. K. L. Wong; S. Wu","State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, China; State Key Laboratory of Digital Manufacturing Equipment and Technology, Institute of Rehabilitation and Medical Robotics, Huazhong University of Science and Technology, Wuhan, China; School of Information Science and Engineering, Institute of Robotics and Intelligent Systems, Wuhan University of Science and Technology, Wuhan, China; School of Information Science and Engineering, Institute of Robotics and Intelligent Systems, Wuhan University of Science and Technology, Wuhan, China","IEEE Access","21 Sep 2020","2020","8","","166460","166471","Accurate 3D gaze estimation using a simple setup remains a challenging issue for head-mounted eye tracking. Current regression-based gaze direction estimation methods implicitly assume that all gaze directions intersect at one point called the eyeball pseudo-center. The effect of this implicit assumption on gaze estimation is unknown. In this paper, we find that this assumption is approximate based on a simulation of all intersections of gaze directions, and it is conditional based on a sensitivity analysis of the assumption in gaze estimation. Hence, we propose a gaze direction estimation method with one mapping surface that satisfies conditions of the assumption by configuring one mapping surface and achieving a high-quality calibration of the eyeball pseudo-center. This method only adds two additional calibration points outside the mapping surface. Furthermore, replacing the eyeball pseudo-center with an additional calibrated surface, we propose a gaze direction estimation method with two mapping surfaces that further improves the accuracy of gaze estimation. This method improves accuracy on the state-of-the-art method by 20 percent (from a mean error of 1.84 degrees to 1.48 degrees) on a public dataset with a usage range of 1 meter and by 17 percent (from a mean error of 2.22 degrees to 1.85 degrees) on a public dataset with a usage range of 2 meters.","2169-3536","","10.1109/ACCESS.2020.3023448","National Natural Science Foundation of China(grant numbers:91648203,U1913601); National Key Research and Development Program of China(grant numbers:2018YFB1307201); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9194715","Head-mounted eye tracking;3D gaze estimation;gaze direction estimation;eyeball center;mapping surface","Estimation;Calibration;Cameras;Three-dimensional displays;Head;Gaze tracking;Two dimensional displays","calibration;eye;gaze tracking;regression analysis","calibration points;accurate regression-based 3D gaze estimation;regression-based gaze direction estimation methods;gaze direction estimation method;eyeball pseudocenter;gaze directions intersect;multiple mapping surfaces;efficiency 20.0 percent;efficiency 17.0 percent","","","","30","CCBY","11 Sep 2020","","","IEEE","IEEE Journals"
"Evaluation of appearance-based eye tracking calibration data selection","Y. Li; Y. Zhan; Z. Yang","School of Computers, Guangdong University of Technology,Guangzhou,China; School of Computers, Guangdong University of Technology,Guangzhou,China; School of Computers, Guangdong University of Technology,Guangzhou,China","2020 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)","1 Sep 2020","2020","","","222","224","Eye tracking is a valuable topic in computer vision. Appearance-based eye tracking is a promising research direction in recent years. Convolutional neural networks (CNN) had been used in gaze estimation, which cover the significant variability in eye appearance caused by unconstrained head motion. With computation capability of consumer devices rapidly evolving, accurate and efficient appearance-based eye tracking has the potential for multipurpose applications. Person-independent networks have limit in improving gaze estimation accuracy. Person-specific network with calibration is more effective than person-independent approaches. Unlike classical eye tracking methods, appearance-based eye tracking has not a clear way to calibration. Our goal is to analyze the impact of calibration data selection and calibration target distribution on person-specific gaze estimation accuracy. We trained person-independent network and use SVR to calibration. We choose two kind of typical distribution targets to evaluation. Use different distribution targets to calibration achieves different accuracy.","","978-1-7281-7005-3","10.1109/ICAICA50127.2020.9181854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9181854","eye tracking;calibration;gaze;human computer interaction","Calibration;Gaze tracking;Estimation error;Conferences;Head;Computer vision","calibration;computer vision;convolutional neural nets;eye;gaze tracking;object detection","eye appearance;accurate appearance-based eye tracking;efficient appearance-based eye tracking;person-independent networks;person-specific network;person-independent approaches;classical eye tracking methods;calibration target distribution;person-specific gaze estimation accuracy;trained person-independent network;calibration achieves different accuracy;convolutional neural networks;appearance-based eye tracking calibration data selection;computer vision","","1","","25","","1 Sep 2020","","","IEEE","IEEE Conferences"
"Unsupervised Representation Learning for Gaze Estimation","Y. Yu; J. -M. Odobez","Idiap Research Institute, CH-1920, Martigny, Switzerland EPFL, CH-1015, Lausanne, Switzerland; Idiap Research Institute, CH-1920, Martigny, Switzerland EPFL, CH-1015, Lausanne, Switzerland","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","5 Aug 2020","2020","","","7312","7322","Although automatic gaze estimation is very important to a large variety of application areas, it is difficult to train accurate and robust gaze models, in great part due to the difficulty in collecting large and diverse data (annotating 3D gaze is expensive and existing datasets use different setups). To address this issue, our main contribution in this paper is to propose an effective approach to learn a low dimensional gaze representation without gaze annotations, which to the best of our best knowledge, is the first work to do so. The main idea is to rely on a gaze redirection network and use the gaze representation difference of the input and target images (of the redirection network) as the redirection variable. A redirection loss in image domain allows the joint training of both the redirection network and the gaze representation network. In addition, we propose a warping field regularization which not only provides an explicit physical meaning to the gaze representations but also avoids redirection distortions. Promising results on few-shot gaze estimation (competitive results can be achieved with as few as ≤ 100 calibration samples), cross-dataset gaze estimation, gaze network pretraining, and another task (head pose estimation) demonstrate the validity of our framework.","2575-7075","978-1-7281-7168-5","10.1109/CVPR42600.2020.00734","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157235","","Estimation;Three-dimensional displays;Training;Face;Two dimensional displays;Robustness","calibration;pose estimation;unsupervised learning","unsupervised representation;automatic gaze estimation;annotating 3D gaze;low dimensional gaze representation;gaze annotations;gaze redirection network;gaze representation difference;target images;redirection variable;redirection loss;image domain;gaze representation network;gaze representations;redirection distortions;few-shot gaze estimation;cross-dataset gaze estimation;gaze network pretraining;head pose estimation","","5","","63","","5 Aug 2020","","","IEEE","IEEE Conferences"
"Convolutional Neural Network-Based Methods for Eye Gaze Estimation: A Survey","A. A. Akinyelu; P. Blignaut","Department of Computer Science and Informatics, University of the Free State, Bloemfontein, South Africa; Department of Computer Science and Informatics, University of the Free State, Bloemfontein, South Africa","IEEE Access","12 Aug 2020","2020","8","","142581","142605","Eye tracking is becoming a very important tool across many domains, including human-computer-interaction, psychology, computer vision, and medical diagnosis. Different methods have been used to tackle eye tracking, however, some of them are inaccurate under real-world conditions, while some require explicit user calibration which can be burdensome. Some of these methods suffer from poor image quality and variable light conditions. The recent success and prevalence of deep learning have greatly improved the performance of eye-tracking. The availability of large-scale datasets has further improved the performance of deep learning-based methods. This article presents a survey of the current state-of-the-art on deep learning-based gaze estimation techniques, with a focus on Convolutional Neural Networks (CNN). This article also provides a survey on other machine learning-based gaze estimation techniques. This study aims to empower the research community with valuable and useful insights that can enhance the design and development of improved and efficient deep learning-based eye-tracking models. This study also provides information on various pre-trained models, network architectures, and open-source datasets that are useful for training deep learning models.","2169-3536","","10.1109/ACCESS.2020.3013540","University of the Free State, South Africa; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153754","Convolutional neural network;deep learning;eye tracking;eye movements;gaze estimation;computer vision;region of interest","Estimation;Solid modeling;Visualization;Machine learning;Computational modeling;Feature extraction;Gaze tracking","computer vision;convolutional neural nets;gaze tracking;learning (artificial intelligence)","deep learning-based eye-tracking models;improved learning-based eye-tracking models;machine learning;convolutional neural networks;deep learning-based methods;variable light conditions;explicit user calibration;real-world conditions;computer vision;eye tracking;eye gaze estimation;convolutional neural network-based methods","","3","","119","CCBY","31 Jul 2020","","","IEEE","IEEE Journals"
"Enhanced Self-Perception in Mixed Reality: Egocentric Arm Segmentation and Database With Automatic Labeling","E. Gonzalez-Sosa; P. Pérez; R. Tolosana; R. Kachach; A. Villegas","Nokia Bell Labs, Madrid, Spain; Nokia Bell Labs, Madrid, Spain; Departamento de Tecnología Electrónica y de las Comunicaciones, Universidad Autónoma de Madrid, Madrid, Spain; Nokia Bell Labs, Madrid, Spain; Nokia Bell Labs, Madrid, Spain","IEEE Access","17 Aug 2020","2020","8","","146887","146900","In this study, we focus on the egocentric segmentation of arms to improve self-perception in Augmented Virtuality (AV). The main contributions of this work are: i) a comprehensive survey of segmentation algorithms for AV; ii) an Egocentric Arm Segmentation Dataset (EgoArm), composed of more than 10, 000 images, demographically inclusive (variations of skin color, and gender), and open for research purposes. We also provide all details required for the automated generation of groundtruth and semi-synthetic images; iii) the proposal of a deep learning network to segment arms in AV; iv) a detailed quantitative and qualitative evaluation to showcase the usefulness of the deep network and EgoArm dataset, reporting results on different real egocentric hand datasets, including GTEA Gaze+, EDSH, EgoHands, Ego Youtube Hands, THU-Read, TEgO, FPAB, and Ego Gesture, which allow for direct comparisons with existing approaches using color or depth. Results confirm the suitability of the EgoArm dataset for this task, achieving improvements up to 40% with respect to the baseline network, depending on the particular dataset. Results also suggest that, while approaches based on color or depth can work under controlled conditions (lack of occlusion, uniform lighting, only objects of interest in the near range, controlled background, etc.), deep learning is more robust in real AV applications.","2169-3536","","10.1109/ACCESS.2020.3013016","Torres Quevedo Fund “PTQ-17-09374” from the Ministerio de Ciencia, Innovación y Universidades; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152989","Egocentric arm segmentation;mixed reality;augmented virtuality;self-perception;arm segmentation;automatic labeling;EgoArm dataset;demographically inclusive","Image segmentation;Image color analysis;Virtual reality;Skin;Resists;Keyboards;Cameras","augmented reality;feature extraction;gesture recognition;image colour analysis;image segmentation;learning (artificial intelligence)","EgoArm dataset;Ego Youtube Hands;AV applications;enhanced self-perception;mixed reality;automatic labeling;Augmented Virtuality;segmentation algorithms;Egocentric Arm Segmentation Dataset;skin color;semisynthetic images;deep learning network;segment arms;detailed quantitative evaluation;qualitative evaluation;deep network;real egocentric hand datasets","","1","","73","CCBY","31 Jul 2020","","","IEEE","IEEE Journals"
"CycleGAN-Based Deblurring for Gaze Tracking in Vehicle Environments","H. S. Yoon; K. R. Park","Division of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea; Division of Electronics and Electrical Engineering, Dongguk University, Seoul, South Korea","IEEE Access","4 Aug 2020","2020","8","","137418","137437","Recently, the need for research on tracking driver gazes is increasing owing to the development of driver convenience systems, such as autonomous driving or intelligent driver monitoring system, to address traffic accidents caused by negligence. A camera is installed in a vehicle to track the driver's gaze in vehicle environment. The accuracy of estimating driver gazes in vehicle environments reduces if a motion blur of the driver occurs, owing to vehicular vibrations during driving. Most past studies on gaze-tracking of a driver in a vehicle did not consider the motion blurs in their experiments. To address this concern, we propose a method for improving the accuracy of gaze estimation by deblurring the blurred images of a driver from the vehicle. This study is the first attempt to calculate a driver's gaze by deblurring a motion blurred image with CycleGAN, whereas simultaneously using the image information from the two cameras in the vehicle. In previous studies, multiple deep CNNs were used for obtaining the images of a driver's eyes and face. In this study, information obtained from the two cameras in the vehicle are integrated into an image with three channels and thereafter deblurred, consequently reducing the time required for training. Whereas in previous studies the gaze position was not calculated for severe blurs by measuring the level of blur from the input image, the gaze position was calculated for all the input images in this study. From the database (Dongguk blurred gaze database (DBGD)) from 26 drivers in actual vehicles and the Columbia gaze dataset (CAVE-DB) that is an open database, the proposed method exhibited greater accuracy than the existing methods.","2169-3536","","10.1109/ACCESS.2020.3012191","National Research Foundation of Korea (NRF) funded by the Ministry of Science and ICT (MSIT) through the Basic Science Research Program(grant numbers:NRF-2019R1F1A1041123,NRF-2019R1A2C1083813,NRF-2020R1A2C1006179); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149867","Deblurring;driver’s gaze detection;CycleGAN;vehicle environment","Vehicles;Cameras;Face;Databases;Tracking;Monitoring","cameras;convolutional neural nets;driver information systems;gaze tracking;image classification;image motion analysis;image restoration;road accidents;road safety","image information;cameras;gaze position;input image;Columbia gaze dataset;vehicle environment;driver convenience systems;intelligent driver monitoring system;motion blur;driver gaze estimation;motion blurred image;cycleGAN;multiple deep CNN;driver gaze-tracking","","","","57","CCBY","27 Jul 2020","","","IEEE","IEEE Journals"
"Automatic pupil detection and gaze estimation using the vestibulo-ocular reflex in a low-cost eye-tracking setup","V. Aharonson; V. Y. Coopoo; K. L. Govender; M. Postema","School of Electrical and Information Engineering, University of the Witwatersrand, Johannesburg, 1 Jan Smuts Laan, Braamfontein 2050, South Africa; School of Electrical and Information Engineering, University of the Witwatersrand, Johannesburg, 1 Jan Smuts Laan, Braamfontein 2050, South Africa; School of Electrical and Information Engineering, University of the Witwatersrand, Johannesburg, 1 Jan Smuts Laan, Braamfontein 2050, South Africa; School of Electrical and Information Engineering, University of the Witwatersrand, Johannesburg, 1 Jan Smuts Laan, Braamfontein 2050, South Africa","SAIEE Africa Research Journal","16 Jul 2020","2020","111","3","120","124","Automatic eye tracking is of interest for interaction with people suffering from amyotrophic lateral sclerosis, for using the eyes to control a computer mouse, and for controlled radiotherapy of uveal melanoma. It has been speculated that gaze estimation accuracy might be improved by using the vestibulo-ocular reflex. This involuntary reflex results in slow, compensatory eye movements, opposing the direction of head motion. We therefore hypothesised that leaving the head to move freely during eye tracking must produce more accurate results than keeping the head fixed, only allowing the eyes to move. The purpose of this study was to create a low-cost eye tracking system that incorporates the vestibulo-ocular reflex in gaze estimation, by keeping the head freely moving. The instrument used comprised a low-cost head-mounted webcam which recorded a single eye. Pupil detection was fully automatic and in real time with a straightforward hybrid colour-based and model-based algorithm, despite the lower-end webcam used for recording and despite the absence of direct illumination. A model-based algorithm and an interpolation-based algorithm were tested in this study. Based on mean absolute angle difference in the gaze estimation results, we conclude that the model-based algorithm performed better when the head was not moving and equally well when the head was moving. With most deviations of the points of gaze from the target points being less than 1° using either algorithm when the head is moving freely, it can be concluded that our setup performs fully within the 2° benchmark from literature, whereas deviations when the head was not moving exceeded 2°. The algorithms used were not previously tested under passive illumination. This was the first study of a low-cost eye-tracking setup taking into account the vestibulo-ocular reflex.","1991-1696","","10.23919/SAIEE.2020.9142605","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9142605","Pupil tracking;video-oculography;eye position mapping;VO reflex","Head;Estimation;Magnetic heads;Lighting;Gaze tracking;Webcams;Eye protection","","","","","","35","","16 Jul 2020","","","SAIEE","SAIEE Journals"
"Mutual Context Network for Jointly Estimating Egocentric Gaze and Action","Y. Huang; M. Cai; Z. Li; F. Lu; Y. Sato","Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; State Key Laboratory of VR System and Technology, SCSE, Beihang University, Beijing, China; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan","IEEE Transactions on Image Processing","17 Jul 2020","2020","29","","7795","7806","In this work, we address two coupled tasks of gaze prediction and action recognition in egocentric videos by exploring their mutual context: the information from gaze prediction facilitates action recognition and vice versa. Our assumption is that during the procedure of performing a manipulation task, on the one hand, what a person is doing determines where the person is looking at. On the other hand, the gaze location reveals gaze regions which contain important and information about the undergoing action and also the non-gaze regions that include complimentary clues for differentiating some fine-grained actions. We propose a novel mutual context network (MCN) that jointly learns action-dependent gaze prediction and gaze-guided action recognition in an end-to-end manner. Experiments on multiple egocentric video datasets demonstrate that our MCN achieves state-of-the-art performance of both gaze prediction and action recognition. The experiments also show that action-dependent gaze patterns could be learned with our method.","1941-0042","","10.1109/TIP.2020.3007841","JST CREST(grant numbers:JPMJCR14E1); JST AIP Acceleration Research of Japan(grant numbers:JPMJCR20U1); National Natural Science Foundation of China(grant numbers:61906064,61972012); Hunan Provincial Natural Science Foundation of China(grant numbers:2020JJ5082); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139335","Gaze prediction;egocentric video;action recognition","Videos;Task analysis;Feature extraction;Context modeling;Predictive models;Visualization;Semantics","gaze tracking;image recognition;learning (artificial intelligence);video signal processing","mutual context network;egocentric videos;action-dependent gaze prediction;gaze-guided action recognition;multiple egocentric video datasets;action-dependent gaze patterns;egocentric gaze estimation;egocentric action estimation;MCN","","1","","70","IEEE","13 Jul 2020","","","IEEE","IEEE Journals"
"3-D Gaze-Estimation Method Using a Multi-Camera-Multi-Light-Source System","J. Chi; J. Liu; F. Wang; Y. Chi; Z. -G. Hou","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Department of Pathology, General Hospital of Northern Theater Command, Shenyang, China; State Key Laboratory of Complex System Management and Control, Institute of Automation, Chinese Academy of Sciences, Beijing, China","IEEE Transactions on Instrumentation and Measurement","9 Nov 2020","2020","69","12","9695","9708","The image of the center of a 3-D circular target is not the center of its imaging ellipse due to the imaging distortion of the spatial circular targets; therefore, traditional 3-D gaze-estimation methods with the multi-camera-multi-light-source (MCMLS) systems generally replace the projection of the pupil center with the virtual image of the pupil center for 3-D gaze estimation. However, this introduces a large gaze-estimation error when the oblique angle between the optical axis of the eye and the camera optical axis is large. To eliminate the error caused by using the virtual image of the pupil center, a 3-D gaze-estimation method using an MCMLS system is developed in this study. We first estimate the cornea center and then determine the matching points of the pupil imaging ellipse using the special polar plane. After the cornea radius and the kappa angle are calibrated, the optical axis of the eye is reconstructed by the refraction planes constructed by the edge points of the pupil imaging ellipses. Thus, the 3-D gaze is estimated by the real-time calculated transformation matrix represented by the kappa angle. The feasibility and performance of the method have been analyzed by the simulations and the experiments. Since the estimation of the spatial pupil center or the construction of the refraction plane using the virtual image of the pupil center is not required, the proposed method mitigates the inherent errors caused by optical axis reconstruction in the traditional methods and simplifies the algorithm for gaze estimation, which has a practical value.","1557-9662","","10.1109/TIM.2020.3006681","National Key Research and Development Program of China(grant numbers:2018YFC2001700); Opening Project of Key Laboratory of Operation Safety Technology on Transport Vehicle, Ministry of Transport, China; Beijing Municipal Natural Science Foundation(grant numbers:4172040); Science and Technology on Electrooptic Control Laboratory(grant numbers:301090704); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9133643","3-D gaze estimation;corneal reflection;multi-camera-multi-light-source (MCMLS) system;optical axis reconstruction;pupil refraction;user calibration","Pupils;Cameras;Cornea;Gaze tracking;Three-dimensional displays;Optical distortion;Calibration","calibration;cameras;eye;gaze tracking;image matching;image reconstruction;image sensors;matrix algebra;stereo image processing","multicamera-multilight-source system;imaging ellipse;imaging distortion;spatial circular targets;virtual image;gaze-estimation error;camera optical axis;MCMLS system;cornea center;kappa angle;pupil imaging ellipses;spatial pupil center;optical axis reconstruction;3D circular target;3D gaze estimation method","","","","46","IEEE","6 Jul 2020","","","IEEE","IEEE Journals"
"3D Eye Model-Based Gaze Tracking System with a Consumer Depth Camera","L. Xu; J. Chi","University of Science and Technology, PRC; University of Science and Technology, PRC","2020 3rd International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","2 Jul 2020","2020","","","293","297","Most existing gaze tracking systems are high-cost, intrusive and difficult to calibrate, and some rely on the infrared illuminant. However, such systems may not work outdoor and meet real-time requirements. This paper proposes a non-intrusive system based on the 3D eyeball model, which does not need the exact infrared illuminant and complicated calibration process and allows the natural movement of the head. In the proposed system, Kinect is used to track the iris center and face model of the person, and the 3D information is easy to obtain. At the same time, point cloud registration algorithm is applied based on feature points in the face model sequence to obtain accurate head pose estimation results. In this paper, a personal calibration process is also proposed to obtain the gaze model parameters for different users, such as the eyeball center and angle kappa. The proposed method has good adaptability to the change of illuminant and head movement. In the actual operating environment, the system speed reaches 30 fps, which can meet the requirements of real-time control.","","978-1-7281-8143-1","10.1109/AEMCSE50948.2020.00070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9131201","Gaze estimation;RGB-D camera;Eye model-based;personal calibration","","calibration;cameras;eye;face recognition;feature extraction;gaze tracking;human computer interaction;image registration;pose estimation","3D eye model-based gaze tracking system;consumer depth camera;existing gaze tracking systems;real-time requirements;nonintrusive system;3D eyeball model;exact infrared illuminant calibration process;complicated calibration process;iris center;point cloud registration algorithm;face model sequence;personal calibration process;gaze model parameters;system speed","","","","18","","2 Jul 2020","","","IEEE","IEEE Conferences"
"Augmenting Dementia Cognitive Assessment With Instruction-Less Eye-Tracking Tests","K. Mengoudi; D. Ravi; K. X. X. Yong; S. Primativo; I. M. Pavisic; E. Brotherhood; K. Lu; J. M. Schott; S. J. Crutch; D. C. Alexander","Department of Computer Science, Centre for Medical Image Computing, University College London, London, U.K.; Department of Neurodegenerative disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, U.K.; Department of Computer Science, Centre for Medical Image Computing, University College London, London, U.K.; Department of Human Science, LUMSA University, Rome, Italy; Department of Neurodegenerative disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, U.K.; Department of Neurodegenerative disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, U.K.; Department of Neurodegenerative disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, U.K.; Department of Neurodegenerative disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, U.K.; Department of Neurodegenerative disease, Dementia Research Centre, Queen Square Institute of Neurology, University College London, London, U.K.; Department of Computer Science, Centre for Medical Image Computing, University College London, London, U.K.","IEEE Journal of Biomedical and Health Informatics","4 Nov 2020","2020","24","11","3066","3075","Eye-tracking technology is an innovative tool that holds promise for enhancing dementia screening. In this work, we introduce a novel way of extracting salient features directly from the raw eye-tracking data of a mixed sample of dementia patients during a novel instruction-less cognitive test. Our approach is based on self-supervised representation learning where, by training initially a deep neural network to solve a pretext task using well-defined available labels (e.g. recognising distinct cognitive activities in healthy individuals), the network encodes high-level semantic information which is useful for solving other problems of interest (e.g. dementia classification). Inspired by previous work in explainable AI, we use the Layer-wise Relevance Propagation (LRP) technique to describe our network's decisions in differentiating between the distinct cognitive activities. The extent to which eye-tracking features of dementia patients deviate from healthy behaviour is then explored, followed by a comparison between self-supervised and handcrafted representations on discriminating between participants with and without dementia. Our findings not only reveal novel self-supervised learning features that are more sensitive than handcrafted features in detecting performance differences between participants with and without dementia across a variety of tasks, but also validate that instruction-less eye-tracking tests can detect oculomotor biomarkers of dementia-related cognitive dysfunction. This work highlights the contribution of self-supervised representation learning techniques in biomedical applications where the small number of patients, the non-homogenous presentations of the disease and the complexity of the setting can be a challenge using state-of-the-art feature extraction methods.","2168-2208","","10.1109/JBHI.2020.3004686","European Union's Horizon 2020(grant numbers:666992); EPSRC(grant numbers:EP/M020533/1,EP/M006093/1); NIHR UCLH Biomedical Research Centre; Alzheimer's Research UK(grant numbers:ARUK-PG2014–1946,ARUK-PG2017–1946); Medical Research Council; Dementias Platform UK; Wolfson Foundation; Alzheimer's Society(grant numbers:453 AS-JF-18-003); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9124654","Eye-tracking;dementia;cognition;deep-learning;representation learning","Dementia;Task analysis;Feature extraction;Semantics;Pupils;Cognition","feature extraction;gaze tracking;image representation;medical disorders;neural nets;supervised learning","dementia patients;deep neural network;Layer-wise Relevance Propagation technique;eye-tracking features;handcrafted representations;self-supervised learning features;dementia-related cognitive dysfunction;feature extraction methods;instructionless cognitive test;instructionless eye-tracking tests;self-supervised representation learning techniques;LRP technique;raw eye-tracking data;eye-tracking technology;dementia cognitive assessment","Cognition;Cognitive Dysfunction;Dementia;Eye-Tracking Technology;Humans;Neuropsychological Tests","","","25","IEEE","24 Jun 2020","","","IEEE","IEEE Journals"
"Real-Time Driver Maneuver Prediction Using LSTM","N. Khairdoost; M. Shirpour; M. A. Bauer; S. S. Beauchemin","Department of Computer Science, Western University, London, ON, Canada; Department of Computer Science, Western University, London, ON, Canada; Department of Computer Science, Western University, London, ON, Canada; Department of Computer Science, Western University, London, ON, Canada","IEEE Transactions on Intelligent Vehicles","23 Nov 2020","2020","5","4","714","724","Driver maneuver prediction is of great importance in designing a modern Advanced Driver Assistance System (ADAS). Such predictions can improve driving safety by alerting the driver to the danger of unsafe or risky traffic situations. In this research, we developed a model to predict driver maneuvers, including left/right lane changes, left/right turns and driving straight forward 3.6 seconds on average before they occur in real time. For this, we propose a deep learning method based on Long Short-Term Memory (LSTM) which utilizes data on the driver's gaze and head position as well as vehicle dynamics data. We applied our approach on real data collected during drives in an urban environment in an instrumented vehicle. In comparison with previous IOHMM techniques that predicted three maneuvers including left/right turns and driving straight, our prediction model is able to anticipate two more maneuvers. In addition to this, our experimental results show that our model using identical dataset improved F1 score by 4% and increased to 84%.","2379-8904","","10.1109/TIV.2020.3003889","Natural Sciences and Engineering Research Council of Canada; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9121758","","Hidden Markov models;Predictive models;Advanced driver assistance systems;Deep learning;Long short term memory;Neural networks","driver information systems;learning (artificial intelligence);recurrent neural nets;road accidents;road traffic;road vehicles","prediction model;LSTM;advanced driver assistance system;risky traffic situations;deep learning method;long short-term memory;vehicle dynamics data;driver maneuver prediction;IOHMM technique;time 3.6 s","","3","","42","Crown","19 Jun 2020","","","IEEE","IEEE Journals"
"3D Gaze Estimation for Head-Mounted Eye Tracking System With Auto-Calibration Method","M. Liu; Y. Li; H. Liu","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong","IEEE Access","10 Jun 2020","2020","8","","104207","104215","The general challenges of 3D gaze estimation for head-mounted eye tracking systems are inflexible marker-based calibration procedure and significant errors of depth estimation. In this paper, we propose a 3D gaze estimation with an auto-calibration method. To acquire the accurate 3D structure of the environment, an RGBD camera is applied as the scene camera of our system. By adopting the saliency detection method, saliency maps can be acquired through scene images, and 3D salient pixels in the scene are considered potential 3D calibration targets. The 3D eye model is built on the basis of eye images to determine gaze vectors. By combining 3D salient pixels and gaze vectors, the auto-calibration can be achieved with our calibration method. Finally, the 3D gaze point is obtained through the calibrated gaze vectors, and the point cloud is generated from the RGBD camera. The experimental result shows that the proposed system can achieve an average accuracy of 3.7° in the range of 1 m to 4 m indoors and 4.0° outdoors. The proposed system also presents a great improvement in depth measurement, which is sufficient for tracking users' visual attention in real scenes.","2169-3536","","10.1109/ACCESS.2020.2999633","Research Grants Council of Hong Kong Project(grant numbers:CityU 11203619); National Natural Science Foundation of China(grant numbers:61873220,61875068); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107144","Head-mounted gaze tracking system;saliency maps;auto-calibration;3D gaze estimation","Three-dimensional displays;Cameras;Estimation;Calibration;Gaze tracking;Resists;Solid modeling","calibration;cameras;estimation theory;gaze tracking;image colour analysis;stereo image processing","head-mounted eye tracking system;autocalibration method;inflexible marker-based calibration procedure;depth estimation;RGBD camera;saliency detection method;3D salient pixels;3D eye model;3D gaze point estimation;gaze vector calibration;3D calibration targets;distance 1.0 m to 4.0 m","","6","","35","CCBY","3 Jun 2020","","","IEEE","IEEE Journals"
"Using Human Gaze to Improve Robustness Against Irrelevant Objects in Robot Manipulation Tasks","H. Kim; Y. Ohmura; Y. Kuniyoshi","Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Technology, Laboratory for Intelligent Systems and Informatics, The University of Tokyo, Tokyo, Japan","IEEE Robotics and Automation Letters","9 Jun 2020","2020","5","3","4415","4422","Deep imitation learning enables the learning of complex visuomotor skills from raw pixel inputs. However, this approach suffers from the problem of overfitting to the training images. The neural network can easily be distracted by task-irrelevant objects. In this letter, we use the human gaze measured by a head-mounted eye tracking device to discard task-irrelevant visual distractions. We propose a mixture density network-based behavior cloning method that learns to imitate the human gaze. The model predicts gaze positions from raw pixel images and crops images around the predicted gazes. Only these cropped images are used to compute the output action. This cropping procedure can remove visual distractions because the gaze is rarely fixated on task-irrelevant objects. This robustness against irrelevant objects can improve the manipulation performance of robots in scenarios where task-irrelevant objects are present. We evaluated our model on four manipulation tasks designed to test the robustness of the model to irrelevant objects. The results indicate that the proposed model can predict the locations of task-relevant objects from gaze positions, is robust to task-irrelevant objects, and exhibits impressive manipulation performance especially in multi-object handling.","2377-3766","","10.1109/LRA.2020.2998410","Scientific Research (A)(grant numbers:JP18H04108); New Energy and Industrial Technology Development Organization; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103290","Deep learning in grasping and manipulation;learning from demonstration;telerobotics and teleoperation;visual servoing;computer vision for automation","Robots;Task analysis;Visualization;Robustness;Cloning;Neural networks;Cameras","gaze tracking;humanoid robots;learning (artificial intelligence);manipulators;neural nets;reliability;visual perception","deep imitation learning;task-irrelevant objects;human gaze;task-irrelevant visual distractions;mixture density network;gaze positions;raw pixel images;task-relevant objects;robot manipulation tasks;behavior cloning method;complex visuomotor skills;neural network;head-mounted eye tracking;visual distraction removal;multiobject handling","","1","","45","CCBY","28 May 2020","","","IEEE","IEEE Journals"
"Gaze Estimation for Assisted Living Environments","P. A. Dias; D. Malafronte; H. Medeiros; F. Odone","Marquette University (EECE),USA; Italian Institute of Technology; Marquette University (EECE),USA; University of Genova (MaLGa-DIBRIS),Italy","2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","279","288","Effective assisted living environments must be able to perform inferences on how their occupants interact with one another as well as with surrounding objects. To accomplish this goal using a vision-based automated approach, multiple tasks such as pose estimation, object segmentation and gaze estimation must be addressed. Gaze direction provides some of the strongest indications of how a person interacts with the environment. In this paper, we propose a simple neural network regressor that estimates the gaze direction of individuals in a multi-camera assisted living scenario, relying only on the relative positions of facial keypoints collected from a single pose estimation model. To handle cases of keypoint occlusion, our model exploits a novel confidence gated unit in its input layer. In addition to the gaze direction, our model also outputs an estimation of its own prediction uncertainty. Experimental results on a public benchmark demonstrate that our approach performs on par with a complex, dataset-specific baseline, while its uncertainty predictions are highly correlated to the actual angular error of corresponding estimations. Finally, experiments on images from a real assisted living environment demonstrate that our model has a higher suitability for its final application.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093439","","Estimation;Uncertainty;Assisted living;Predictive models;Feature extraction;Task analysis;Logic gates","assisted living;cameras;computer vision;feature extraction;image segmentation;learning (artificial intelligence);neural nets;pose estimation;regression analysis;video signal processing","object segmentation;gaze estimation;gaze direction;simple neural network regressor;multicamera assisted living scenario;single pose estimation model;assisted living environment;vision-based automated approach","","2","","36","","14 May 2020","","","IEEE","IEEE Conferences"
"Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation in the Wild","Z. Wang; J. Zhao; C. Lu; H. Huang; F. Yang; L. Li; Y. Guo","Columbia University; Institute of North Electronic Equipment,Beijing,China; XPENG Motors; XPENG Motors; XPENG Motors; XPENG Motors; XPENG Motors","2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","3432","3441","Unconstrained remote gaze estimation remains challenging mostly due to its vulnerability to the large variability in head-pose. Prior solutions struggle to maintain reliable accuracy in unconstrained remote gaze tracking. Among them, appearance-based solutions demonstrate tremendous potential in improving gaze accuracy. However, existing works still suffer from head movement and are not robust enough to handle real-world scenarios. Especially most of them study gaze estimation under controlled scenarios where the collected datasets often cover limited ranges of both head-pose and gaze which introduces further bias. In this paper, we propose novel end-to-end appearance-based gaze estimation methods that could more robustly incorporate different levels of head-pose representations into gaze estimation. Our method could generalize to real-world scenarios with low image quality, different lightings and scenarios where direct head-pose information is not available. To better demonstrate the advantage of our methods, we further propose a new benchmark dataset with the most rich distribution of head-gaze combination reflecting real-world scenarios. Extensive evaluations on several public datasets and our own dataset demonstrate that our method consistently outperforms the state-of-the-art by a significant margin.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093476","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093476","","Face;Estimation;Task analysis;Cameras;Image quality;Lighting","eye;feature extraction;gaze tracking;learning (artificial intelligence);pose estimation","head movement detection;unconstrained remote gaze estimation;prior solutions struggle;unconstrained remote gaze tracking;appearance-based solutions;gaze accuracy;end-to-end appearance-based gaze estimation;head-pose representations;direct head-pose information;head-gaze combination","","1","","56","","14 May 2020","","","IEEE","IEEE Conferences"
"Offset Calibration for Appearance-Based Gaze Estimation via Gaze Decomposition","Z. Chen; B. E. Shi",The Hong Kong University of Science and Technology; The Hong Kong University of Science and Technology,"2020 IEEE Winter Conference on Applications of Computer Vision (WACV)","14 May 2020","2020","","","259","268","Appearance-based gaze estimation provides relatively unconstrained gaze tracking. However, subject-independent models achieve limited accuracy partly due to individual variations. To improve estimation, we propose a gaze decomposition method that enables low complexity calibration, i.e., using calibration data collected when subjects view only one or a few gaze targets and the number of images per gaze target is small. Lowering the complexity of calibration makes it more convenient and less timeconsuming for the user, and more widely applicable. Motivated by our finding that the inter-subject squared bias exceeds the intra-subject variance for a subject-independent estimator, we decompose the gaze estimate into the sum of a subject-independent term estimated from the input image by a deep convolutional network and a subject-dependent bias term. During training, both the weights of the deep network and the bias terms are estimated. During testing, if no calibration data is available, we can set the bias term to zero. Otherwise, the bias term can be estimated from images of the subject gazing at known gaze targets. Experimental results on three datasets show that without calibration, our method outperforms state-of-the-art by at least 6.3%. For low complexity calibration sets, our method outperforms other calibration methods. More complex calibration algorithms do not outperform our method until the size of the calibration set is excessively large. Even then, the gains obtained by alternatives are small, e.g., only 0.1° lower error for 64 gaze targets. Source code is available at https://github.com/czk32611/Gaze-Decomposition.","2642-9381","978-1-7281-6553-0","10.1109/WACV45572.2020.9093419","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9093419","","Calibration;Complexity theory;Feature extraction;Estimation error;Lighting;Head","calibration;convolutional neural nets;gaze tracking;learning (artificial intelligence)","offset calibration;appearance-based gaze estimation;relatively unconstrained gaze tracking;subject-independent models;gaze decomposition method;calibration data;gaze target;inter-subject squared bias;intra-subject variance;subject-independent estimator;gaze estimate;subject-independent term;subject-dependent bias term;known gaze targets;low complexity calibration sets;calibration methods;complex calibration algorithms;calibration set;gaze targets","","","","51","","14 May 2020","","","IEEE","IEEE Conferences"
"Removal of the Infrared Light Reflection of Eyeglass Using Multi-Channel CycleGAN Applied for the Gaze Estimation Images","Y. Onuki; K. Kudo; l. Kumazawa",Tokyo lnstitute of Technology; Tokyo lnstitute of Technology; Tokyo lnstitute of Technology,"2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","11 May 2020","2020","","","590","591","In virtual reality (VR) environments, the importance of the eye gaze estimation is rapidly increasing. The geometrical model base method is commonly used by equipping infrared (IR) light sources and cameras inside of the head mount displays (HMDs). Some HMDs enable users to wear with spectacles on, and, in this case, IR light reflections of the eyeglass often causes serious obstruction to detect those of the corneal. In this study, we propose the multi-channel CycleGAN to generate the eye images with no eyeglass from those with eyeglass. Proposed method has 4 channels input, which consists of three normal eye images at different time points and an image in blinking, in order to distinguish stationary and moving reflections in images. Proposal achieved to selectively remove the eyeglass reflections and keep the corneal reflections alive in sequential gaze estimation images.","","978-1-7281-6532-5","10.1109/VRW50115.2020.00146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090544","Virtual reality;gaze estimation;generative adversarial network;reflection removal","Reflection;Estimation;Generators;Training;Conferences;Virtual reality;Light sources","","","","","","4","","11 May 2020","","","IEEE","IEEE Conferences"
"Gaze+Gesture Interface: Considering Social Acceptability","H. Heo; M. Lee; S. Kim; Y. Hwang","Korea Electronics Technology Institute,Republic of Korea; Korea Electronics Technology Institute,Republic of Korea; Korea Electronics Technology Institute,Republic of Korea; Korea Electronics Technology Institute,Republic of Korea","2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)","11 May 2020","2020","","","690","691","In public places like cafes, the usage of smart glasses with interfaces including controller, touchpad, voice, or mid-air gesture not only receives a lot of interest from people around him or her but also cannot protect the privacy of individuals. If smart glasses become more advanced and more popular than regular glasses in the future, socially acceptable user interfaces can be required. In this paper, we propose a user interface on HoloLens by using gaze tracking instead of head tracking for navigation, and unobtrusive gesture based on deep learning instead of mid-air gestures for selection/manipulation, that is more socially acceptable than the existing user interfaces on smart glasses. A study was conducted to investigate social acceptability from the users’ perspective, and the results showed the advantages of the proposed method to improve social acceptability.","","978-1-7281-6532-5","10.1109/VRW50115.2020.00196","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090522","Human-centered computing;Human computer interaction (HCI);Interaction techniques;Gestrual input","Task analysis;Smart glasses;Fatigue;Glass;Gaze tracking;Cameras","","","","","","7","","11 May 2020","","","IEEE","IEEE Conferences"
"Learning a 3D Gaze Estimator With Adaptive Weighted Strategy","X. Zhou; J. Jiang; Q. Liu; J. Fang; S. Chen; H. Cai","College of Electrical and Information Engineering, Quzhou University, Quzhou, China; College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, China; College of Electrical and Information Engineering, Quzhou University, Quzhou, China; College of Computer Science Technology, Zhejiang University of Technology, Hangzhou, China; Department of Computer Science, Loughborough University, Loughborough, U.K.","IEEE Access","13 May 2020","2020","8","","82142","82152","As a method of predicting the target's attention distribution, gaze estimation plays an important role in human-computer interaction. In this paper, we learn a 3D gaze estimator with adaptive weighted strategy to get the mapping from the complete images to the gaze vector. We select the both eyes, the complete face and their fusion features as the input of the regression model of gaze estimator. Considering that the different areas of the face have different contributions on the results of gaze estimation under free head movement, we design a new learning strategy for the regression net. To improve the efficiency of the regression model to a great extent, we propose a weighted network that can adjust the learning strategy of the regression net adaptively. Experimental results conducted on the MPIIGaze and EyeDiap datasets demonstrate that our method can achieve superior performance compared with other state-of-the-art 3D gaze estimation methods.","2169-3536","","10.1109/ACCESS.2020.2990685","National Natural Science Foundation of China(grant numbers:U1709207,61876168,61906168); National Basic Research Program of China (973 Program)(grant numbers:2018YFB1305200); Zhejiang Provincial Natural Science Foundation of China(grant numbers:LY18F030020,LY15F020041); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079544","Gaze estimation;weighted network;regression model;adaptive strategy","Estimation;Face;Three-dimensional displays;Adaptation models;Feature extraction;Cameras","computer vision;eye;gaze tracking;learning (artificial intelligence);motion estimation;regression analysis","adaptive weighted strategy;gaze vector;learning strategy;3D gaze estimator;human-computer interaction;regression model;free head movement;EyeDiap datasets;MPIIGaze dataset","","","","41","CCBY","27 Apr 2020","","","IEEE","IEEE Journals"
"Optimization Algorithm for Driver Monitoring System using Deep Learning Approach","M. W. Yoo; D. S. Han","Kyungpook National University,Department of Future Automotive and IT Convergence,Daegu,Republic of Korea; School of Electronics Engineering, Kyungpook National University,Daegu,Republic of Korea","2020 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","16 Apr 2020","2020","","","043","046","The driver monitoring system (DMS), also known as driver attention monitor, plays an important role for vehicle safety systems. In DMS, the system detects the driver's activities such as the state of being a driver sleepy or failure to give sufficient attention to avoid accidents. To reduce the driver mistakes while driving, the system warns the driver with an alarm sound or vibrations. For the efficient implementation of DMS, the system should work in real time without any delay. However, the current DMS has many challenges implementation to the complexity of network implementation. To reduce the DMS complexity for real time implementation, we propose an optimization algorithm, which uses a camera images for monitoring the driver activities. From the input camera images, we extract the driver's state information from the region of interest (ROI). In addition, the proposed system also extracts the driver's head pose and gaze information and monitors the driver states during driving. The experiment results from the proposed methods show accurate driver's state information and warns the driver immediately when any mistake occur from driver side.","","978-1-7281-4985-1","10.1109/ICAIIC48513.2020.9065222","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9065222","Deep neural network (DNN);Key-point detection;3D Transformation;camera calibration","Vehicles;Face;Optimization;Monitoring;Detection algorithms;Face detection","cameras;driver information systems;learning (artificial intelligence);pose estimation;road safety;traffic engineering computing","optimization algorithm;driver monitoring system;driver attention monitor;vehicle safety systems;current DMS;DMS complexity;driver activities","","","","3","","16 Apr 2020","","","IEEE","IEEE Conferences"
"Deep Fusion for 3D Gaze Estimation From Natural Face Images Using Multi-Stream CNNs","A. Ali; Y. -G. Kim","Department of Computer Engineering, Sejong University, Seoul, South Korea; Department of Computer Engineering, Sejong University, Seoul, South Korea","IEEE Access","22 Apr 2020","2020","8","","69212","69221","Over the last few decades, eye gaze estimation techniques have been thoroughly investigated by many researchers. However, predicting a 3D gaze from a 2D natural image remains challenging because it has to deal with several issues such as diverse head positions, face shape transformation, illumination variations, and subject individuality. Many previous studies employ convolutional neural networks (CNNs) for this task, and yet the accuracy needs improvement for its practical use. In this paper, we propose a 3D gaze estimation framework based on the data science perspective: First, a novel neural network architecture is designed to exploit every possible visual attribute such as the states of both eyes and the head position, including several augmentations; secondly, the data fusion method is utilized by incorporating multiple gaze datasets. Extensive experiments were carried out using two standard eye gaze datasets, including comparative analysis. The experimental results suggest that our method outperforms state-of-the-art with 2.8 degrees for MPIIGaze and 3.05 degrees for EYEDIAP dataset, respectively, indicating that it has a potential for real applications.","2169-3536","","10.1109/ACCESS.2020.2986815","Institute for Information and Communications Technology Promotion (IITP) funded by the Korea Government (MSIT), through personalized advertisement platform based on viewers’ attention and emotion using deep-learning(grant numbers:2017-0-00731); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9062592","Gaze estimation;data fusion;convolutional neural networks;MPIIGaze;EYEDIAP","Estimation;Three-dimensional displays;Head;Feature extraction;Cameras;Task analysis;Computer architecture","computer vision;convolutional neural nets;face recognition;gaze tracking;image fusion;object tracking","multiple gaze datasets;data fusion method;head position;possible visual attribute;data science perspective;3D gaze estimation framework;convolutional neural networks;subject individuality;illumination variations;shape transformation;diverse head positions;2D natural image;eye gaze estimation techniques;multistream CNNs;natural face images;deep fusion;standard eye gaze datasets","","1","","55","CCBY","9 Apr 2020","","","IEEE","IEEE Journals"
"Noise-Robust Pupil Center Detection Through CNN-Based Segmentation With Shape-Prior Loss","S. Y. Han; H. J. Kwon; Y. Kim; N. I. Cho","Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, South Korea; Department of Electrical and Computer Engineering, Institute of New Media and Communication (INMC), Seoul National University, Seoul, South Korea","IEEE Access","14 Apr 2020","2020","8","","64739","64749","Detecting the pupil center plays a key role in human-computer interaction, especially for gaze tracking. The conventional deep learning-based method for this problem is to train a convolutional neural network (CNN), which takes the eye image as the input and gives the pupil center as a regression result. In this paper, we propose an indirect use of the CNN for the task, which first segments the pupil region by a CNN as a classification problem, and then finds the center of the segmented region. This is based on the observation that CNN works more robustly for the pupil segmentation than for the pupil center-point regression when the inputs are noisy IR images. Specifically, we use the UNet model for the segmentation of pupil regions in IR images and then find the pupil center as the center of mass of the segment. In designing the loss function for the segmentation, we propose a new loss term that encodes the convex shape-prior for enhancing the robustness to noise. Precisely, we penalize not only the deviation of each predicted pixel from the ground truth label but also the non-convex shape of pupils caused by the noise and reflection. For the training, we make a new dataset of 111,581 images with hand-labeled pupil regions from 29 IR eye video sequences. We also label commonly used datasets (ExCuSe and ElSe dataset) that are considered real-world noisy ones to validate our method. Experiments show that the proposed method performs better than the conventional methods that directly find the pupil center as a regression result.","2169-3536","","10.1109/ACCESS.2020.2985095","Samsung Electronics Company, Ltd; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055424","Convex shape prior;deep learning;pupil segmentation;U-Net","Pupils;Image segmentation;Shape;Estimation;Image edge detection;Noise robustness;Learning systems","eye;image classification;image segmentation;image sequences;learning (artificial intelligence);neural nets;regression analysis;video signal processing","nonconvex shape;pupil segmentation;segmented region;deep learning-based method;shape-prior loss;CNN-based segmentation;noise-robust pupil center detection;hand-labeled pupil regions;pupil center-point regression","","3","","46","CCBY","2 Apr 2020","","","IEEE","IEEE Journals"
"Small Object Detection in Aerial Imagery using RetinaNet with Anchor Optimization","M. Ahmad; M. Abdullah; D. Han","Sejong University,Dept. of Computer Engineering,Seoul,South Korea; Sejong University,Dept. of Computer Engineering,Seoul,South Korea; Sejong University,Dept. of Computer Engineering,Seoul,South Korea","2020 International Conference on Electronics, Information, and Communication (ICEIC)","2 Apr 2020","2020","","","1","3","Deep Learning has successfully solved many computer vision problems sometimes in conjunction with traditional computer vision methods and sometimes by replacing them. In this paper, we aim to solve the problem of object detection by employing different methods from deep learning as well as computer vision. Significant amount of work is done in the domain of generic object detection, where usually objects (foreground) cover majority of image space as compared to background. In this paper we will focus on detecting small objects which constitute a tiny area as compared to background such as aerial imagery where desired objects such as people, cars etc. tend to appear relatively small. Such images have an intrinsic imbalanced class problem because background samples dominate object samples. We propose to use an anchor optimization method which will help reduce unnecessary region proposals as well as it can generate customized anchors depending upon the dataset. It can be used in conjunction with any single stage object detection framework. Its empirically noted that this anchor optimization technique improves accuracy over baseline frameworks.","","978-1-7281-6289-8","10.1109/ICEIC49074.2020.9051269","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9051269","object detection;aerial imagery;anchors;anchor optimization","Deep learning;Computer vision;Optimization methods;Object detection;Proposals;Automobiles","computer vision;gaze tracking;learning (artificial intelligence);neural nets;object detection;optimisation","aerial imagery;deep learning;computer vision;intrinsic imbalanced class problem;small object detection;anchor optimization;single stage object detection;RetinaNet","","1","","15","","2 Apr 2020","","","IEEE","IEEE Conferences"
"How Far Are We From Quantifying Visual Attention in Mobile HCI?","M. Bâce; S. Staal; A. Bulling","Department of Computer Science, ETH Zurich; Department of Computer Science, ETH Zurich; Institute for Visualisation and Interactive SystemsUniversity of Stuttgart","IEEE Pervasive Computing","15 May 2020","2020","19","2","46","55","With an ever-increasing number of mobile devices competing for attention, quantifying when, how often, or for how long users look at their devices has emerged as a key challenge in mobile human-computer interaction. Encouraged by recent advances in automatic eye contact detection using machine learning and device-integrated cameras, we provide a fundamental investigation into the feasibility of quantifying overt visual attention during everyday mobile interactions. In this article, we discuss the main challenges and sources of error associated with sensing visual attention on mobile devices in the wild, including the impact of face and eye visibility, the importance of robust head poses estimation, and the need for accurate gaze estimation. Our analysis informs future research on this emerging topic and underlines the potential of eye contact detection for exciting new applications toward next-generation pervasive attentive user interfaces.","1558-2590","","10.1109/MPRV.2020.2967736","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9052000","","Face recognition;Cameras;Visualization;Estimation;Detectors;Mobile handsets;Human computer interaction","eye;face recognition;human computer interaction;learning (artificial intelligence);mobile computing;pose estimation;user interfaces","quantifying visual attention;mobile HCI;ever-increasing number;mobile devices;mobile human-computer interaction;automatic eye contact detection;machine learning;device-integrated cameras;everyday mobile interactions;sensing visual attention;eye visibility;next-generation pervasive attentive user interfaces","","","","20","IEEE","31 Mar 2020","","","IEEE","IEEE Magazines"
"Gaze Estimation by Exploring Two-Eye Asymmetry","Y. Cheng; X. Zhang; F. Lu; Y. Sato","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science, ETH Zurich, Zurich, Switzerland; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan","IEEE Transactions on Image Processing","30 Mar 2020","2020","29","","5259","5272","Eye gaze estimation is increasingly demanded by recent intelligent systems to facilitate a range of interactive applications. Unfortunately, learning the highly complicated regression from a single eye image to the gaze direction is not trivial. Thus, the problem is yet to be solved efficiently. Inspired by the two-eye asymmetry as two eyes of the same person may appear uneven, we propose the face-based asymmetric regression-evaluation network (FARE-Net) to optimize the gaze estimation results by considering the difference between left and right eyes. The proposed method includes one face-based asymmetric regression network (FAR-Net) and one evaluation network (E-Net). The FAR-Net predicts 3D gaze directions for both eyes and is trained with the asymmetric mechanism, which asymmetrically weights and sums the loss generated by two-eye gaze directions. With the asymmetric mechanism, the FAR-Net utilizes the eyes that can achieve high performance to optimize network. The E-Net learns the reliabilities of two eyes to balance the learning of the asymmetric mechanism and symmetric mechanism. Our FARE-Net achieves leading performances on MPIIGaze, EyeDiap and RT-Gene datasets. Additionally, we investigate the effectiveness of FARE-Net by analyzing the distribution of errors and ablation study.","1941-0042","","10.1109/TIP.2020.2982828","National Natural Science Foundation of China(grant numbers:61972012); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050633","Gaze estimation;asymmetric regression;evaluation network;eye appearance","Estimation;Face;Three-dimensional displays;Cameras;Task analysis;Feature extraction","computer vision;eye;gaze tracking;learning (artificial intelligence);motion estimation;neural nets;regression analysis","two-eye asymmetry;eye gaze estimation;highly complicated regression;single eye image;face-based asymmetric regression-evaluation network;gaze estimation results;face-based asymmetric regression network;E-Net;3D gaze directions;asymmetric mechanism;two-eye gaze directions;FARE-Net achieves;MPIIGaze;EyeDiap;RT-gene datasets","","7","","59","IEEE","30 Mar 2020","","","IEEE","IEEE Journals"
"Driver Gaze Region Estimation Based on Computer Vision","X. Shan; Z. Wang; X. Liu; M. Lin; L. Zhao; J. Wang; G. Wang",Shandong University; Shandong University; Shandong University; Shandong University; Shandong University; Shandong University; Shandong University,"2020 12th International Conference on Measuring Technology and Mechatronics Automation (ICMTMA)","30 Mar 2020","2020","","","357","360","Traditional methods of driver's gaze estimation usually need additional equipment to obtain the driver's facial and eye features, which is difficult to apply in real life. In this paper, a gaze region estimation method based on computer vision is proposed, which reduces the hardware requirements of the experiment. In order to achieve this goal, a new eye feature extraction method and an improved random forest algorithm are adopted in this paper. Finally, the driver's line of sight is determined by combining the head and eye features. The driver's head features are obtained by pose from orthography and scaling with iterations (POSIT) algorithm. Then, the accurate positions of the driver's corner of the eye and the pupils are obtained by the from coarse to fine method, and the eye line direction is estimated according to the relative position of the pupil in the eye area. Finally, the improved random forest is used to estimate the driver's gaze region. The experimental results show that the accuracy of this method is 94.12% in the real driver's gaze data set.","2157-1481","978-1-7281-7081-7","10.1109/ICMTMA50254.2020.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050168","Gaze zone;Driver state;computer vision","","computer vision;driver information systems;eye;face recognition;feature extraction;iterative methods;pose estimation","computer vision;driver;additional equipment;gaze region estimation method;hardware requirements;eye feature extraction method;improved random forest algorithm;iterations algorithm;accurate positions;fine method;eye line direction;relative position;eye area;POSIT;pose from orthography and scaling with iterations algorithm","","","","15","","30 Mar 2020","","","IEEE","IEEE Conferences"
"ISeeColor: Method for Advanced Visual Analytics of Eye Tracking Data","K. Panetta; Q. Wan; S. Rajeev; A. Kaszowska; A. L. Gardony; K. Naranjo; H. A. Taylor; S. Agaian","Department of Electrical and Computer Engineering, Tufts University, Medford, MA, USA; Department of Electrical and Computer Engineering, Tufts University, Medford, MA, USA; Department of Electrical and Computer Engineering, Tufts University, Medford, MA, USA; Department of Psychology, Tufts University, Medford, MA, USA; U.S. Army Combat Capabilities Development Command Soldier Center, Natick, MA, USA; Department of Electrical and Computer Engineering, Tufts University, Medford, MA, USA; Department of Psychology, Tufts University, Medford, MA, USA; Department of Computer Science, The City University of New York, New York, NY, USA","IEEE Access","20 Mar 2020","2020","8","","52278","52287","Recent advances in head-mounted eye-tracking technology have allowed researchers to monitor eye movements during locomotion in real-world environments, increasing the ecological validity of research on human gaze behavior. While collecting eye-tracking data is becoming more accessible, visual analytics of eye-tracking data remains difficult and time-consuming. As such, there is a significant need for developing efficient visualization and analysis tools for large-scale eye-tracking data. This work develops a first-of-its-kind eye-tracking data visualization and analysis system that allows for automatic recognition of independent objects within field-of-vision, using deep-learning-based semantic segmentation. This system recolors the fixated objects-of-interest by integrating gaze fixation information with semantic maps. The system effectively allows researchers to automatically infer what objects users view and for how long in dynamic contexts. The contributions are 1) a data visualization and analysis system that uses deep-learning technology along with eye-tracking data to automatically recognize objects-of-interest from head-mounted eye-tracking video recordings, and 2) a graphical user interface that presents objects-of-interest annotation along with eye-tracking data information. The architecture is tested with an outdoor case study of users walking around the Tufts University campus as part of a navigation study, which was administered by a team of research psychologists.","2169-3536","","10.1109/ACCESS.2020.2980901","U.S. Army Combat Capabilities Development Command(grant numbers:W911QY-15-2-0001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9036879","Eye-trackers;cognitive science;data visualization;data analysis;deep-learning","Data visualization;Visual analytics;Image color analysis;Semantics;Image segmentation;Annotations","gaze tracking;graphical user interfaces;human factors;image segmentation;learning (artificial intelligence);vision","eye-tracking data visualization;ISeeColor;eye-tracking data information;head-mounted eye-tracking video recordings;eye movements;head-mounted eye-tracking technology;visual analytics","","","","53","CCBY","16 Mar 2020","","","IEEE","IEEE Journals"
"Fast Head Pose Estimation via Rotation-Adaptive Facial Landmark Detection for Video Edge Computation","W. Wang; X. Chen; S. Zheng; H. Li","College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, China; College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, China; College of Electronic Information and Automation, Tianjin University of Science and Technology, Tianjin, China; IriStar Technology Ltd., Beijing, China","IEEE Access","10 Mar 2020","2020","8","","45023","45032","The human head pose estimation is an important and challenging problem, which provides the estimation of the head posture in 3D space from 2D image. It is a crucial technique for face recognition, gaze estimation, facial attribute recognition, etc. However, fast head pose estimation executing on the terminal for video edge computation has many challenges due to the computational complexity of the existing algorithms. In this paper, we propose a fast head pose estimation method based on a novel Rotation-Adaptive facial landmark detection powered by Local Binary Feature (RALBF). The landmark detection method is structured through fusing the prior of the rotation information provided by the Progressive Calibration Networks (PCN) face detector to a Local Binary Feature (LBF) based landmark detection method, which improves the robustness against head pose variations and simultaneously keep the computing efficiency. RALBF is trained and tested on 300W dataset and AFLW2000 dataset, it is verified by the accuracy evaluation that RALBF performs better than LBF. To improve the speed of head pose estimation, the 68, 51 and 10 landmarks distribution schemes are explored and compared on speed and accuracy. In the 10 landmarks scheme, the head pose estimation running once only takes 8.3ms on Intel i7-6700HQ CPU and takes 21.8ms on HiSilicon SoC Hi3519AV100, and the average error of Euler angle is 5.9973° when the face yaw angle is between ±35° on AFLW2000 3D dataset. Experiments demonstrate our approach performing well on real scenes.","2169-3536","","10.1109/ACCESS.2020.2977729","Tianjin Science and Technology Committee(grant numbers:18YFZCGX00360,18JCQNJC74700,19YFYSQY00060); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9020163","Head pose estimation;facial landmark detection;PnP problem;local binary features","Face;Pose estimation;Feature extraction;Shape;Detectors;Computational modeling","calibration;computational complexity;face recognition;feature extraction;pose estimation","fast head pose estimation;video edge computation;human head;head posture;gaze estimation;novel rotation-adaptive facial landmark detection;local binary feature based landmark detection method","","2","","38","CCBY","2 Mar 2020","","","IEEE","IEEE Journals"
"Automatic Lung Nodule Detection Combined With Gaze Information Improves Radiologists’ Screening Performance","G. Aresta; C. Ferreira; J. Pedrosa; T. Araújo; J. Rebelo; E. Negrão; M. Morgado; F. Alves; A. Cunha; I. Ramos; A. Campilho","Faculty of Engineering of University of Porto (FEUP), Porto, Portugal; FEUP and INESC TEC, Porto, Portugal; INESC TEC, Porto, Portugal; FEUP and INESC TEC, Porto, Portugal; Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; INESC TEC, University of Trás-os-Montes e Alto Douro (UTAD), Vila Real, Portugal; Faculty of Medicine of University of Porto (FMUP), Porto, Portugal; INESC TEC and FEUP, Porto, Portugal","IEEE Journal of Biomedical and Health Informatics","7 Oct 2020","2020","24","10","2894","2901","Early diagnosis of lung cancer via computed tomography can significantly reduce the morbidity and mortality rates associated with the pathology. However, searching lung nodules is a high complexity task, which affects the success of screening programs. Whilst computer-aided detection systems can be used as second observers, they may bias radiologists and introduce significant time overheads. With this in mind, this study assesses the potential of using gaze information for integrating automatic detection systems in the clinical practice. For that purpose, 4 radiologists were asked to annotate 20 scans from a public dataset while being monitored by an eye tracker device, and an automatic lung nodule detection system was developed. Our results show that radiologists follow a similar search routine and tend to have lower fixation periods in regions where finding errors occur. The overall detection sensitivity of the specialists was $\mathbf {0.67\pm 0.07}$, whereas the system achieved 0.69. Combining the annotations of one radiologist with the automatic system significantly improves the detection performance to similar levels of two annotators. Filtering automatic detection candidates only for low fixation regions still significantly improves the detection sensitivity without increasing the number of false-positives.","2168-2208","","10.1109/JBHI.2020.2976150","LNDetector; European Regional Development Fund; Operational Programme for Competitiveness - COMPETE 2020 Programme; National Fundus; Portuguese funding agency; Fundação para a Ciência e a Tecnologia(grant numbers:POCI-01-0145-FEDER-016673); FCT(grant numbers:SFRH/BD/120435/2016); FCT(grant numbers:SFRH/BD/146437/2019); FCT(grant numbers:SFRH/BD/122365/2016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007735","Lung cancer;computer-aided diagnosis;eye-tracking;deep learning;clinical environment","Lung;Cancer;Sensitivity;Computed tomography;Search problems;Informatics;Task analysis","cancer;computerised tomography;diagnostic radiography;lung;medical image processing","searching lung nodules;mortality rates;computed tomography;lung cancer;automatic detection candidates;detection performance;automatic system;detection sensitivity;automatic lung nodule detection system;eye tracker device;integrating automatic detection systems;gaze information;computer-aided detection systems","Deep Learning;Eye-Tracking Technology;Fixation, Ocular;Humans;Lung Neoplasms;Radiographic Image Interpretation, Computer-Assisted;Radiologists;Tomography, X-Ray Computed","","","26","IEEE","24 Feb 2020","","","IEEE","IEEE Journals"
"Effects of Depth Information on Visual Target Identification Task Performance in Shared Gaze Environments","A. Erickson; N. Norouzi; K. Kim; J. J. LaViola; G. Bruder; G. F. Welch",University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida; University of Central Florida,"IEEE Transactions on Visualization and Computer Graphics","31 Mar 2020","2020","26","5","1934","1944","Human gaze awareness is important for social and collaborative interactions. Recent technological advances in augmented reality (AR) displays and sensors provide us with the means to extend collaborative spaces with real-time dynamic AR indicators of one's gaze, for example via three-dimensional cursors or rays emanating from a partner's head. However, such gaze cues are only as useful as the quality of the underlying gaze estimation and the accuracy of the display mechanism. Depending on the type of the visualization, and the characteristics of the errors, AR gaze cues could either enhance or interfere with collaborations. In this paper, we present two human-subject studies in which we investigate the influence of angular and depth errors, target distance, and the type of gaze visualization on participants' performance and subjective evaluation during a collaborative task with a virtual human partner, where participants identified targets within a dynamically walking crowd. First, our results show that there is a significant difference in performance for the two gaze visualizations ray and cursor in conditions with simulated angular and depth errors: the ray visualization provided significantly faster response times and fewer errors compared to the cursor visualization. Second, our results show that under optimal conditions, among four different gaze visualization methods, a ray without depth information provides the worst performance and is rated lowest, while a combination of a ray and cursor with depth information is rated highest. We discuss the subjective and objective performance thresholds and provide guidelines for practitioners in this field.","1941-0506","","10.1109/TVCG.2020.2973054","National Science Foundation(grant numbers:1564065); Collaborative Award(grant numbers:1800961,1800947,1800922); University of Central Florida; University of Florida; Stanford University; Office of Naval Research(grant numbers:N00014-17-1-2927); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8998348","Shared Gaze;Augmented Reality;Depth Error;Gaze Visualization;Performance Measures","Visualization;Task analysis;Collaboration;Three-dimensional displays;Real-time systems;Gaze tracking;Augmented reality","augmented reality;cognition;data visualisation;groupware;human computer interaction;human factors;neurophysiology;visual perception","fewer errors;cursor visualization;subjective performance thresholds;objective performance thresholds;gaze visualization methods;faster response times;ray visualization;simulated angular depth errors;gaze visualizations ray;dynamically walking crowd;virtual human partner;collaborative task;subjective evaluation;target distance;human-subject studies;collaborations;AR gaze cues;display mechanism;underlying gaze estimation;three-dimensional cursors;real-time dynamic AR indicators;collaborative spaces;sensors;collaborative interactions;social interactions;human gaze awareness;shared gaze environments;visual target identification task performance;depth information","Adolescent;Adult;Augmented Reality;Computer Graphics;Eye-Tracking Technology;Female;Fixation, Ocular;Humans;Male;Task Performance and Analysis;Young Adult","","","46","IEEE","13 Feb 2020","","","IEEE","IEEE Journals"
"Robot Navigation in Crowds by Graph Convolutional Networks With Attention Learned From Human Gaze","Y. Chen; C. Liu; B. E. Shi; M. Liu","Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China; Department of Electronics and Computer Engineering, Hong Kong University of Science and Technology, Hong Kong, China","IEEE Robotics and Automation Letters","25 Feb 2020","2020","5","2","2754","2761","Safe and efficient crowd navigation for mobile robot is a crucial yet challenging task. Previous work has shown the power of deep reinforcement learning frameworks to train efficient policies. However, their performance deteriorates when the crowd size grows. We suggest that this can be addressed by enabling the network to identify and pay attention to the humans in the crowd that are most critical to navigation. We propose a novel network utilizing a graph representation to learn the policy. We first train a graph convolutional network based on human gaze data that accurately predicts human attention to different agents in the crowd as they perform a navigation task based on a top down view of the environment. We incorporate the learned attention into a graph-based reinforcement learning architecture. The proposed attention mechanism enables the assignment of meaningful weightings to the neighbors of the robot, and has the additional benefit of interpretability. Experiments on real-world dense pedestrian datasets with various crowd sizes demonstrate that our model outperforms state-of-art methods, increasing task completion rate by 18.4% and decreasing navigation time by 16.4%.","2377-3766","","10.1109/LRA.2020.2972868","National Natural Science Foundation of China(grant numbers:U1713211); Shenzhen Science, Technology and Innovation Commission (SZSTI)(grant numbers:JCYJ20160428154842603); Research Grant Council of Hong Kong SAR Government, China(grant numbers:11210017,21202816,16211015); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8990034","Autonomous vehicle navigation;deep learning in robotics and automation;social human-robot interaction","Navigation;Task analysis;Reinforcement learning;Robot sensing systems;Collision avoidance;Aggregates","control engineering computing;convolutional neural nets;graph theory;learning (artificial intelligence);mobile robots;path planning","navigation time reduction;human attention;human gaze data;graph representation;deep reinforcement learning frameworks;mobile robot;efficient crowd navigation;safe crowd navigation;graph convolutional network;robot navigation;task completion rate;crowd size;attention mechanism;graph-based reinforcement learning architecture;learned attention;navigation task","","9","","32","IEEE","10 Feb 2020","","","IEEE","IEEE Journals"
"Robust Head Pose Estimation Using Extreme Gradient Boosting Machine on Stacked Autoencoders Neural Network","M. T. Vo; T. Nguyen; T. Le","Institute of Research and Development, Duy Tan University, Da Nang, Vietnam; Faculty of Information Technology, Ho Chi Minh City Open University, Ho Chi Minh City, Vietnam; Faculty of Information Technology, Ho Chi Minh City University of Technology (HUTECH), Ho Chi Minh, Vietnam","IEEE Access","8 Jan 2020","2020","8","","3687","3694","Head pose estimation is an important sign in helping robots and other intelligence machines understand human. It plays a vital role in designing human computer interaction systems because many applications rely on precise results of head pose angles such as human behavior analysis, gaze estimation, 3D head reconstruction etc. This study presents a robust approach for estimating the head pose angles in a single image. More specifically, the proposed system first encodes the global features extracted from Histogram of Oriented Gradients in a multi stacked autoencoders neural network. Based on the hidden nodes in deep layers, Autoencoder has been proposed for feature reduction while maintaining the key information of data. A scalable gradient boosting machine is then employed to train and classify the embedded features. Experiences have evaluated on the Pointing 04 dataset and show that the proposed approach outperforms the state-of-the-art methods with the low head pose angle errors in pitch and yaw as 6.16° and 7.17°, respectively.","2169-3536","","10.1109/ACCESS.2019.2962974","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945218","Head pose estimation;autoencoder;feature reduction;gradient boosting;global features","Head;Magnetic heads;Pose estimation;Feature extraction;Boosting;Neural networks;Two dimensional displays","feature extraction;human computer interaction;image classification;learning (artificial intelligence);neural nets;pose estimation","scalable gradient boosting machine;head pose estimation;extreme gradient boosting machine;stacked autoencoders neural network;human computer interaction systems;feature reduction;feature extraction;histogram of oriented gradients;embedded feature classification","","1","","53","CCBY","30 Dec 2019","","","IEEE","IEEE Journals"
"Iris Feature-Based 3-D Gaze Estimation Method Using a One-Camera-One-Light-Source System","J. Liu; J. Chi; N. Lu; Z. Yang; Z. Wang","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Transactions on Instrumentation and Measurement","9 Jun 2020","2020","69","7","4940","4954","Multicamera or multilight-source is generally used to estimate the 3-D coordinates of the cornea center for 3-D gaze estimation in the existing gaze trackers. Although some model-based methods use one-camera systems to estimate 3-D gazes, which include some user-dependent eye parameters preset by fixed values, a one-camera-one-light-source system is still unable to achieve 3-D gaze estimation by considering individual differences. In this article, an iris feature-based method using one camera and one light source for 3-D gaze estimation is proposed. The iris radius and the kappa angle are determined during user calibration. The optical axis is reconstructed by the iris center and its normal vector from only the iris features. Therefore, with the real-time estimation of kappa angle, the 3-D gaze is estimated by an optimization method. Computer simulations and practical experiments have been performed to analyze the feasibility and robustness of the proposed method. This article's innovation lies in achieving 3-D gaze estimation based on only one-camera and one-light source with calibrated eye-specific parameters, which breaks through the limitation of the existing methods using a 3-D cornea center on the system's hardware requirements of 3-D gaze estimation. The simplified hardware system has great application values, especially in the portable mobile devices widely used today. Moreover, a binocular model is used to optimize the results of the point of regard, which improves the accuracy of 3-D gaze estimation effectively.","1557-9662","","10.1109/TIM.2019.2956612","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1401203); Opening Project of Key Laboratory of Operation Safety Technology on Transport Vehicle, Ministry of Transport, China; Beijing Municipal Natural Science Foundation(grant numbers:4172040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8932669","3-D gaze estimation;iris radius;kappa angle;one-camera-one-light-source (OCOLS);user calibration","Three-dimensional displays;Estimation;Gaze tracking;Calibration;Head;Cameras;Iris","calibration;cameras;eye;face recognition;feature extraction;gaze tracking;image resolution;solid modelling","iris feature-based 3-D gaze estimation method;one-camera-one-light-source system;multicamera;multilight-source;3-D coordinates;cornea center;iris radius;kappa angle;user calibration;normal vector;optimization method;portable mobile devices;binocular model","","2","","43","IEEE","13 Dec 2019","","","IEEE","IEEE Journals"
"Low Cost Gaze Estimation: Knowledge-Based Solutions","I. Martinikorena; A. Larumbe-Bergera; M. Ariz; S. Porta; R. Cabeza; A. Villanueva","Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, Spain; Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, Spain; Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, Spain; Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, Spain; Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, Spain; Department of Electrical, Electronic and Communications Engineering, Public University of Navarre, Pamplona, Spain","IEEE Transactions on Image Processing","10 Jan 2020","2020","29","","2328","2343","Eye tracking technology in low resolution scenarios is not a completely solved issue to date. The possibility of using eye tracking in a mobile gadget is a challenging objective that would permit to spread this technology to non-explored fields. In this paper, a knowledge based approach is presented to solve gaze estimation in low resolution settings. The understanding of the high resolution paradigm permits to propose alternative models to solve gaze estimation. In this manner, three models are presented: a geometrical model, an interpolation model and a compound model, as solutions for gaze estimation for remote low resolution systems. Since this work considers head position essential to improve gaze accuracy, a method for head pose estimation is also proposed. The methods are validated in an optimal framework, I2Head database, which combines head and gaze data. The experimental validation of the models demonstrates their sensitivity to image processing inaccuracies, critical in the case of the geometrical model. Static and extreme movement scenarios are analyzed showing the higher robustness of compound and geometrical models in the presence of user's displacement. Accuracy values of about 3° have been obtained, increasing to values close to 5° in extreme displacement settings, results fully comparable with the state-of-the-art.","1941-0042","","10.1109/TIP.2019.2946452","Ministerio de Economía y Competitividad(grant numbers:TIN2014-52897-R); Ministerio de Ciencia, Innovación y Universidades(grant numbers:TIN2017-84388-R); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8876860","Gaze estimation methods;low resolution;eye tracking","Estimation;Head;Image resolution;Gaze tracking;Databases;Cameras","gaze tracking;image motion analysis;image representation;image resolution;interpolation;pose estimation","image processing;user displacement;knowledge-based solutions;eye tracking technology;low cost gaze estimation;I2Head database;head position;remote low resolution systems;interpolation model;geometrical model;high resolution paradigm;low resolution settings;knowledge based approach","","2","","41","IEEE","21 Oct 2019","","","IEEE","IEEE Journals"
"How is Gaze Influenced by Image Transformations? Dataset and Model","Z. Che; A. Borji; G. Zhai; X. Min; G. Guo; P. Le Callet","Shanghai Key Laboratory of Digital Media Processing and Transmissions, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; MarkableAI Inc., Brooklyn, NY, USA; Shanghai Key Laboratory of Digital Media Processing and Transmissions, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Digital Media Processing and Transmissions, Institute of Image Communication and Network Engineering, Shanghai Jiao Tong University, Shanghai, China; Baidu Research, Institute of Deep Learning, Beijing, China; Équipe Image, Perception et Interaction, Laboratoire des Sciences du Numérique de Nantes, Université de Nantes, Nantes, France","IEEE Transactions on Image Processing","10 Jan 2020","2020","29","","2287","2300","Data size is the bottleneck for developing deep saliency models, because collecting eye-movement data is very time-consuming and expensive. Most of current studies on human attention and saliency modeling have used high-quality stereotype stimuli. In real world, however, captured images undergo various types of transformations. Can we use these transformations to augment existing saliency datasets? Here, we first create a novel saliency dataset including fixations of 10 observers over 1900 images degraded by 19 types of transformations. Second, by analyzing eye movements, we find that observers look at different locations over transformed versus original images. Third, we utilize the new data over transformed images, called data augmentation transformation (DAT), to train deep saliency models. We find that label-preserving DATs with negligible impact on human gaze boost saliency prediction, whereas some other DATs that severely impact human gaze degrade the performance. These label-preserving valid augmentation transformations provide a solution to enlarge existing saliency datasets. Finally, we introduce a novel saliency model based on generative adversarial networks (dubbed GazeGAN). A modified U-Net is utilized as the generator of the GazeGAN, which combines classic “skip connection” with a novel “center-surround connection” (CSC) module. Our proposed CSC module mitigates trivial artifacts while emphasizing semantic salient regions, and increases model nonlinearity, thus demonstrating better robustness against transformations. Extensive experiments and comparisons indicate that GazeGAN achieves state-of-the-art performance over multiple datasets. We also provide a comprehensive comparison of 22 saliency models on various transformed scenes, which contributes a new robustness benchmark to saliency community. Our code and dataset are available at: https://github.com/CZHQuality/Sal-CFS-GAN.","1941-0042","","10.1109/TIP.2019.2945857","National Natural Science Foundation of China(grant numbers:61831015,61771305,61927809,61901260); China Postdoctoral Science Foundation(grant numbers:BX20180197,2019M651496); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8866748","Human gaze;saliency prediction;data augmentation;generative adversarial networks;model robustness","Data models;Observers;Image resolution;Visualization;Mathematical model;Semantics;Robustness","eye;feature extraction;gaze tracking;learning (artificial intelligence);neural nets;object detection;transforms","eye movements;data augmentation transformation;DAT;deep saliency models;human gaze boost saliency prediction;GazeGAN;saliency community;image transformations;eye-movement data;human attention;saliency modeling;high-quality stereotype stimuli;image capture;label-preservation DAT;label-preservation valid augmentation transformations;center-surround connection module;skip connection;generative adversarial networks","","18","","52","IEEE","14 Oct 2019","","","IEEE","IEEE Journals"
"A Dual-Cameras-Based Driver Gaze Mapping System With an Application on Non-Driving Activities Monitoring","L. Yang; K. Dong; A. J. Dmitruk; J. Brighton; Y. Zhao","School of Aerospace, Transport and Manufacturing, Cranfield University, Bedfordshire, U.K.; Chongqing Automotive Collaborative Innovation Centre, Chongqing University, Chongqing, China; School of Aerospace, Transport and Manufacturing, Cranfield University, Bedfordshire, U.K.; School of Aerospace, Transport and Manufacturing, Cranfield University, Bedfordshire, U.K.; School of Aerospace, Transport and Manufacturing, Cranfield University, Bedfordshire, U.K.","IEEE Transactions on Intelligent Transportation Systems","2 Oct 2020","2020","21","10","4318","4327","Characterisation of the driver's non-driving activities (NDAs) is of great importance to the design of the take-over control strategy in Level 3 automation. Gaze estimation is a typical approach to monitor the driver's behaviour since the eye gaze is normally engaged with the human activities. However, current eye gaze tracking techniques are either costly or intrusive which limits their applicability in vehicles. This paper proposes a low-cost and non-intrusive dual-cameras based gaze mapping system that visualises the driver's gaze using a heat map. The challenges introduced by complex head movement during NDAs and camera distortion are addressed by proposing a nonlinear polynomial model to establish the relationship between the face features and eye gaze on the simulated driver's view. The Root Mean Square Error of this system in the in-vehicle experiment for the X and Y direction is 7.80±5.99 pixel and 4.64±3.47 pixel respectively with the image resolution of $1440 \times 1080$ pixels. This system is successfully demonstrated to evaluate three NDAs with visual attention. This technique, acting as a generic tool to monitor driver's visual attention, will have wide applications on NDA characterisation for intelligent design of take over strategy and driving environment awareness for current and future automated vehicles.","1558-0016","","10.1109/TITS.2019.2939676","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8836623","Driver attention evaluation;Level 3 automation;camera mapping;system identification;heat map","Vehicles;Cameras;Feature extraction;Face;Monitoring;Visualization;Gaze tracking","cameras;driver information systems;eye;gaze tracking;image motion analysis;image resolution;road safety","dual-cameras-based driver gaze mapping system;nondriving activities;NDAs;gaze estimation;human activities;current eye gaze;heat map;camera distortion;simulated driver;driving environment awareness;automated vehicles","","3","","37","IEEE","13 Sep 2019","","","IEEE","IEEE Journals"
"MUGGLE: MUlti-Stream Group Gaze Learning and Estimation","N. Zhuang; B. Ni; Y. Xu; X. Yang; W. Zhang; Z. Li; W. Gao","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronics Engineering and Computer Science (EECS), Peking University, Beijing, China","IEEE Transactions on Circuits and Systems for Video Technology","1 Oct 2020","2020","30","10","3637","3650","Being able to accurately predict the common gaze point of a group of persons is of particular interest to precise marketing and automatic group attention assessment. Group gaze estimation faces challenges including small face/head size and outlier observers. To address these challenges, we proposed a novel framework called Multi-stream Group Gaze Learning and Estimation (MUGGLE). The MUGGLE infrastructure includes two inference streams: 1) a holistic stream which utilizes fused attention map as input to a global deep convolutional structure to explore the global geometric configurations and contexts of interesting persons in the scene; and 2) an aggregative stream which robustly aggregates individual gazes via a recurrent structure (e.g., LSTM) to obtain outlier-tolerant estimation. Both streams are seamlessly integrated via a fusion network. Extensive experiments are performed on a fully annotated group gaze image dataset with 8,000+ images and 100,000+ faces (which is publicly releasable). The results demonstrate the effectiveness of the proposed MUGGLE framework in group gaze estimation.","1558-2205","","10.1109/TCSVT.2019.2940479","State Key Research and Development Program(grant numbers:2016YFB1001003); National Science Foundation of China(grant numbers:61976137,U1611461); CCF-Tencent Open Fund; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832257","Group gaze;LSTM;holistic;aggregative","Databases;Streaming media;Observers;Head;Aggregates;Cameras","convolutional neural nets;estimation theory;face recognition;gaze tracking;learning (artificial intelligence)","outlier-tolerant estimation;fully annotated group gaze image dataset;MUGGLE framework;common gaze point;automatic group attention assessment;inference streams;aggregative stream;Multistream Group Gaze Learning and Estimation;group gaze estimation;global deep convolutional structure","","1","","51","IEEE","11 Sep 2019","","","IEEE","IEEE Journals"
"Your Eyes Reveal Your Secrets: An Eye Movement Based Password Inference on Smartphone","Y. Wang; W. Cai; T. Gu; W. Shao","Northwestern Polytechnical University, Xi'an, China; Northwestern Polytechnical University, Xi'an, China; RMIT University, Melbourne, VIC, Australia; RMIT University, Melbourne, VIC, Australia","IEEE Transactions on Mobile Computing","1 Oct 2020","2020","19","11","2714","2730","The widespread use of smartphones has brought great convenience to our daily lives, while at the same time we have been increasingly exposed to security threats. Keystroke security is essential to user privacy protection. In this paper, we present GazeRevealer, a novel side-channel based keystroke inference framework to infer sensitive inputs on smartphone from video recordings of victim's eye patterns captured from smartphone front camera. We observe that eye movements typically follow the keystrokes typing on the number-only soft keyboard during password input. By exploiting eye movement patterns, we are able to infer the passwords being entered. We propose a novel algorithm to extract sensitive eye images from video streams, and classify these images with Support Vector Classification. We also propose a novel classification enhancement algorithm to further improve classification accuracy. Compared with prior keystroke detection approaches, GazeRevealer does not require any external auxiliary devices, and it only relies on smartphone front camera. We evaluate the performance of GazeRevealer on several smartphones under different real-life usage scenarios. The results show that GazeRevealer achieves an inference rate of 77.89 percent for single key number and an inference rate of 84.38 percent for 6-digit password in the ideal case.","1558-0660","","10.1109/TMC.2019.2934690","Key Science and Technology Program(grant numbers:2015GY015); Australian Research Council(grant numbers:DP180103932,DP190101888); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798759","Keystroke inference;gaze estimation;mobile security","Password;Cameras;Keyboards;Estimation;Head;Magnetic heads;Wireless fidelity","authorisation;data privacy;gaze tracking;human computer interaction;image classification;keyboards;mobile computing;pattern classification;smart phones;support vector machines;video recording","eye movement patterns;sensitive eye images;video streams;support vector classification;GazeRevealer;smartphone front camera;password inference;security threats;keystroke security;user privacy protection;inference framework;video recordings;keyboard;keystroke detection approaches;classification enhancement algorithm","","3","","48","IEEE","14 Aug 2019","","","IEEE","IEEE Journals"
"Cross-Validated Locally Polynomial Modeling for 2-D/3-D Gaze Tracking With Head-Worn Devices","D. Su; Y. Li; H. Chen","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong","IEEE Transactions on Industrial Informatics","8 Jan 2020","2020","16","1","510","521","In the context of wearable gaze tracking techniques, the problems of two-dimensional (2-D) and three-dimensional (3-D) gaze estimation can be viewed as inferring 2-D epipolar lines and 3-D visual axes from eye monitoring cameras. To this end, in this article, a simple local polynomial model is proposed to back-project a pupil center onto its corresponding visual axis. Based on this approximation, a homographylike relation is derived in a local manner, and via the Leave-One-Out cross-validation criterion, training gaze samples at one certain depth is leveraged to partition entire input space into multiple overlapping subregions. Then, the gaze data at another depth are utilized to recover the epipolar point, i.e., the image eyeball center. Thus, given a pupil image, the corresponding epipolar line can be determined by the resolved homographylike mapping and the epipolar point. By using the same partition structure, 3-D gaze prediction model can be inferred by solving a nonlinear optimization problem, which aims to minimize the angular disparities between training visual directions and prediction ones. Meanwhile, it is necessary to form a good starting point and suitable constraints for the optimization problem. Otherwise, it may end up with trivial solutions, i.e., faraway eye positions. To facilitate the practical implementation of our proposed method, we also analyze how the spatial distribution of calibration points impacts the model learning accuracy. The experiment results justify the effectiveness of our proposed gaze estimation method for both the normal vision and eyewear users.","1941-0050","","10.1109/TII.2019.2933481","Research Grants Council of Hong Kong(grant numbers:11255716); National Natural Science Foundation of China(grant numbers:61873220); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789483","Two-dimensional/three-dimensional (2-D/3-D) gaze point;human–machine interaction;locally polynomial model;mobile gaze tracking;parallax","Three-dimensional displays;Visualization;Two dimensional displays;Estimation;Solid modeling;Cameras;Calibration","cameras;computer vision;eye;gaze tracking;geometry;learning (artificial intelligence);nonlinear programming;polynomials","two-dimensional gaze estimation;three-dimensional gaze estimation;homography like relation;leave-one-out cross-validation criterion;homography like mapping;3D gaze tracking;2D gaze tracking;2-D epipolar lines;visual directions;model learning accuracy;calibration points;faraway eye positions;nonlinear optimization problem;3-D gaze prediction model;partition structure;epipolar line;pupil image;image eyeball center;epipolar point;gaze data;multiple overlapping subregions;cross-validation criterion;visual axis;pupil center;eye monitoring cameras;3-D visual axes;wearable gaze tracking techniques;head-worn devices;cross-validated locally polynomial modeling","","6","","29","IEEE","6 Aug 2019","","","IEEE","IEEE Journals"
"A Novel Multi-Camera Global Calibration Method for Gaze Tracking System","J. Chi; Z. Yang; G. Zhang; T. Liu; Z. Wang","School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; Key Laboratory of Operation Safety Technology on Transport Vehicle, Ministry of Transport, Beijing, China; School of Automation and Electrical Engineering, University of Science and Technology Beijing, Beijing, China; School of Computer and Communication Engineering, University of Science and Technology Beijing, Beijing, China","IEEE Transactions on Instrumentation and Measurement","7 Apr 2020","2020","69","5","2093","2104","In a 3-D gaze tracking system, the system calibration is used to determine the positions of the light sources and the screen in the camera coordinate system, which plays an important role in the estimation of the 3-D gaze and the point of regard (PoR). The traditional way for the system calibration is the mirror-based method, in which a mirror is used to make the light sources and the screen can be captured by the camera through specular reflection. However, the mirror-based method is imprecise and complex. In this paper, a novel system-calibration method for the 3-D gaze tracking system is proposed. On the basis of the system camera, a calibration camera is placed on the opposite of the screen to constitute a multi-camera system. The images are captured by the two cameras to estimate the light sources and the screen of the system camera coordinate frame. Simulations and experiments were carried out to illustrate the validation of the method. This method solves the problem that the light sources and the screen are not in view of the system camera. In addition, it is easy to implement while ensuring high accuracy and providing a new method for other similar calibration systems.","1557-9662","","10.1109/TIM.2019.2922754","National Basic Research Program of China (973 Program)(grant numbers:2017YFB1002804); Opening Project of Key Laboratory of Operation Safety Technology on Transport Vehicle, Ministry of Transport, China; Natural Science Foundation of Beijing Municipality(grant numbers:4172040); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8763999","Coordinate transformation matrix;Gaze model;Gaze tracking;infrared light source;system global calibration","Cameras;Calibration;Light sources;Three-dimensional displays;Mirrors;Gaze tracking;Estimation","calibration;cameras;gaze tracking;light sources;position measurement","PoR;point of regard;3D gaze estimation;camera coordinate system;calibration camera systems;multicamera global calibration method;mirror-based method;3D gaze tracking system;light source estimation","","5","","21","IEEE","15 Jul 2019","","","IEEE","IEEE Journals"
"Measuring Saccade Latency Using Smartphone Cameras","H. -Y. Lai; G. Saavedra-Peña; C. G. Sodini; V. Sze; T. Heldt","Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Institute for Medical Engineering and Science, and the Microsystems Technology Laboratory, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science and the Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Electrical Engineering and Computer Science, Institute for Medical Engineering and Science, and the Research Laboratory of Electronics, Massachusetts Institute of Technology, Cambridge, MA, USA","IEEE Journal of Biomedical and Health Informatics","5 Mar 2020","2020","24","3","885","897","Objective: Accurate quantification of neurodegenerative disease progression is an ongoing challenge that complicates efforts to understand and treat these conditions. Clinical studies have shown that eye movement features may serve as objective biomarkers to support diagnosis and tracking of disease progression. Here, we demonstrate that saccade latency-an eye movement measure of reaction time-can be measured robustly outside of the clinical environment with a smartphone camera. Methods: To enable tracking of saccade latency in large cohorts of patients and control subjects, we combined a deep convolutional neural network for gaze estimation with a model-based approach for saccade onset determination that provides automated signal-quality quantification and artifact rejection. Results: Simultaneous recordings with a smartphone and a high-speed camera resulted in negligible differences in saccade latency distributions. Furthermore, we demonstrated that the constraint of chinrest support can be removed when recording healthy subjects. Repeat smartphone-based measurements of saccade latency in 11 self-reported healthy subjects resulted in an intraclass correlation coefficient of 0.76, showing our approach has good to excellent test-retest reliability. Additionally, we conducted more than 19000 saccade latency measurements in 29 self-reported healthy subjects and observed significant intra- and inter-subject variability, which highlights the importance of individualized tracking. Lastly, we showed that with around 65 measurements we can estimate mean saccade latency to within less-than-10-ms precision, which takes within 4 min with our setup. Conclusion and Significance: By enabling repeat measurements of saccade latency and its distribution in individual subjects, our framework opens the possibility of quantifying patient state on a finer timescale in a broader population than previously possible.","2168-2208","","10.1109/JBHI.2019.2913846","3M Non-Tenured Faculty; GE Global Research; SenseTime through MIT's Quest for Intelligence; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703178","Eye tracking;convolutional neural networks;health monitoring;saccade latency;mobile imaging","Diseases;Cameras;Task analysis;Visualization;Portable computers;Biomedical measurement;Face","biomechanics;biomedical optical imaging;cameras;convolutional neural nets;diseases;eye;medical image processing;smart phones","smartphone;artifact rejection;gaze estimation;saccade latency measurements;saccade latency distributions;high-speed camera;automated signal-quality quantification;deep convolutional neural network;eye movement;neurodegenerative disease progression;smartphone camera;time 4.0 min","Adult;Algorithms;Eye-Tracking Technology;Female;Humans;Image Processing, Computer-Assisted;Male;Middle Aged;Neural Networks, Computer;Saccades;Smartphone;Young Adult","1","","59","OAPA","30 Apr 2019","","","IEEE","IEEE Journals"
"Gaze-angle Impact on Iris Segmentation using CNNs","E. Jalilian; A. Uhl; M. Karakaya","University of Salzburg,Salzburg,Austria; University of Salzburg,Salzburg,Austria; University of Central Arkansas,Conway,Arkansas,USA","2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)","3 Sep 2020","2019","","","1","9","Emerging standoff iris recognition systems operate under unconstrained conditions and the iris images captured by these systems are more subject to off-angle acquisition distortions. While deep learning techniques (e.g. convolutional neural networks (CNNs)) are increasingly becoming a tool of choice for iris segmentation tasks, yet there is a significant lack of information about how these distortions affect the performance of such networks. In this work, we thoroughly discuss the general effect of different gaze-angles on ocular biometrics and relate the findings to off-angle iris segmentation using CNNs. In particular, we conduct systematical analysis on the impact of different gaze-angles on segmentation performance of two CNNs with different architectures. The networks' performance turns out to have a direct relation to the closeness of gaze-angles in the training and testing images, and it declines as the gaze-angles diverge. We further investigate the effect of (i) increasing the quantity of iris training data in case of gaze-angles in training and test data match, and (ii) considering iris training data consisting of several distinct gaze-angles (we obtain promising results using the second configuration). Finally, we compare our results to those of some classical iris segmentation algorithms, where the CNNs are found to outperform the classical algorithms.","2474-9699","978-1-7281-1522-1","10.1109/BTAS46853.2019.9185970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185970","","Iris recognition;Iris;Image segmentation;Training;Distortion;Cameras","biometrics (access control);eye;feature extraction;image capture;image recognition;image segmentation;image texture;iris recognition;learning (artificial intelligence);neural nets","gaze-angle impact;emerging standoff;recognition systems;iris images;acquisition distortions;deep learning techniques;iris segmentation tasks;different gaze-angles;off-angle iris segmentation;segmentation performance;testing images;gaze-angles diverge;iris training data;distinct gaze-angles;classical iris segmentation algorithms","","1","","31","","3 Sep 2020","","","IEEE","IEEE Conferences"
"A Secure Face Anti-spoofing Approach Using Deep Learning","M. Safarzadeh; M. Ghasemi; J. Khoramdel; A. N. Ardekany","Mechatronics Lab., K. N. Toosi University of Technology,Tehran,Iran; Mechatronics Lab., K. N. Toosi University of Technology,Tehran,Iran; Mechatronics Lab., K. N. Toosi University of Technology,Tehran,Iran; Mechatronics Lab., K. N. Toosi University of Technology,Tehran,Iran","2019 7th International Conference on Robotics and Mechatronics (ICRoM)","20 Apr 2020","2019","","","322","327","Face recognition is an attractive field for researchers since the past years and it is going to be the most practical method in identity recognition in the future. But even modern face detection systems have the problem of spoofing and it is safe to say it is the most serious obstacle in the way of becoming practical in more secure fields of identity recognition. In this paper, firstly, we discuss object detection systems in particular face detection and their problems such as using the person's picture in mobile phones or any other screen-based devices instead of a real person's face and also using masks to make faces like someone's face or other spoofing goals. Then, we introduce our method to prevent these attacks to have a high-secure and reliable face recognition system. Finally, some illustrations will be provided to demonstrate the practical and economic benefits of the presented approach.","2572-6889","978-1-7281-6604-9","10.1109/ICRoM48714.2019.9071842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9071842","Anti-spoofing;Face detection;Thermal;gaze;Face classification;Thermal photography;Gaze response","","face recognition;feature extraction;learning (artificial intelligence);object detection","secure face antispoofing approach;face detection systems;identity recognition;deep learning;economic benefits;practical benefits;high-secure face recognition system;screen-based devices;object detection systems","","","","38","","20 Apr 2020","","","IEEE","IEEE Conferences"
"An Efficient Dense Network for Semantic Segmentation of Eyes Images Captured with Virtual Reality Lens","A. Valenzuela; C. Arellano; J. Tapia",Universidad Andres Bello; Universidad Andres Bello; Universidad Tecnologica de Chile,"2019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","16 Apr 2020","2019","","","28","34","Eye-tracking and Gaze estimation are difficult tasks that may be used for several applications including human-computer interfaces, salience detection and Virtual reality amongst others. This paper presents a segmentation algorithm based on deep learning that efficiently discriminates pupils, iris, and sclera from the background in images captured using a Virtual Reality lens. A light network called DensetNet10 trained from scratch is proposed. It contains fewer parameters than traditional architectures based on DenseNet which allows it to be used in mobile device applications. Experiments show that this network achieved higher IOU rates when comparing with DensetNet56-67-103 and DeeplabV3+ models in the Facebook database. Furthermore, this method reached 8th place in The Facebook semantic segmentation challenge with 0.94293 mean IOU and 202.084 parameters with a final score of 0.97147.","","978-1-7281-5686-6","10.1109/SITIS.2019.00017","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9067945","Biometrics, Semantic Segmentation, Facebook Challengue","Image segmentation;Semantics;Computer architecture;Solid modeling;Convolution;Computational modeling;Virtual reality","eye;feature extraction;human computer interaction;image capture;image segmentation;learning (artificial intelligence);object detection;virtual reality;visual databases","salience detection;segmentation algorithm;deep learning;virtual reality lens;light network;DensetNet10;mobile device applications;DensetNet56-67-103;Facebook semantic segmentation challenge;dense network;eye-tracking;gaze estimation;human-computer interfaces;eyes image capture;IOU rates;DeeplabV3+ model","","1","","18","","16 Apr 2020","","","IEEE","IEEE Conferences"
"Visual Attention-Based Object Detection in Cluttered Environments","E. M. Silva Machado; I. Carrillo; M. Collado; L. Chen",De Montfort University; ISOIN; ISOIN; De Montfort University,"2019 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)","9 Apr 2020","2019","","","133","139","The study of human visual attention is considered a hot topic in the field of activity recognition, experimental psychology research and human computer interaction. The importance of detecting user objects of interest in real time is critical to provide accurate cues about the user intentions.However, current methods for visual attention extraction and object detection suffer from low performance when moving to ongoing condition. Inherent complexity of cluttered environmentsis considered the major barrier to achieve good performances. To address this challenge, we present a novel method that includes head-worn eye tracker and egocentric video. Our method exploits sliding window-based time series approach in conjunction with aHeuristic probabilistic function to analyse user fixations around potential object of interest in an egocentric video. We evaluate the proposed method using a new dataset annotated with user gaze data and object within a frame image. Our experimental results show that our approach can outperforms several state-of-the-art commonality visual attention-based object detection methods.","","978-1-7281-4034-6","10.1109/SmartWorld-UIC-ATC-SCALCOM-IOP-SCI.2019.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9060082","Object Detection;Visual Attention;Cluttered Environments;Fixations;Deep Learning","Visualization;Object detection;Feature extraction;Task analysis;Cameras;Object recognition;Psychology","feature extraction;gaze tracking;object detection;time series;video signal processing","visual attention-based object detection;cluttered environments;human visual attention;activity recognition;experimental psychology research;human computer interaction;user intentions;visual attention extraction;head-worn eye tracker;egocentric video;user gaze data;user fixations;sliding window-based time series","","","","19","","9 Apr 2020","","","IEEE","IEEE Conferences"
"Integrating Action-aware Features for Saliency Prediction via Weakly Supervised Learning","J. Feng; S. Li; Y. Sui; L. Meng; C. Zhu","Research Center of Second Research Insititute of CAAC,Chengdu,China; University of Electronic Science and Technology of China,Chengdu,China; Research Center of Second Research Insititute of CAAC,Chengdu,China; University of Electronic Science and Technology of China,Chengdu,China; University of Electronic Science and Technology of China,Chengdu,China","2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)","5 Mar 2020","2019","","","974","979","Deep learning has been widely studied for saliency prediction. Despite the great performance improvement introduced by deep saliency models, some high-level concepts that contribute to the saliency prediction, such as text, objects of gaze and action, locations of motion, and expected locations of people, have not been explicitly considered. This paper investigates the objects of action and motion, and proposes to use action-aware features to compensate deep saliency models. The action-aware features are generated via weakly supervised learning using an extra action classification network trained with existing image based action datasets. Then a feature fusion module is developed to integrate the action-aware features for saliency prediction. Experiments show that the proposed saliency model with the action-aware features achieves better performance on three public benchmark datasets. More experiments are further conducted to analyze the effectiveness of the action-aware features in saliency prediction. To the best of our knowledge, this study is the first attempt on explicitly integrating objects of action and motion concept into deep saliency models.","2640-0103","978-1-7281-3248-8","10.1109/APSIPAASC47483.2019.9023127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9023127","","Training;Feature extraction;Predictive models;Saliency detection;Neural networks;Context modeling;Supervised learning","feature extraction;image classification;image fusion;image motion analysis;neural nets;supervised learning","action-aware features;saliency prediction;weakly supervised learning;deep saliency models;motion concept;image based action datasets;deep learning;action classification network;feature fusion module","","","","32","","5 Mar 2020","","","IEEE","IEEE Conferences"
"EyeNet: A Multi-Task Deep Network for Off-Axis Eye Gaze Estimation","Z. Wu; S. Rajendran; T. Van As; V. Badrinarayanan; A. Rabinovich","Magic Leap Inc, USA; Magic Leap Inc, USA; Magic Leap Inc, USA; Magic Leap Inc, USA; Magic Leap Inc, USA","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3683","3687","Eye gaze estimation is a crucial component in Virtual and Mixed Reality. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid blocking the user's gaze, this view-point makes drawing eye related inferences very challenging. In this work, we present EyeNet, the first single deep neural network which solves multiple heterogeneous tasks related to eye gaze estimation for an off-axis camera setting. The tasks include eye segmentation, IR LED glints detection, pupil and cornea center estimation. We benchmark all tasks on MagicEyes, a large and new dataset of 587 subjects with varying morphology, gender, skin-color, make-up and imaging conditions.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00455","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022317","Eye Tracking;Gaze Estimation;mixed reality;computer vision;deep learning;multi task learning","Cornea;Estimation;Three-dimensional displays;Task analysis;Two dimensional displays;Cameras;Light emitting diodes","cameras;eye;gaze tracking;neural nets","EyeNet;multitask deep network;off-axis eye gaze estimation;eye related inferences;single deep neural network;multiple heterogeneous tasks;off-axis camera setting;eye segmentation;pupil;cornea center estimation;IR LED glints detection;MagicEyes","","1","","20","","5 Mar 2020","","","IEEE","IEEE Conferences"
"A Generalized and Robust Method Towards Practical Gaze Estimation on Smart Phone","T. Guo; Y. Liu; H. Zhang; X. Liu; Y. Kwak; B. I. Yoo; J. Han; C. Choi","Samsung Research China - Beijing, China; Samsung Research China - Beijing, China; Samsung Research China - Beijing, China; Samsung Research China - Beijing, China; Samsung Advanced Institute of Technology, Korea; Samsung Advanced Institute of Technology, Korea; Samsung Advanced Institute of Technology, Korea; Samsung Advanced Institute of Technology, Korea","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1131","1139","Gaze estimation for ordinary smart phone, e.g. estimating where the user is looking at on the phone screen, can be applied in various applications. However, the widely used appearance-based CNN methods still have two issues for practical adoption. First, due to the limited dataset, gaze estimation is very likely to suffer from over-fitting, leading to poor accuracy at run time. Second, the current methods are usually not robust, i.e. their prediction results having notable jitters even when the user is performing gaze fixation, which degrades user experience greatly. For the first issue, we propose a new tolerant and talented (TAT) training scheme, which is an iterative random knowledge distillation framework enhanced with cosine similarity pruning and aligned orthogonal initialization. The knowledge distillation is a tolerant teaching process providing diverse and informative supervision. The enhanced pruning and initialization is a talented learning process prompting the network to escape from the local minima and re-born from a better start. For the second issue, we define a new metric to measure the robustness of gaze estimator, and propose an adversarial training based Disturbance with Ordinal loss (DwO) method to improve it. The experimental results show that our TAT method achieves state-of-the-art performance on GazeCapture dataset, and that our DwO method improves the robustness while keeping comparable accuracy.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022325","Gaze estimation;robustness;over fitting;smart phone","Training;Estimation;Robustness;Jitter;Measurement;Task analysis;Face","gaze tracking;iterative methods;learning (artificial intelligence);mobile computing;smart phones","smart phone;gaze estimation;phone screen;gaze fixation;user experience;iterative random knowledge distillation framework;enhanced pruning;gaze estimator;DwO method;ordinal loss method;appearance-based CNN methods;TAT method;tolerant and talented training scheme;TAT training scheme","","1","","31","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Learning to Personalize in Appearance-Based Gaze Tracking","E. Lindén; J. Sjöstrand; A. Proutiere",Tobii; Tobii; KTH Royal Insitute of Technology,"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1140","1148","Personal variations severely limit the performance of appearance-based gaze tracking. Adapting to these variations using standard neural network model-adaption methods is difficult. The problems range from overfitting, due to small amounts of training data, to underfitting, due to restrictive model architectures. We tackle these problems by introducing SPatial Adaptive GaZe Estimator (SPAZE ). By modeling personal variations as a low-dimensional latent parameter space, SPAZE provides just enough adaptability to capture the range of personal variations without being prone to overfitting. Calibrating SPAZE for a new person reduces to solving a small and simple optimization problem. SPAZE achieves an error of 2.70 degrees on the MPIIGaze dataset, improving on the state-of-the-art by 14 %. We contribute to gaze tracking research by empirically showing that personal variations are well-modeled as a 3-dimensional latent parameter space for each eye. We show that this low-dimensionality is expected by examining model-based approaches to gaze tracking.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00145","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022231","appearance based gaze estimation;convolutional neural network;deep learning;gaze tracking","Computer vision;Conferences","gaze tracking;neural nets","SPAZE;personal variations;3-dimensional latent parameter space;appearance-based gaze tracking;restrictive model architectures;low-dimensional latent parameter space;spatial adaptive gaze estimator;neural network model-adaption methods;MPIIGaze dataset","","1","","25","","5 Mar 2020","","","IEEE","IEEE Conferences"
"D-ID-Net: Two-Stage Domain and Identity Learning for Identity-Preserving Image Generation From Semantic Segmentation","N. Damer; F. Boutros; F. Kirchbuchner; A. Kuijper",Fraunhofer Institute for Computer Graphics Research IGD; Fraunhofer Institute for Computer Graphics Research IGD; Fraunhofer Institute for Computer Graphics Research IGD; Fraunhofer Institute for Computer Graphics Research IGD,"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3677","3682","Training functionality-demanding AR/VR systems require accurate and robust gaze estimation and tracking solutions. Achieving such a performance requires the availability of diverse eye image data that might only be acquired by the means of image generation. Works addressing the generation of such images did not target realistic and identity-specific images, nor did they address the practical-relevant case of generation from semantic labels. Therefore, this work proposes a solution to generate realistic and identity-specific images that correspond to semantic labels, given samples of a specific identity. Our proposed solution consists of two stages. In the first stage, a network is trained to transform the semantic label into a corresponding eye image of a generic identity. The second stage is an identity-specific network that induces identity details on the generic eye image. The results of our D-ID-Net solutions shows a high degree of identity-preservation and similarity to the ground-truth images, with an RMSE of 7.235.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9021978","Biometric image generation;Eye biometrics;Identity preservation","Semantics;Training;Image segmentation;Gallium nitride;Image generation;Databases;Robustness","eye;image segmentation;learning (artificial intelligence)","D-ID-Net solutions;identity-preservation;ground-truth images;two-stage domain;identity learning;identity-preserving image generation;semantic segmentation;robust gaze estimation;diverse eye image data;identity-specific images","","3","","26","","5 Mar 2020","","","IEEE","IEEE Conferences"
"On-Device Few-Shot Personalization for Real-Time Gaze Estimation","J. He; K. Pham; N. Valliappan; P. Xu; C. Roberts; D. Lagun; V. Navalpakkam",Google Inc.; University of Maryland at College Park; Google Inc.; Google Inc.; Google Inc.; Google Inc.; Google Inc.,"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1149","1158","Building fast and accurate gaze estimation models without additional specialized hardware is a hard problem. In this paper, we present on-device few-shot personalization methods for 2D gaze estimation. The proposed supervised method achieves better accuracy using as few as 2-5 calibration points per user compared to prior methods that require more than 13 calibration points. In addition, we propose an unsupervised personalization method which uses only unlabeled facial images to improve gaze estimation accuracy. Our best personalized model achieves 24-26% better accuracy (measured by mean error) on phones compared to the state-of-the-art using <; =5 calibration points per user. It is also computationally efficient, requiring 20x fewer FLOPS when compared to prior methods. This unlocks a variety of important real world applications such as using gaze for accessibility, gaming and human-computer interaction while running entirely on-device in real-time.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00146","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9021975","gaze;gaze tracking;few shot learning;on device learning;personalization","Estimation;Calibration;Computational modeling;Face;Two dimensional displays;Data models;Cameras","face recognition;flip-flops;gaze tracking;image sensors;unsupervised learning","real-time gaze estimation;accurate gaze estimation models;supervised method;calibration points;unsupervised personalization method;unlabeled facial images;gaze estimation accuracy;personalized model;on-device few-shot personalization methods;2D gaze estimation","","8","","49","","5 Mar 2020","","","IEEE","IEEE Conferences"
"RT-BENE: A Dataset and Baselines for Real-Time Blink Estimation in Natural Environments","K. Cortacero; T. Fischer; Y. Demiris","Imperial College London, United Kingdom; Imperial College London, United Kingdom; Imperial College London, United Kingdom","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1159","1168","In recent years gaze estimation methods have made substantial progress, driven by the numerous application areas including human-robot interaction, visual attention estimation and foveated rendering for virtual reality headsets. However, many gaze estimation methods typically assume that the subject's eyes are open; for closed eyes, these methods provide irregular gaze estimates. Here, we address this assumption by first introducing a new open-sourced dataset with annotations of the eye-openness of more than 200,000 eye images, including more than 10,000 images where the eyes are closed. We further present baseline methods that allow for blink detection using convolutional neural networks. In extensive experiments, we show that the proposed baselines perform favourably in terms of precision and recall. We further incorporate our proposed RT-BENE baselines in the recently presented RT-GENE gaze estimation framework where it provides a real-time inference of the openness of the eyes. We argue that our work will benefit both gaze estimation and blink estimation methods, and we take steps towards unifying these methods.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00147","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022030","blink detection;gaze estimation;deep learning;Convolutional neural network","Estimation;Feature extraction;Head;Semantics;Task analysis;Labeling;Glass","convolutional neural nets;estimation theory;gaze tracking;inference mechanisms","blink estimation methods;human-robot interaction;visual attention estimation;virtual reality headsets;open-sourced dataset;blink detection;irregular gaze estimation methods;RT-GENE gaze estimation framework;convolutional neural networks;inference mechanism","","1","","50","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Eye-MMS: Miniature Multi-Scale Segmentation Network of Key Eye-Regions in Embedded Applications","F. Boutros; N. Damer; F. Kirchbuchner; A. Kuijper","Fraunhofer Institute for Computer Graphics Research IGD, Germany and Technische Universität Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Germany and Technische Universität Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Germany and Technische Universität Darmstadt, Germany; Fraunhofer Institute for Computer Graphics Research IGD, Germany and Technische Universität Darmstadt, Germany","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3665","3670","Segmentation of the iris or sclera is an essential processing block in ocular biometric systems. However, human-computer interaction, as in VR/AR applications, requires multiple region segmentation to enable smoother interaction and eye-tracking. Such application does not only demand highly accurate and generalizable segmentation, it requires such segmentation model to be appropriate for the limited computational power of embedded systems. This puts strict limits on the size of the deployed deep learning models. This work presents a miniature multi-scale segmentation network consisting of inter-connected convolutional modules. We present a baseline multi-scale segmentation network and modify it to reduce its parameters by more than 80 times, while reducing its accuracy by less than 3%, resulting in our Eye-MMS model containing only 80k parameters. This work is developed on the OpenEDS database and is conducted in preparation for the OpenEDS Semantic Segmentation Challenge.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00452","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022048","Semantic segmentation;Biometrics;Eye segmentation;Embedded biometrics","Image segmentation;Iris recognition;Databases;Solid modeling;Semantics;Image resolution;Computational modeling","augmented reality;biometrics (access control);embedded systems;eye;gaze tracking;human computer interaction;image segmentation;learning (artificial intelligence);neural nets","miniature multiscale segmentation network;eye regions;embedded applications;ocular biometric systems;human-computer interaction;multiple region segmentation;eye-tracking;embedded systems;deep learning;baseline multiscale segmentation network;Eye-MMS model;OpenEDS Semantic Segmentation Challenge;OpenEDS database;AR application;VR application","","9","","26","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Multitask Learning to Improve Egocentric Action Recognition","G. Kapidis; R. Poppe; E. van Dam; L. Noldus; R. Veltkamp","Noldus Information Technology / Utrecht University, The Netherlands; Utrecht University, The Netherlands; Noldus Information Technology, The Netherlands; Noldus Information Technology, The Netherlands; Utrecht University, The Netherlands","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","4396","4405","In this work we employ multitask learning to capitalize on the structure that exists in related supervised tasks to train complex neural networks. It allows training a network for multiple objectives in parallel, in order to improve performance on at least one of them by capitalizing on a shared representation that is developed to accommodate more information than it otherwise would for a single task. We employ this idea to tackle action recognition in egocentric videos by introducing additional supervised tasks. We consider learning the verbs and nouns from which action labels consist of and predict coordinates that capture the hand locations and the gaze-based visual saliency for all the frames of the input video segments. This forces the network to explicitly focus on cues from secondary tasks that it might otherwise have missed resulting in improved inference. Our experiments on EPIC-Kitchens and EGTEA Gaze+ show consistent improvements when training with multiple tasks over the single-task baseline. Furthermore, in EGTEA Gaze+ we outperform the state-of-the-art in action recognition by 3.84%. Apart from actions, our method produces accurate hand and gaze estimations as side tasks, without requiring any additional input at test time other than the RGB video clips.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022063","egocentric vision;multitask learning;action recognition;gaze estimation;hand detection","Task analysis;Videos;Activity recognition;Training;Visualization;Three-dimensional displays;Streaming media","image colour analysis;image motion analysis;image segmentation;learning (artificial intelligence);neural nets;pose estimation;video signal processing","nouns;hand locations;gaze-based visual saliency;input video segments;improved inference;EGTEA Gaze;action labels;EGTEA Gaze+;EPIC-Kitchens;verbs;supervised tasks;egocentric videos;shared representation;complex neural networks;multitask;egocentric action recognition;RGB video clips;gaze estimations;single-task baseline","","1","","58","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Learning Spatiotemporal Attention for Egocentric Action Recognition","M. Lu; D. Liao; Z. Li","Simon Faser Universiy, Canada; Huawei Technologies, Canada; Zhejiang University, China; Simon Faser Universiy, Canada","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","4425","4434","Recognizing camera wearers' actions from videos captured by the head-mounted camera is a challenging task. Previous methods often utilize attention models to characterize the relevant spatial regions to facilitate egocentric action recognition. Inspired by the recent advances of spatiotemporal feature learning using 3D convolutions, we propose a simple yet efficient module for learning spatiotemporal attention in egocentric videos with human gaze as supervision. Our model employs a two-stream architecture which consists of an appearance-based stream and motion-based stream. Each stream has the spatiotemporal attention module (STAM) to produce an attention map, which helps our model to focus on the relevant spatiotemporal regions of the video for action recognition. The experimental results demonstrate that our model is able to outperform the state-of-the-art methods by a large margin on the standard EGTEA Gaze+ dataset and produce attention maps that are consistent with human gaze.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022139","Spatiotemporal Attention;Egocentric Action Recognition;STAM;Gaze","Spatiotemporal phenomena;Videos;Three-dimensional displays;Predictive models;Task analysis;Cameras;Training","cameras;gaze tracking;image motion analysis;image recognition;learning (artificial intelligence);spatiotemporal phenomena;video signal processing","human gaze;two-stream architecture;appearance-based stream;motion-based stream;spatiotemporal attention module;egocentric action recognition;head-mounted camera;attention models;relevant spatial regions;spatiotemporal feature learning;egocentric videos;spatiotemporal attention learning;STAM;EGTEA Gaze+ dataset","","2","","39","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Facial Pose Estimation by Deep Learning from Label Distributions","Z. Liu; Z. Chen; J. Bai; S. Li; S. Lian",cloudminds; cloudminds; Beihang University; cloudminds; cloudminds,"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1232","1240","Facial pose estimation has gained a lot of attentions in many practical applications, such as human-robot interaction, gaze estimation and driver monitoring. Meanwhile, end-to-end deep learning-based facial pose estimation is becoming more and more popular. However, facial pose estimation suffers from a key challenge: the lack of sufficient training data for many poses, especially for large poses. Inspired by the observation that the faces under close poses look similar, we reformulate the facial pose estimation as a label distribution learning problem, considering each face image as an example associated with a Gaussian label distribution rather than a single label, and construct a convolutional neural network which is trained with a multi-loss function on AFLW dataset and 300W-LP dataset to predict the facial poses directly from color image. Extensive experiments are conducted on several popular benchmarks, including AFLW2000, BIWI, AFLW and AFW, where our approach shows a significant advantage over other state-of-the-art methods.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00156","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022536","facial pose estimation;deep learning;label distribution","Pose estimation;Training;Cameras;Head;Machine learning;Three-dimensional displays","convolutional neural nets;face recognition;Gaussian distribution;learning (artificial intelligence);pose estimation","deep learning-based facial pose estimation;label distribution learning problem;Gaussian label distribution;color image;convolutional neural network","","5","","60","","5 Mar 2020","","","IEEE","IEEE Conferences"
"SalGaze: Personalizing Gaze Estimation using Visual Saliency","Z. Chang; J. M. Di Martino; Q. Qiu; S. Espinosa; G. Sapiro",Duke University; Duke University; Duke University; Duke University; Duke University,"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1169","1178","Traditional gaze estimation methods typically require explicit user calibration to achieve high accuracy. This process is cumbersome and recalibration is often required when there are changes in factors such as illumination and pose. To address this challenge, we introduce SalGaze, a framework that utilizes saliency information in the visual content to transparently adapt the gaze estimation algorithm to the user without explicit user calibration. We design an algorithm to transform a saliency map into a differentiable loss map that can be used for the optimization of CNN-based models. SalGaze is also able to greatly augment standard point calibration data with implicit video saliency calibration data using a unified framework. We show accuracy improvements over 24% using our technique on existing methods.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00148","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022412","Gaze Estimation;Saliency;Deep Learning;Convolutional Neural Network;Calibration","Estimation;Calibration;Visualization;Machine learning;Head;Computational modeling;Lighting","convolutional neural nets;gaze tracking;optimisation;pose estimation;video signal processing","SalGaze;visual saliency;explicit user calibration;saliency information;visual content;gaze estimation algorithm;saliency map;standard point calibration data;CNN","","1","","57","","5 Mar 2020","","","IEEE","IEEE Conferences"
"U2Eyes: A Binocular Dataset for Eye Tracking and Gaze Estimation","S. Porta; B. Bossavit; R. Cabeza; A. Larumbe-Bergera; G. Garde; A. Villanueva","Public University of Navarre, Spain; Trinity College Dublin, Ireland; Public University of Navarre, Spain; Public University of Navarre, Spain; Public University of Navarre, Spain; Public University of Navarre, Spain","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3660","3664","Theory shows that huge amount of labelled data are needed in order to achieve reliable classification/regression methods when using deep/machine learning techniques. However, in the eye tracking field, manual annotation is not a feasible option due to the wide variability to be covered. Hence, techniques devoted to synthesizing images show up as an opportunity to provide vast amounts of annotated data. Considering that the well-known UnityEyes tool provides a framework to generate single eye images and taking into account that both eyes information can contribute to improve gaze estimation accuracy we present U2Eyes dataset, that is publicly available. It comprehends about 6 million of synthetic images containing binocular data. Furthermore, the physiology of the eye model employed is improved, simplified dynamics of binocular vision are incorporated and more detailed 2D and 3D labelled data are provided. Additionally, an example of application of the dataset is shown as work in progress. Employing U2Eyes as training framework Supervised Descent Method (SDM) is used for eyelids segmentation. The model obtained as result of the training process is then applied on real images from GI4E dataset showing promising results.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00451","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022577","eye tracking;low resolution;binocular dataset;unity;annotation","Estimation;Gaze tracking;Visualization;Face;Cameras;Eyelids","computer vision;eye;feature extraction;gaze tracking;image classification;image segmentation;learning (artificial intelligence);regression analysis","U2Eyes dataset;synthetic images;binocular data;eye model;binocular vision;labelled data;training framework;binocular dataset;eye tracking field;annotated data;single eye images;eyes information;gaze estimation accuracy;UnityEyes tool;supervised descent method;eyelids segmentation;GI4E dataset","","1","","14","","5 Mar 2020","","","IEEE","IEEE Conferences"
"RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking","A. K. Chaudhary; R. Kothari; M. Acharya; S. Dangi; N. Nair; R. Bailey; C. Kanan; G. Diaz; J. B. Pelz","Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; Rochester Institute of Technology, USA","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3698","3702","Accurate eye segmentation can improve eye-gaze estimation and support interactive computing based on visual attention; however, existing eye segmentation methods suffer from issues such as person-dependent accuracy, lack of robustness, and an inability to be run in real-time. Here, we present the RITnet model, which is a deep neural network that combines U-Net and DenseNet. RITnet is under 1 MB and achieves 95.3% accuracy on the 2019 OpenEDS Semantic Segmentation challenge. Using a GeForce GTX 1080 Ti, RITnet tracks at > 300Hz, enabling real-time gaze tracking applications. Pre-trained models and source code are available.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00568","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022181","","Semantics;Image segmentation;Computational modeling;Iris;Robustness;Convolution;Real-time systems","gaze tracking;image segmentation;learning (artificial intelligence);neural nets","GeForce GTX 1080 Ti;DenseNet;U-Net;2019 OpenEDS semantic segmentation challenge;gaze tracking applications;person-dependent accuracy;eye segmentation methods;eye-gaze estimation;deep neural network;RITnet model","","4","","19","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Unsupervised Outlier Detection in Appearance-Based Gaze Estimation","Z. Chen; D. Deng; J. Pi; B. E. Shi","The Hong Kong University of Science and Technology, Hong Kong SAR; The Hong Kong University of Science and Technology, Hong Kong SAR; The Hong Kong University of Science and Technology, Hong Kong SAR; The Hong Kong University of Science and Technology, Hong Kong SAR","2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","1088","1097","Appearance-based gaze estimation maps RGB images to estimates of gaze directions. One problem in gaze estimation is that there always exist low-quality samples (outliers) in which the eyes are barely visible. These low-quality samples are mainly caused by blinks, occlusions (e.g. by eye glasses), blur (e.g. due to motion) and failures of the eye landmark detection. Training on these outliers degrades the performance of gaze estimators, since they have no or limited information about gaze directions. It is also risky to give estimates based on these images in real-world applications, as these estimates may be unreliable. To solve this problem, we propose an algorithm that detects outliers without supervision. Based on the input images with only gaze labels, the proposed algorithm learns to predict a gaze estimates and an additional confidence score, which alleviates the impact of outliers during learning. We evaluated this algorithm on the MPIIGaze dataset and on an internal dataset. In cross-subject evaluation, our experimental results show that the proposed algorithm results in a better gaze estimator (8% improvement). The proposed algorithm is also able to reliably detect outliers during testing, with a precision of 0.71 when the recall is 0.63.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00139","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022626","Appearance based gaze estimation;Outlier detection","Estimation;Feature extraction;Training;Head;Testing;Lighting;Detectors","computer vision;feature extraction;gaze tracking;image colour analysis;learning (artificial intelligence);motion estimation;neural nets","MPIIGaze dataset;RGB images;appearance-based gaze estimation maps;unsupervised outlier detection;gaze labels;eye landmark detection;low-quality samples;gaze directions","","1","","44","","5 Mar 2020","","","IEEE","IEEE Conferences"
"MinENet: A Dilated CNN for Semantic Segmentation of Eye Features","J. Perry; A. Fernandez",University of Texas at San Antonio; University of Texas at San Antonio,"2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)","5 Mar 2020","2019","","","3671","3676","Fast and accurate eye tracking is a critical task for a range of research in virtual and augmented reality, attention tracking, mobile applications, and medical analysis. While deep neural network models excel at image analysis tasks, existing approaches to segmentation often consider only one class, emphasize classification over segmentation, or come with prohibitively high resource costs. In this work, we propose MinENet, a minimized efficient neural network architecture designed for fast multi-class semantic segmentation. We demonstrate performance of MinENet on the OpenEDS Semantic Segmentation Challenge dataset, against a baseline model as well as standard state-of-the-art neural network architectures - a convolutional neural network (CNN) and a dilated CNN. Our encoder-decoder architecture improves accuracy of multi-class segmentation of eye features in this large-scale high-resolution dataset, while also providing a design that is demonstrably lightweight and efficient.","2473-9944","978-1-7281-5023-9","10.1109/ICCVW.2019.00453","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022100","image segmentation;cnn;deep learning;eye tracking","Image segmentation;Semantics;Complexity theory;Task analysis;Neural networks;Decoding;Iris","convolutional neural nets;feature extraction;gaze tracking;image classification;image resolution;image segmentation;neural net architecture","convolutional neural network;CNN;encoder-decoder architecture;multiclass segmentation;eye features;large-scale high-resolution dataset;MinENet;eye tracking;virtual reality;augmented reality;attention tracking;mobile applications;medical analysis;deep neural network models;image analysis tasks;classification;OpenEDS Semantic Segmentation Challenge dataset;neural network architectures;multiclass semantic segmentation","","6","","18","","5 Mar 2020","","","IEEE","IEEE Conferences"
"Gaze360: Physically Unconstrained Gaze Estimation in the Wild","P. Kellnhofer; A. Recasens; S. Stent; W. Matusik; A. Torralba",MIT; Massachusetts Institute of Technology; Toyota Research Institute; MIT; MIT,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","6911","6920","Understanding where people are looking is an informative social cue. In this work, we present Gaze360, a large-scale remote gaze-tracking dataset and method for robust 3D gaze estimation in unconstrained images. Our dataset consists of 238 subjects in indoor and outdoor environments with labelled 3D gaze across a wide range of head poses and distances. It is the largest publicly available dataset of its kind by both subject and variety, made possible by a simple and efficient collection method. Our proposed 3D gaze model extends existing models to include temporal information and to directly output an estimate of gaze uncertainty. We demonstrate the benefits of our model via an ablation study, and show its generalization performance via a cross-dataset evaluation against other recent gaze benchmark datasets. We furthermore propose a simple self-supervised approach to improve cross-dataset domain adaptation. Finally, we demonstrate an application of our model for estimating customer attention in a supermarket setting. Our dataset and models will be made available at http://gaze360.csail.mit.edu.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00701","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010825","","Cameras;Three-dimensional displays;Estimation;Face;Adaptation models;Lighting","gaze tracking;pose estimation;stereo image processing","gaze benchmark datasets;cross-dataset domain adaptation;3D gaze model;simple collection method;head poses;outdoor environments;indoor environments;unconstrained images;large-scale remote gaze-tracking dataset;gaze estimation","","10","","35","","27 Feb 2020","","","IEEE","IEEE Conferences"
"SID4VAM: A Benchmark Dataset With Synthetic Images for Visual Attention Modeling","D. Berga; X. R. F. Vidal; X. Otazu; X. M. Pardo",Computer Vision Center; University of Santiago de Compostela; Computer Vision Center; Universidade de Santiago de Compostela,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","8788","8797","A benchmark of saliency models performance with a synthetic image dataset is provided. Model performance is evaluated through saliency metrics as well as the influence of model inspiration and consistency with human psychophysics. SID4VAM is composed of 230 synthetic images, with known salient regions. Images were generated with 15 distinct types of low-level features (e.g. orientation, brightness, color, size...) with a target-distractor pop-out type of synthetic patterns. We have used Free-Viewing and Visual Search task instructions and 7 feature contrasts for each feature category. Our study reveals that state-of-the-art Deep Learning saliency models do not perform well with synthetic pattern images, instead, models with Spectral/Fourier inspiration outperform others in saliency metrics and are more consistent with human psychophysical experimentation. This study proposes a new way to evaluate saliency models in the forthcoming literature, accounting for synthetic images with uniquely low-level feature contexts, distinct from previous eye tracking image datasets.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00888","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008799","","Visualization;Task analysis;Computational modeling;Measurement;Gaze tracking;Predictive models;Benchmark testing","gaze tracking;learning (artificial intelligence);neural nets","SID4VAM;visual attention modeling;saliency metrics;target-distractor pop-out type;synthetic patterns;synthetic pattern images;eye tracking image datasets;visual search task instructions;human psychophysics;free-viewing;deep learning saliency models","","","","64","","27 Feb 2020","","","IEEE","IEEE Conferences"
"Few-Shot Adaptive Gaze Estimation","S. Park; S. D. Mello; P. Molchanov; U. Iqbal; O. Hilliges; J. Kautz",ETH Zurich; NVIDIA Research; NVIDIA; NVIDIA Research; ETH Zurich; NVIDIA,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","9367","9376","Inter-personal anatomical differences limit the accuracy of person-independent gaze estimation networks. Yet there is a need to lower gaze errors further to enable applications requiring higher quality. Further gains can be achieved by personalizing gaze networks, ideally with few calibration samples. However, over-parameterized neural networks are not amenable to learning from few examples as they can quickly over-fit. We embrace these challenges and propose a novel framework for Few-shot Adaptive GaZE Estimation (Faze) for learning person-specific gaze networks with very few (≤ 9) calibration samples. Faze learns a rotation-aware latent representation of gaze via a disentangling encoder-decoder architecture along with a highly adaptable gaze estimator trained using meta-learning. It is capable of adapting to any new person to yield significant performance gains with as few as 3 samples, yielding state-of-the-art performance of 3.18-deg on GazeCapture, a 19% improvement over prior art. We open-source our code at https://github.com/NVlabs/few_shot_gaze.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008783","","Head;Estimation;Task analysis;Calibration;Training;Decoding;Robustness","gaze tracking;learning (artificial intelligence);motion estimation;neural net architecture","over-parameterized neural networks;few-shot adaptive gaze estimation;person-independent gaze estimation;encoder-decoder architecture;meta-learning","","10","","55","","27 Feb 2020","","","IEEE","IEEE Conferences"
"Photo-Realistic Monocular Gaze Redirection Using Generative Adversarial Networks","Z. He; A. Spurr; X. Zhang; O. Hilliges",ETH Zürich and University of Zürich; ETH; ETH Zurich; ETH Zurich,"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","6931","6940","Gaze redirection is the task of changing the gaze to a desired direction for a given monocular eye patch image. Many applications such as videoconferencing, films, games, and generation of training data for gaze estimation require redirecting the gaze, without distorting the appearance of the area surrounding the eye and while producing photo-realistic images. Existing methods lack the ability to generate perceptually plausible images. In this work, we present a novel method to alleviate this problem by leveraging generative adversarial training to synthesize an eye image conditioned on a target gaze direction. Our method ensures perceptual similarity and consistency of synthesized images to the real images. Furthermore, a gaze estimation loss is used to control the gaze direction accurately. To attain high-quality images, we incorporate perceptual and cycle consistency losses into our architecture. In extensive evaluations we show that the proposed method outperforms state-of-the-art approaches in terms of both image quality and redirection precision. Finally, we show that generated images can bring significant improvement for the gaze estimation task if used to augment real training data.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9008804","","Task analysis;Gallium nitride;Estimation;Training;Generators;Training data;Three-dimensional displays","estimation theory;gaze tracking;learning (artificial intelligence)","videoconferencing;perceptually plausible imaging;monocular eye patch imaging;generative adversarial networks;photorealistic monocular gaze redirection;training data;cycle consistency losses;gaze estimation loss;target gaze direction;generative adversarial training;photorealistic imaging","","9","","35","","27 Feb 2020","","","IEEE","IEEE Conferences"
"Real-Time Algorithms for Head Mounted Gaze Tracker","A. Starostenko; F. Kozin; R. Gorbachev",Moscow Institute of Physics and Technology; Moscow Institute of Physics and Technology; Moscow Institute of Physics and Technology,"2019 International Conference on Artificial Intelligence: Applications and Innovations (IC-AIAI)","24 Feb 2020","2019","","","86","863","We introduce a set of real-time algorithms for head mounted gaze tracker consisting of three cameras: two cameras for the eyes and one camera for the scene. The direction of the optical axis of the eye in three-dimensional space is calculated using the reflection of IR LEDs from the cornea. Individual features of the user are taken into account using the short-term calibration procedure. The described algorithms combine high accuracy in determining the point of gaze with high speed. The procedure for determining the point of gaze consists of the following algorithms: estimation of the position of the pupils on the eye cameras frames using of the threshold processing taking into account the histogram of the frame and further approximation of the pupil by an ellipse; estimation of the IR LEDs glare position on the frames of the eye cameras using threshold processing; filtration of the glares by brightness, size, circularity, and of the glares beyond the iris, the size of the iris is estimated by the distance from eye camera to pupil position calculated on the previous frame; indexation of the glares with the template matching; estimation of the optical axis angles of the eye using a spherical model of the cornea with the nonlinear optimization methods; estimation of the point of gaze on the scene camera frame using individual user features found during the calibration process. During calibration, the movement of the ArUco calibration mark and its selection on the scene camera frame are used. To calculate the gaze position on the scene camera, a regression algorithm is used, which implicitly takes into account the individual characteristics of the user.","","978-1-7281-4326-2","10.1109/IC-AIAI48757.2019.00025","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007317","gaze estimation;head mount devices;object detection and tracking;human-computer interaction","Cameras;Calibration;Estimation;Light emitting diodes;Mathematical model;Cornea;Optical refraction","calibration;cameras;eye;image sensors;image sequences;pose estimation;regression analysis","gaze tracker;real-time algorithms;three-dimensional space;cornea;short-term calibration procedure;described algorithms;eye cameras frames;threshold processing;eye camera;pupil position;previous frame;optical axis angles;scene camera frame;calibration process;gaze position;regression algorithm;IR LED glare position","","","","7","","24 Feb 2020","","","IEEE","IEEE Conferences"
"Single-trial Classification of Fixation-related Potentials in Guided Visual Search Tasks using A Riemannian Network","J. Shen; X. Li; H. Zeng; A. Song","Southeast University,Jiangsu Key Lab of Remote Sense and Control,Nanjing,China; Southeast University,Jiangsu Key Lab of Remote Sense and Control,Nanjing,China; Southeast University,Jiangsu Key Lab of Remote Sense and Control,Nanjing,China; Southeast University,Jiangsu Key Lab of Remote Sense and Control,Nanjing,China","2019 IEEE Symposium Series on Computational Intelligence (SSCI)","20 Feb 2020","2019","","","375","379","Brain responses to visual stimulus can provide information about cognitive process or intentions. Several studies show that it is feasible to use stimulus-dependent modulation of the evoked brain responses after gaze movements (i.e., Fixation Related Potential, FRP) to predict the interested object of human. However, the performance of the state-of the-art shallow models for FRP classification is still far from satisfactory. Recent years, Riemannian geometry based on deep learning has gained its popularity in many image and video processing tasks, thanks to their ability to learn appropriate statistical representations while respecting Riemannian geometry of the data in such fields. In this paper, we have investigated a Riemannian network for classifying FRP in guided visual search task. Experiment results showed that the Riemannian network improved classification performance significantly in comparison to the shallow methods.","","978-1-7281-2485-8","10.1109/SSCI44817.2019.9002946","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9002946","fixation related potential;guided visual search;deep learning;Riemannian geometry","Covariance matrices;Electroencephalography;Visualization;Task analysis;Brain modeling;Feature extraction;Geometry","brain;cognition;electroencephalography;learning (artificial intelligence);medical signal processing;neurophysiology;visual evoked potentials","visual search task;single-trial classification;fixation-related potentials;visual stimulus;cognitive process;stimulus-dependent modulation;evoked brain responses;gaze movements;FRP classification;Riemannian geometry;deep learning;video processing tasks;Riemannian network classification performance","","","","20","","20 Feb 2020","","","IEEE","IEEE Conferences"
"Maneuvering Assistance of Teleoperation Robot Based on Identification of Gaze Movement","R. Arita; S. Suzuki","Tokyo Denki University,Department of Robotics and Mechatronics,Tokyo,Japan,120-8551; Tokyo Denki University,Department of Robotics and Mechatronics,Tokyo,Japan,120-8551","2019 IEEE 17th International Conference on Industrial Informatics (INDIN)","30 Jan 2020","2019","1","","565","570","Narrow angle of view and the time delay of command make operation of a teleoperation robots difficult. In order to solve this problem, a teleoperation system utilizing the followings is presented in this paper: measurement of operator's gaze movement, estimation of gaze point, and the saliency of monitor image proposes. In this system, operator's future gaze target beyond several seconds is estimated using the gaze motion model and an estimated gaze region from probability of saliency map. This paper introduces the configuration of the support system, modeling of gaze movement, generation of the probability of saliency map, and measurement/analysis in robot maneuver preliminary experiment.","2378-363X","978-1-7281-2927-3","10.1109/INDIN41052.2019.8972290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972290","Teleoperation robot;Gaze estimation;Saliency map","","estimation theory;gaze tracking;motion control;position control;probability;robot vision;telerobotics","gaze movement;teleoperation robot;teleoperation system;gaze point estimation;gaze motion model;gaze region estimation;saliency map;maneuvering assistance","","1","","22","","30 Jan 2020","","","IEEE","IEEE Conferences"
"Where Am I Looking: Localizing Gaze In Reconstructed 3D Space","D. Parikh; Y. Lu; Y. Xin; D. Wu; J. Pelz; G. Lu","Rochester Institute of Technology,Chester Carlson Center for Imaging Science,USA; Rochester Institute of Technology,Chester Carlson Center for Imaging Science,USA; Tencent Deep Sea Lab,China; Tencent Deep Sea Lab,China; Rochester Institute of Technology,Chester Carlson Center for Imaging Science,USA; Rochester Institute of Technology,Chester Carlson Center for Imaging Science,USA","2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","28 Jan 2020","2019","","","1","5","We propose a method to estimate the 3D gaze of the observer onto the scene using a portable eye tracker with a monocular camera. We reconstruct the 3D scene using Structure from Motion (SfM) and use camera pose and 3D reconstruction information of the scene to localize the position and pose of the observer in the 3D reconstructed space. Along with the position, we can obtain the 2D gaze of the observer using an eye-tracker. Each person may have a different perspective of the same 3D object in the scene, observing it from different positions. We use this information to fuse these multiple perspectives in 3D space to get a better understanding of how differently each observer perceives the same scene, compared to others. In the entire system, we developed a convo-lutional neural network to detect and track eye movement and a camera re-localization method to localize the camera in 3D environment. Based on our novel eye tracking and camera re-localization methods, we can accurately localize the gaze in the 3D reconstructed environment.","","978-1-7281-2723-1","10.1109/GlobalSIP45357.2019.8969158","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8969158","","","cameras;convolutional neural nets;gaze tracking;image reconstruction;pose estimation;solid modelling;stereo image processing","3D space reconstruction;portable eye tracker;monocular camera;3D reconstruction information;eye movement;3D gaze estimation;3D scene reconstructed environment;structure from motion;convolutional neural network;camera re-localization method","","","","24","","28 Jan 2020","","","IEEE","IEEE Conferences"
"Efficient Pupil Detection with a Convolutional Neural Network","C. Miron; A. Pasarica; R. G. Bozomitu; V. Manta; R. Timofte; R. Ciucu","”Gheorghe Asachi” Technical University,Iasi,Romania; ”Gheorghe Asachi” Technical University,Iasi,Romania; ”Gheorghe Asachi” Technical University,Iasi,Romania; ”Gheorghe Asachi” Technical University,Iasi,Romania; ”Gheorghe Asachi” Technical University,Iasi,Romania; Politehnica University of Bucharest,Bucharest,Romania","2019 E-Health and Bioengineering Conference (EHB)","28 Jan 2020","2019","","","1","4","The paper proposes an efficient convolutional neural network for pupil detection capable of state-of-the art accuracy without pretraining. The system architecture consists of twelve convolution layers and two max pool layers with only 108450 prediction parameters. The validation conducted on ExCuSe datasets show that our solution achieves state-of-the-art performance in terms of detection rate within a pixel error bound.","2575-5145","978-1-7281-2603-6","10.1109/EHB47216.2019.8969984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8969984","eye tracking;convolutional neural network;machine learning;deep learning","","convolutional neural nets;gaze tracking;learning (artificial intelligence)","pupil detection;convolutional neural network;system architecture;convolution layers;max pool layers;detection rate;pixel error bound","","","","25","","28 Jan 2020","","","IEEE","IEEE Conferences"
"Region-wise Polynomial Regression for 3D Mobile Gaze Estimation","D. Su; Y. F. Li; H. Chen","City University of Hong Kong, Kowloon,Department of Mechanical Engineering,Hong Kong; City University of Hong Kong, Kowloon,Department of Mechanical Engineering,Hong Kong; City University of Hong Kong, Kowloon,Department of Mechanical Engineering,Hong Kong","2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","28 Jan 2020","2019","","","907","913","In the context of mobile gaze tracking techniques, a 3D gaze point can be calculated as the middle point between two 3D visual axes. To infer gaze directions and eyeball positions, a nonlinear optimization problem is typically formulated to minimize the angular disparities between the training gaze directions and prediction ones. Nonetheless, the experimental results reported by some previous works show that this kind of approaches are very likely to yield large prediction errors hence considered less useful for human-machine interactions. In this study, we aim to address this widespread issue in three aspects. At first, instead of using a global regression model, a simple local polynomial model is proposed to back-project a pupil center onto its corresponding visual axis. Based on the Leave-One-Out cross-validation criterion, the partition structure is automatically learned in the process of resolving a homography-like relationship. Secondly, a good starting point for nonlinear-optimization is obtained by the image eyeball center, which can be estimated by systematic parallax errors. Meanwhile, it is necessary to add the suitable constraints for 3D eye positions. Otherwise, the optimization may end up with trivial solutions, i.e., faraway eye positions. Thirdly, we explore a strategy for designing the spatial distribution of calibration points in a principled manner. The experiment results demonstrate that an encouraging gaze estimation accuracy can be achieved by our proposed framework for both the normal vision and eyewear users.","2153-0866","978-1-7281-4004-9","10.1109/IROS40897.2019.8968536","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8968536","","","eye;gaze tracking;human computer interaction;learning (artificial intelligence);optical tracking;polynomials;regression analysis","region-wise polynomial regression;3D mobile gaze estimation;mobile gaze tracking techniques;3D gaze point;3D visual axes;eyeball positions;nonlinear optimization problem;angular disparities;human-machine interactions;global regression model;simple local polynomial model;pupil center;visual axis;cross-validation criterion;image eyeball center;systematic parallax errors;3D eye positions;faraway eye positions;calibration points;gaze estimation","","","","18","","28 Jan 2020","","","IEEE","IEEE Conferences"
"Application of Deep Learning to Pre-processing of Cousumer's Eye Tracking Data in Supermarket","K. Ishibashi",Kansai University,"2019 International Conference on Data Mining Workshops (ICDMW)","13 Jan 2020","2019","","","341","348","The purpose of this study is to automate pre-processing of eye tracking data. In an investigation with eye tracking in a field such as supermarket, pre-processing of data has enormous cost. This is because it is very difficult to map consumer's gaze points to certain snapshot due to her/his movement. This study uses deep learning for automating pre-processing of eye tracking data. General object recognition using deep learning can classify an image into various classes. The proposed method attempts to classify fixated object of consumer into three classes, product, promotion and etc., based on the result of general object recognition. This paper discusses the applicability of proposed method through cross validation using eye tracking data preprocessed manually.","2375-9259","978-1-7281-4896-0","10.1109/ICDMW.2019.00057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955495","eye tracking;consumer behavior;pre-processing;depp learning;general object recognition","Gaze tracking;Object recognition;Deep learning;Visualization;Motion pictures;Estimation;Data mining","eye;feature extraction;gaze tracking;image classification;image recognition;learning (artificial intelligence);object recognition","eye tracking data;supermarket;map consumer;deep learning;automating pre-processing","","","","11","","13 Jan 2020","","","IEEE","IEEE Conferences"
"Obscured 3D Point-of-Gaze Estimation by Multipoint Cloud Data","W. Pichitwong; K. Chamnongthai","king mongkut’s university of technology thonburi,dept. electronic and telecommunication engineering,Bangkok,Thailand; king mongkut’s university of technology thonburi,dept. electronic and telecommunication engineering,Bangkok,Thailand","2019 16th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)","13 Jan 2020","2019","","","947","950","Point cloud sensor is currently used to sense point cloud data information which is 3D position on the surface of target object. Point cloud data information can be input to improve for 3D point of gaze estimation (3D POG). Presently, there are limitation on creating point cloud data information on target object since point cloud data cannot be found if any obstacle in front of the sensor, there are shadow projection. This paper proposes method of multipoint cloud data to create point cloud data on the surface of target object and obscured point cloud data in the shadow projection. Eye tracker sensor provides 3D eyes position data and 2D POG on screen data which each origin represents center of eye tracker and center of screen respectively. These mentioned data are integrated by model fitting to draw a straight line, originating from the center point between left pupil and right pupil, which passes through the2D POG on virtual screen and ends when the line meets the closest point on the target object. In performance evaluation of proposed method, firstly the obscured point cloud data are successfully defined. Secondly, experiment by 4 participants by watching 9 units of testing objects at 2 seconds in free move provide the result of 3D POG estimation at average distance errors by 1.09 cm.","","978-1-7281-3361-4","10.1109/ECTI-CON47248.2019.8955244","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955244","3-D gaze estimation;multipoint cloud data","","gaze tracking;human computer interaction;image sensors;object detection;stereo image processing;visual perception","multipoint cloud data;point cloud sensor;point cloud data information;target object;3D POG;3D eyes position data;2D POG;screen data;obscured 3D point-of-gaze estimation;virtual screen","","","","11","","13 Jan 2020","","","IEEE","IEEE Conferences"
"Mixed Effects Neural Networks (MeNets) With Applications to Gaze Estimation","Y. Xiong; H. J. Kim; V. Singh",Univ. of Wisconsin-Madison; Korea Univ.; Univ. of Wisconsin-Madison,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","7735","7744","There is much interest in computer vision to utilize commodity hardware for gaze estimation. A number of papers have shown that algorithms based on deep convolutional architectures are approaching accuracies where streaming data from mass-market devices can offer good gaze tracking performance, although a gap still remains between what is possible and the performance users will expect in real deployments. We observe that one obvious avenue for improvement relates to a gap between some basic technical assumptions behind most existing approaches and the statistical properties of the data used for training. Specifically, most training datasets involve tens of users with a few hundreds (or more) repeated acquisitions per user. The non i.i.d. nature of this data suggests better estimation may be possible if the model explicitly made use of such “repeated measurements” from each user as is commonly done in classical statistical analysis using so-called mixed effects models. The goal of this paper is to adapt these “mixed effects” ideas from statistics within a deep neural network architecture for gaze estimation, based on eye images. Such a formulation seeks to specifically utilize information regarding the hierarchical structure of the training data - each node in the hierarchy is a user who provides tens or hundreds of repeated samples. This modification yields an architecture that offers state of the art performance on various publicly available datasets improving results by 10-20%.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954429","Face;Gesture;and Body Pose;Motion and Tracking","","computer vision;gaze tracking;learning (artificial intelligence);neural net architecture;statistical analysis","gaze tracking performance;mass-market devices;streaming data;deep convolutional architectures;commodity hardware;computer vision;training data;gaze estimation;deep neural network architecture;mixed effects models;classical statistical analysis;training datasets;statistical properties;performance users","","10","","62","","9 Jan 2020","","","IEEE","IEEE Conferences"
"Improving Few-Shot User-Specific Gaze Adaptation via Gaze Redirection Synthesis","Y. Yu; G. Liu; J. Odobez","Idiap, and EPFL; Idiap Research Institute; IDIAP/EPFL, SWITZERLAND","2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","11929","11938","As an indicator of human attention gaze is a subtle behavioral cue which can be exploited in many applications. However, inferring 3D gaze direction is challenging even for deep neural networks given the lack of large amount of data (groundtruthing gaze is expensive and existing datasets use different setups) and the inherent presence of gaze biases due to person-specific difference. In this work, we address the problem of person-specific gaze model adaptation from only a few reference training samples. The main and novel idea is to improve gaze adaptation by generating additional training samples through the synthesis of gaze-redirected eye images from existing reference samples. In doing so, our contributions are threefold:(i) we design our gaze redirection framework from synthetic data, allowing us to benefit from aligned training sample pairs to predict accurate inverse mapping fields; (ii) we proposed a self-supervised approach for domain adaptation; (iii) we exploit the gaze redirection to improve the performance of person-specific gaze estimation. Extensive experiments on two public datasets demonstrate the validity of our gaze retargeting and gaze estimation framework.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01221","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953732","Face;Gesture;and Body Pose;Image and Video Synthesis; Vision Applications and Systems","","feature extraction;gaze tracking;image representation;learning (artificial intelligence);neural nets;pose estimation","person-specific gaze model adaptation;reference training samples;gaze-redirected eye images;gaze redirection framework;domain adaptation;person-specific gaze estimation;gaze retargeting;gaze estimation framework;few-shot user-specific gaze adaptation;gaze redirection synthesis;human attention gaze;subtle behavioral cue;3D gaze direction;deep neural networks;self-supervised approach","","6","","47","","9 Jan 2020","","","IEEE","IEEE Conferences"
"LAEO-Net: Revisiting People Looking at Each Other in Videos","M. J. Marín-Jiménez; V. Kalogeiton; P. Medina-Suárez; A. Zisserman",Univ. of Córdoba; Univ. of Oxford; Univ. of Cordoba; Univ. of Oxford,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","3472","3480","Capturing the `mutual gaze' of people is essential for understanding and interpreting the social interactions between them. To this end, this paper addresses the problem of detecting people Looking At Each Other (LAEO) in video sequences. For this purpose, we propose LAEO-Net, a new deep CNN for determining LAEO in videos. In contrast to previous works, LAEO-Net takes spatio-temporal tracks as input and reasons about the whole track. It consists of three branches, one for each character's tracked head and one for their relative position. Moreover, we introduce two new LAEO datasets: UCO-LAEO and AVA-LAEO. A thorough experimental evaluation demonstrates the ability of LAEO-Net to successfully determine if two people are LAEO and the temporal window where it happens. Our model achieves state-of-the-art results on the existing TVHID-LAEO video dataset, significantly outperforming previous approaches.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8954303","Face;Gesture;and Body Pose;Action Recognition ; Deep Learning ; Video Analytics; Vision Applications and Systems","","computer vision;convolutional neural nets;image sequences;object detection;pose estimation;video signal processing","LAEO-Net;TVHID-LAEO video dataset;video sequences;UCO-LAEO;AVA-LAEO;Looking At Each Other;deep CNN;computer vision;deep convolutional neural network","","5","","33","","9 Jan 2020","","","IEEE","IEEE Conferences"
"Generalizing Eye Tracking With Bayesian Adversarial Learning","K. Wang; R. Zhao; H. Su; Q. Ji",RPI; Rensselaer Polytechnic Institute; IBM; Rensselaer Polytechnic Institute,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","11899","11908","Existing appearance-based gaze estimation approaches with CNN have poor generalization performance. By systematically studying this issue, we identify three major factors: 1) appearance variations; 2) head pose variations and 3) over-fitting issue with point estimation. To improve the generalization performance, we propose to incorporate adversarial learning and Bayesian inference into a unified framework. In particular, we first add an adversarial component into traditional CNN-based gaze estimator so that we can learn features that are gaze-responsive but can generalize to appearance and pose variations. Next, we extend the point-estimation based deterministic model to a Bayesian framework so that gaze estimation can be performed using all parameters instead of only one set of parameters. Besides improved performance on several benchmark datasets, the proposed method also enables online adaptation of the model to new subjects/environments, demonstrating the potential usage for practical real-time eye tracking applications.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953976","Face;Gesture;and Body Pose","","belief networks;convolutional neural nets;feature extraction;gaze tracking;learning (artificial intelligence);pose estimation","Bayesian adversarial learning;appearance-based gaze estimation;point estimation;Bayesian inference;point-estimation based deterministic model;real-time eye tracking applications;head pose variations;CNN-based gaze estimator","","8","","55","","9 Jan 2020","","","IEEE","IEEE Conferences"
"Neuro-Inspired Eye Tracking With Eye Movement Dynamics","K. Wang; H. Su; Q. Ji",RPI; IBM; Rensselaer Polytechnic Institute,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","9823","9832","Generalizing eye tracking to new subjects/environments remains challenging for existing appearance-based methods. To address this issue, we propose to leverage on eye movement dynamics inspired by neurological studies. Studies show that there exist several common eye movement types, independent of viewing contents and subjects, such as fixation, saccade, and smooth pursuits. Incorporating generic eye movement dynamics can therefore improve the generalization capabilities. In particular, we propose a novel Dynamic Gaze Transition Network (DGTN) to capture the underlying eye movement dynamics and serve as the topdown gaze prior. Combined with the bottom-up gaze measurements from the deep convolutional neural network, our method achieves better performance for both within-dataset and cross-dataset evaluations compared to state-of-the-art. In addition, a new DynamicGaze dataset is also constructed to study eye movement dynamics and eye gaze estimation.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.01006","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953735","Face;Gesture;and Body Pose","","convolutional neural nets;gaze tracking;user interfaces","neuro-inspired eye tracking;appearance-based methods;eye gaze estimation;generic eye movement dynamics;dynamic gaze transition network;eye tracking generalization;DGTN;bottom-up gaze measurements;deep convolutional neural network","","2","","55","","9 Jan 2020","","","IEEE","IEEE Conferences"
"Emotion-Aware Human Attention Prediction","M. O. Cordel; S. Fan; Z. Shen; M. S. Kankanhalli",De La Salle Univ.; National Univ. of Singapore; National Univ. of Singapore; National Univ. of Singapore,"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","9 Jan 2020","2019","","","4021","4030","Despite the recent success in face recognition and object classification, in the field of human gaze prediction, computer models are still struggling to accurately mimic human attention. One main reason is that visual attention is a complex human behavior influenced by multiple factors, ranging from low-level features (e.g., color, contrast) to high-level human perception (e.g., objects interactions, object sentiment), making it difficult to model computationally. In this work, we investigate the relation between object sentiment and human attention. We first introduce a new evaluation metric (AttI) for measuring human attention that focuses on human fixation consensus. A series of empirical data analyses with AttI indicate that emotion-evoking objects receive attention favor, especially when they co-occur with emotionally-neutral objects, and this favor varies with different image complexity. Based on the empirical analyses, we design a deep neural network for human attention prediction which allows the attention bias on emotion-evoking objects to be encoded in its feature space. Experiments on two benchmark datasets demonstrate its superior performance, especially on metrics that evaluate relative importance of salient regions. This research provides the clearest picture to date on how object sentiments influence human attention, and it makes one of the first attempts to model this phenomenon computationally.","2575-7075","978-1-7281-3293-8","10.1109/CVPR.2019.00415","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8953511","Scene Analysis and Understanding;Deep Learning ; Others; Representation Learning; Vision Applications and Systems","","emotion recognition;face recognition;feature extraction;learning (artificial intelligence);neural nets;object detection;visual perception","human fixation consensus;emotion-evoking objects;attention favor;emotionally-neutral objects;attention bias;object sentiment;emotion-aware human attention prediction;face recognition;object classification;human gaze prediction;visual attention;complex human behavior;high-level human perception","","2","","39","","9 Jan 2020","","","IEEE","IEEE Conferences"
"OGaze: Gaze Prediction in Egocentric Videos for Attentional Object Selection","M. Al-Naser; S. A. Siddiqui; H. Ohashi; S. Ahmed; N. Katsuyki; S. Takuto; A. Dengel","German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Hitachi Ltd., Tokyo, Japan; German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany; Hitachi Ltd., Tokyo, Japan; Hitachi Ltd., Tokyo, Japan; German Research Center for Artificial Intelligence (DFKI), Kaiserslautern, Germany","2019 Digital Image Computing: Techniques and Applications (DICTA)","2 Jan 2020","2019","","","1","8","This paper proposes a novel gaze-estimation model for attentional object selection tasks. The key features of our model are two-fold: (i) usage of the deformable convolutional layers to better incorporate spatial dependencies of different shapes of objects and background, (ii) formulation of the gaze-estimation problem in two different ways, i.e. as a classification as well as a regression problem. We combine the two different formulations using a joint loss that incorporates both the cross-entropy as well as the mean-squared error in order to train our model. The experimental results on two publicly available datasets indicates that our model not only achieved real-time performance (13-18 FPS), but also outperformed the state-of-the-art models on the OSdataset along with comparable performance on GTEA-plus dataset.","","978-1-7281-3857-2","10.1109/DICTA47822.2019.8945893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945893","","Videos;Estimation;Predictive models;Computational modeling;Feature extraction;Solid modeling;Task analysis","entropy;feature extraction;gaze tracking;image motion analysis;learning (artificial intelligence);regression analysis;video signal processing","deformable convolutional layers;incorporate spatial dependencies;gaze-estimation problem;regression problem;joint loss;cross-entropy;mean-squared error;OGaze;gaze prediction;egocentric videos;novel gaze-estimation model;attentional object selection tasks","","1","","33","","2 Jan 2020","","","IEEE","IEEE Conferences"
"Real-Time Human Gaze Estimation","T. Rowntree; C. Pontecorvo; I. Reid",The University of Adelaide; Defence Science and Technology; The University of Adelaide,"2019 Digital Image Computing: Techniques and Applications (DICTA)","2 Jan 2020","2019","","","1","7","This paper describes a system for estimating the course gaze or 1D head pose of multiple people in a video stream from a moving camera in an indoor scene. The system runs at 30 Hz and can detect human heads with a F-Score of 87.2% and predict their gaze with an average error 20.9° including when they are facing directly away from the camera. The system uses two Convolutional Neural Networks (CNNs) for head detection and gaze estimation respectively and uses common tracking and filtering techniques for smoothing predictions over time. This paper is application-focused and so describes the individual components of the system as well as the techniques used for collecting data and training the CNNs.","","978-1-7281-3857-2","10.1109/DICTA47822.2019.8945919","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945919","","Head;Agriculture;Cameras;Target tracking;Estimation;Magnetic heads;Kalman filters","convolutional neural nets;gaze tracking;image filtering;image motion analysis;object detection;object tracking;pose estimation;smoothing methods;video cameras;video streaming","video stream;moving camera;indoor scene;convolutional neural networks;filtering techniques;real time human gaze estimation;course gaze estimation;1D head pose;human head detection;tracking techniques;smoothing predictions;CNN","","","","12","","2 Jan 2020","","","IEEE","IEEE Conferences"
"MedGaze: Gaze Estimation on WCE Images Based on a CNN Autoencoder","G. Dimas; D. Iakovidis; A. Koulaouzidis",University of Thessaly; University of Thessaly; Royal Infirmary of Edinburgh,"2019 IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE)","27 Dec 2019","2019","","","363","367","The interpretation of medical images depends on physicians' experience. Over time, physicians develop their ability to examine the images, and this is usually reflected on gaze patterns they follow to observe visual cues, which lead them to diagnostic decisions. In the context of gaze prediction, graph and machine learning methods have been proposed for the visual saliency estimation on generic images. In this work we preset a novel and robust gaze estimation methodology based on physicians' eye fixations, using convolutional neural networks combined with regularization methods, on medical images taken during Wireless Capsule Endoscopy (WCE). Furthermore, we present a novel dataset of physicians' eye fixation patterns which was used for the training of the neural network model. The model was able to achieve 68.5% Judd's Area Under the receiver operating Characteristic (AUC-J).","2471-7819","978-1-7281-4617-1","10.1109/BIBE.2019.00071","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941676","machine learning, convolutional neural networks, saliency, eye-tracking, gaze estimation","Medical services;Visualization;Biomedical imaging;Training;Estimation;Gaze tracking;Decoding","convolutional neural nets;endoscopes;feature extraction;gaze tracking;graph theory;learning (artificial intelligence);medical image processing","MedGaze;WCE images;CNN autoencoder;medical images;gaze patterns;visual cues;diagnostic decisions;gaze prediction;graph theory;machine learning methods;visual saliency estimation;convolutional neural networks;regularization methods;physician eye fixations;wireless capsule endoscopy;Judd area under the receiver operating characteristic;gaze estimation methodology","","2","","24","","27 Dec 2019","","","IEEE","IEEE Conferences"
"Context-Aware Driver Distraction Severity Classification using LSTM Network","A. Fasanmade; S. Aliyu; Y. He; A. H. Al-Bayatti; M. S. Sharif; A. S. Alfakeeh","Faculty of Computing, Engineering and Media, De Montfort University,Leicester,UK; Faculty of Computing, Engineering and Media, De Montfort University,Leicester,UK; Faculty of Computing, Engineering and Media, De Montfort University,Leicester,UK; Faculty of Computing, Engineering and Media, De Montfort University,Leicester,UK; School of Architecture, Computing and Engineering, College of Arts, Technology and Innovation, UEL University Way, Dockland Campus,London,UK,E16 2RD; Faculty of Computing and Information Technology, King Abdulaziz University,Jeddah,KSA","2019 International Conference on Computing, Electronics & Communications Engineering (iCCECE)","27 Dec 2019","2019","","","147","152","Advanced Driving Assistance Systems (ADAS) has been a critical component in vehicles and vital to the safety of vehicle drivers and public road transportation systems. In this paper, we present a deep learning technique that classifies drivers' distraction behaviour using three contextual awareness parameters: speed, manoeuver and event type. Using a video coding taxonomy, we study drivers' distractions based on events information from Regions of Interest (RoI) such as hand gestures, facial orientation and eye gaze estimation. Furthermore, a novel probabilistic (Bayesian) model based on the Long short-term memory (LSTM) network is developed for classifying driver's distraction severity. This paper also proposes the use of frame-based contextual data from the multi-view TeleFOT naturalistic driving study (NDS) data monitoring to classify the severity of driver distractions. Our proposed methodology entails recurrent deep neural network layers trained to predict driver distraction severity from time series data.","","978-1-7281-2138-3","10.1109/iCCECE46942.2019.8941966","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941966","Context awareness;Driver Distraction;Severity prediction;dynamic Bayesian networks (DBN);LSTM networks;time series","Vehicles;Accidents;Monitoring;Task analysis;Safety;Visualization","Bayes methods;learning (artificial intelligence);recurrent neural nets;road safety;road traffic;road vehicles;traffic engineering computing;video coding","context-aware driver distraction severity classification;LSTM network;Advanced Driving Assistance Systems;vehicle drivers;public road transportation systems;deep learning technique;contextual awareness parameters;long short-term memory network;frame-based contextual data;multiview TeleFOT naturalistic driving study;recurrent deep neural network layers;regions of interest;video coding taxonomy;probabilistic Bayesian model","","","","25","","27 Dec 2019","","","IEEE","IEEE Conferences"
"A multi-layer artificial intelligence and sensing based affective conversational embodied agent","S. DiPaola; Ö. N. Yalçin","School of Interactive Art and Tech, Simon Fraser University,Vancouver,Canada; School of Interactive Art and Tech, Simon Fraser University,Vancouver,Canada","2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)","8 Dec 2019","2019","","","91","92","Building natural and conversational virtual humans is a task of formidable complexity. We believe that, especially when building agents that affectively interact with biological humans in real-time, a cognitive science-based, multilayered sensing and artificial intelligence (AI) systems approach is needed. For this demo, we show a working version (through human interaction with it) our modular system of natural, conversation 3D virtual human using AI or sensing layers. These including sensing the human user via facial emotion recognition, voice stress, semantic meaning of the words, eye gaze, heart rate, and galvanic skin response. These inputs are combined with AI sensing and recognition of the environment using deep learning natural language captioning or dense captioning. These are all processed by our AI avatar system allowing for an affective and empathetic conversation using an NLP topic-based dialogue capable of using facial expressions, gestures, breath, eye gaze and voice language-based two-way back and forth conversations with a sensed human. Our lab has been building these systems in stages over the years.","","978-1-7281-3891-6","10.1109/ACIIW.2019.8925291","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8925291","artificial intelligence;conversational agent;embodied agent;deep learning;sensing systems;biosensing;affective computing;embodied character agents","Real-time systems;Emotion recognition;Streaming media;Machine learning;Biosensors","artificial intelligence;avatars;cognition;emotion recognition;face recognition;human computer interaction;interactive systems;learning (artificial intelligence);natural language processing;software agents","eye gaze;sensed human;affective conversational embodied agent;conversational virtual humans;formidable complexity;building agents;biological humans;cognitive science-based;multilayered sensing;working version;human interaction;modular system;conversation 3D virtual human;AI layers;sensing layers;facial emotion recognition;galvanic skin response;deep learning natural language captioning;AI avatar system;affective conversation;empathetic conversation;NLP topic-based dialogue;artificial intelligence system approach","","","","3","","8 Dec 2019","","","IEEE","IEEE Conferences"
"Self-Adaptive Appearance-Based Eye-Tracking with Online Transfer Learning","B. Klein Salvalaio; G. de Oliveira Ramos",SAP Labs Latin America; Universidade do Vale do Rio dos Sinos,"2019 8th Brazilian Conference on Intelligent Systems (BRACIS)","5 Dec 2019","2019","","","383","388","Eye-tracking plays a role in human-computer interactions and has proven useful in a wide variety of domains. We consider appearance-based eye-tracking, where one tracks eye movements based solely on conventional images (rather than on sophisticated additional hardware). Recent advances made in Deep Learning and, in particular, convolutional neural networks have allowed appearance-based eye-tracking to achieve better results than ever. However, current literature still lacks methods that generalize to different combinations of user, environment and device. In this work, we introduce Online Deep Appearance-Based Eye-Tracking (ODABE), which overcomes such a limitation by considering online transfer learning, thus enabling eye-tracking models to self-adapt to different context very rapidly. Our results show that ODABE improves upon previous research when context changes, decreasing the prediction error by 50.95% on average, on tested cases.","2643-6264","978-1-7281-4253-1","10.1109/BRACIS.2019.00074","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924035","Deep Learning;Eye-Tracking;Gaze-Tracking;Transfer Learning","Context modeling;Face;Neural networks;Training;Webcams;Proposals;Machine learning","convolutional neural nets;gaze tracking;human computer interaction;image classification;learning (artificial intelligence)","online transfer learning;online deep appearance-based eye-tracking;self-adaptive appearance-based eye-tracking;human-computer interactions;deep learning;convolutional neural networks;ODABE","","","","19","","5 Dec 2019","","","IEEE","IEEE Conferences"
"Focus on Area Tracking Based on Deep Learning for Multiple Users","H. Chen; B. Hwang; W. Hsu; C. Kao; W. Chen","Department of Computer and Communication Engineering, Ming Chuan University, Taipei, Taiwan; Department of Computer and Communication Engineering, Ming Chuan University, Taipei, Taiwan; Department of Information Management, National Taichung University of Science and Technology, Taichung, Taiwan; Department of Applied Information Technology, Hsing Wu University of Science and Technology, Taipei, Taiwan; Department of Computer and Communication Engineering, Ming Chuan University, Taipei, Taiwan","IEEE Access","23 Dec 2019","2019","7","","179477","179491","Most eye-tracking experiments are limited to single subjects because gaze points are difficult to track when multiple users are involved and environmental factors might cause interference. To overcome this problem, this paper proposes a method for gaze tracking that can be applied for multiple users simultaneously. Four models, including FASEM, FAEM and FAFRCM in the signal-user environment, as well as FAEM and FAMAM in the multiple-user environment, are proposed, and we collected raw data of gazing behaviors to train the models. Through a modified VGG19 architecture and adjusting the Number of Convolutional Layers (NoCL), we obtained and compared the accuracy of various models to determine the most suitable architecture. Since data for multiple-users is not easy to obtain, in this paper, we trained the model first with single users, then extended it to multiple users with transfer learning. Finally, we propose an adaptive method to integrate the benefits of FAEM and FAMAM.","2169-3536","","10.1109/ACCESS.2019.2956953","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8918265","Multiple users;gaze-tracking;position clustering;deep learning","Training;Data models;Cameras;Machine learning;Head;Gaze tracking;Advertising","convolutional neural nets;gaze tracking;learning (artificial intelligence);object tracking","transfer learning;NoCL;convolutional layers;modified VGG19 architecture;raw data collection;FAFRCM;FASEM;FAEM;gaze points;deep learning;eye-tracking experiments;multiple-user environment;signal-user environment;gaze tracking","","","","28","CCBY","2 Dec 2019","","","IEEE","IEEE Journals"
"Deep Spatio-Temporal Modeling for Object-Level Gaze-Based Relevance Assessment","K. Stavridis; A. Psaltis; A. Dimou; G. T. Papadopoulos; P. Daras",Centre for Research and Technology Hellas; Centre for Research and Technology Hellas; Centre for Research and Technology Hellas; Centre for Research and Technology Hellas; Centre for Research and Technology Hellas,"2019 27th European Signal Processing Conference (EUSIPCO)","18 Nov 2019","2019","","","1","5","The current work investigates the problem of objectlevel relevance assessment prediction, taking into account the user's captured gaze signal (behaviour) and following the Deep Learning (DL) paradigm. Human gaze, as a sub-conscious response, is influenced from several factors related to the human mental activity. Several studies have so far proposed methodologies based on the use of gaze statistical modeling and naive classifiers for assessing images or image patches as relevant or not to the user's interests. Nevertheless, the outstanding majority of literature approaches only relied so far on the use of handcrafted features and relative simple classification schemes. On the contrary, the current work focuses on the use of DL schemes that will enable the modeling of complex patterns in the captured gaze signal and the subsequent derivation of corresponding discriminant features. Novel contributions of this study include: a) the introduction of a large-scale annotated gaze dataset, suitable for training DL models, b) a novel method for gaze modeling, capable of handling gaze sensor errors, and c) a DL based method, able to capture gaze patterns for assessing image objects as relevant or non-relevant, with respect to the user's preferences. Extensive experiments demonstrate the efficiency of the proposed method, taking also into consideration key factors related to the human gaze behaviour.","2076-1465","978-9-0827-9703-9","10.23919/EUSIPCO.2019.8902990","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902990","Gaze modeling;DL;relevance assessment","Task analysis;Predictive models;Search problems;Context modeling;Visualization;Monitoring;Europe","gaze tracking;image classification;learning (artificial intelligence);neural nets;statistical analysis","gaze statistical modeling;image patches;large-scale annotated gaze dataset;DL models;gaze modeling;gaze sensor errors;gaze patterns;human gaze behaviour;object-level gaze-based relevance assessment;human mental activity;gaze signal;deep spatio-temporal modeling;object-level relevance assessment prediction","","","","22","","18 Nov 2019","","","IEEE","IEEE Conferences"
"Gaze Tracking by Joint Head and Eye Pose Estimation Under Free Head Movement","S. Cristina; K. P. Camilleri","University of Malta,Department of Systems and Control Engineering,Msida,Malta; University of Malta,Department of Systems and Control Engineering,Msida,Malta","2019 27th European Signal Processing Conference (EUSIPCO)","18 Nov 2019","2019","","","1","5","Recent trends in the field of eye-gaze tracking have been shifting towards the estimation of gaze direction in everyday life settings, hence calling for methods that alleviate the constraints typically associated with existing methods, which limit their applicability in less controlled conditions. In this paper, we propose a method for eye-gaze estimation as a function of both eye and head pose components, without requiring prolonged user-cooperation prior to gaze estimation. Our method exploits the trajectories of salient feature trackers spread randomly over the face region for the estimation of the head rotation angles, which are subsequently used to drive a spherical eye-in-head rotation model that compensates for the changes in eye region appearance under head rotation. We investigate the validity of the proposed method on a publicly available data set.","2076-1465","978-9-0827-9703-9","10.23919/EUSIPCO.2019.8902786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902786","Eye-gaze tracking;pervasive;passive","Faces;Estimation;Shape;Tracking;Kalman filters;Feature extraction","eye;face recognition;feature extraction;gaze tracking;pose estimation","joint head;free head movement;eye-gaze tracking;gaze direction;everyday life settings;controlled conditions;eye-gaze estimation;prolonged user-cooperation;salient feature trackers;head rotation angles;spherical eye-in-head rotation model;eye region appearance","","","","30","","18 Nov 2019","","","IEEE","IEEE Conferences"
"A Framework For Human Behaviour Detection Using Combined Analysis of Facial Expression and Eye Gaze","P. J. Das; A. k. Talukdar; K. K. Sarma","Gauhati University,Dept. of Electronics and Communication Engineering,Guwahati,India,781014; Gauhati University,Dept. of Electronics and Communication Engineering,Guwahati,India,781014; Gauhati University,Dept. of Electronics and Communication Engineering,Guwahati,India,781014","2019 2nd International Conference on Innovations in Electronics, Signal Processing and Communication (IESC)","18 Nov 2019","2019","","","154","160","Facial appearances like a happy, sad, disgust, surprise, angry, fear and Eye Gaze Estimations are the fastest means of communication while conveying any type of information. Expressions not only expose the sensitivity or feelings of any person but can also be used to judge his/her mental states. Therefore, it has become a wide interest area of research due to its applications to the fields like Human Computer Interface (HCI), Man Machine Interface (MMI) etc. In order to get more effective human behavior identification, we have presented a combination of facial expression recognition and eye gaze estimation technique where we have to recognize the real time expressions by using Convolutional Neural Network (CNN) model with transfer learning method. After that, we determine the eye gaze by using Viola Jones, Circular Hough Transform (CHT) and a geometric method. With the combination of both processes, we able to predict the human behavior like drivers concentration levels. In this way experiments are carried out on MUG database for our own trained CNN model based on VGG16 (Visual Geometry Group) pre trained model and gives better performance with accuracy of 93% for training and 94% for overall process.","","978-1-7281-0744-8","10.1109/IESPC.2019.8902367","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902367","Face detection;face recognition;facial expression;eye gaze estimation;transfer learning CNN;VGG16","","","","","","","9","","18 Nov 2019","","","IEEE","IEEE Conferences"
"Training a Camera to Perform Long-Distance Eye Tracking by Another Eye-Tracker","W. Li; Q. Dong; H. Jia; S. Zhao; Y. Wang; L. Xie; Q. Pan; F. Duan; T. Liu","Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, China; Cortical Architecture Imaging and Discovery Lab, University of Georgia, Athens, GA, USA; Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, China; Department of Instrument Science and Technology, Zhejiang University, Hangzhou, China; Xi’an iSoftStone Network Technology Company Ltd., Xi’an, China; Department of Automation and Intelligence, College of Artificial Intelligence, Nankai University, Tianjin, China; Cortical Architecture Imaging and Discovery Lab, University of Georgia, Athens, GA, USA","IEEE Access","31 Oct 2019","2019","7","","155313","155324","Appearance-based gaze estimation techniques have been greatly advanced in these years. However, using a single camera for appearance-based gaze estimation has been limited to short distance in previous studies. In addition, labeling of training samples has been a time-consuming and unfriendly step in previous appearance-based gaze estimation studies. To bridge these significant gaps, this paper presents a new long-distance gaze estimation paradigm: train a camera to perform eye tracking by another eye tracker, named Learning-based Single Camera eye tracker (LSC eye-tracker). In the training stage, the LSC eye-tracker simultaneously acquired gaze data by a commercial trainer eye tracker and face appearance images by a long-distance trainee camera, based on which deep convolutional neural network (CNN) models are utilized to learn the mapping from appearance images to gazes. In the application stage, the LSC eye-tracker works alone to predict gazes based on the acquired appearance images by the single camera and the trained CNN models. Our experimental results show that the LSC eye-tracker enables both population-based eye tracking and personalized eye tracking with promising accuracy and performance.","2169-3536","","10.1109/ACCESS.2019.2949150","National Natural Science Foundation of China(grant numbers:61673224); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880636","Eye tracking;gaze estimation;human-computer interaction;machine learning","Cameras;Gaze tracking;Estimation;Training;Face;Data models;Labeling","cameras;convolutional neural nets;data acquisition;estimation theory;eye;face recognition;gaze tracking;image resolution;learning (artificial intelligence);object tracking","commercial trainer eye tracker;face appearance images;long-distance trainee camera;LSC eye-tracker works;population-based eye tracking;deep CNN;personalized eye tracking;camera training;long-distance eye tracking;learning-based single camera eye tracker;LSC eye-tracker;long-distance gaze estimation;gaze data acquisition;deep convolutional neural network","","2","","40","CCBY","23 Oct 2019","","","IEEE","IEEE Journals"
"Gaze Estimation Based on Neural Network","M. Luo; X. Liu; W. Huang","Nanchang University,School of Information Engineering,Nanchang,China; Nanchang University,School of Information Engineering,Nanchang,China; Nanchang University,School of Information Engineering,Nanchang,China","2019 IEEE 2nd International Conference on Electronic Information and Communication Technology (ICEICT)","23 Oct 2019","2019","","","590","594","In this study, we selected some state-of-the-art or classic neural network models for recently popular research problem gaze estimation. Experimental analysis based on eyegaze data set compares different models and suggests that neural networks can achieve excellent performance in gaze estimation problem while original Capsule Network provides the best performance and 18-layers ResNet is second-best in all models.","","978-1-5386-9298-1","10.1109/ICEICT.2019.8846281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846281","gaze estimation;neural network;Capsule;ResNet","Estimation;Convolution;Light sources;Training;Feature extraction;Neural networks;Analytical models","","","","","","11","","23 Oct 2019","","","IEEE","IEEE Conferences"
"Understanding Face Age Estimation: humans and machine","J. Alarifi; J. Fry; D. Dancey; M. H. Yap","Faculty of Science and Engineering, Manchester Metropolitan University; Faculty of Science and Engineering, Manchester Metropolitan University; Faculty of Science and Engineering, Manchester Metropolitan University; Faculty of Science and Engineering, Manchester Metropolitan University","2019 International Conference on Computer, Information and Telecommunication Systems (CITS)","11 Oct 2019","2019","","","1","5","Face age estimation is an important part of many disciplines, including dermatology, cosmetology, and computer vision. Traditional age estimation studies focus on certain parts of the face to analyse its surface topology. With the advances of deep learning, many Convolutional Neural Networks (CNNs) now outperform traditional methods in the age estimation task. However, it is still not clear what type of features these networks learn when estimating age. This study aims to investigate which facial features are important, for humans and for CNNs, on the age estimation of women in five age groups. We then compare the heat-maps from the human eye gaze with those of the CNN. We consider two main research questions: (1) Which facial regions do humans look at when performing age estimation? (2) Do humans and machine focus on the same facial regions when estimating the age? We answered these questions by conducting two experiments. In the first experiment, we used an eye-tracking software to detect where on the face the gaze of human participants focused the most when they were asked to assign the person in the image to an age class. In the second experiment, we used transfer learning on the network pre-trained on ImageNet, then fine-tuned the network on a benchmark face age dataset to classify the same images shown to the participants. The heat-maps of the VGG16 network were then visualised using Gradient-based Class Activation Map (Grad-CAM). The results showed how our model was almost as accurate as humans in predicting the age of a person from a single image of their face (CNN: 60%; humans: 61%). The results also showed that people mainly look at the eyes and nose when predicting a person's age, while the features learned by the CNN included the eyes, the mouth, and the skin surface.","","978-1-7281-1374-6","10.1109/CITS.2019.8862107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8862107","","Face;Skin;Estimation;Aging;Task analysis;Computer vision;Heating systems","computer vision;face recognition;feature extraction;image classification;learning (artificial intelligence);neural nets;skin","facial regions;human participants;age class;benchmark face age dataset;heat-maps;person;face age estimation;traditional age estimation studies;Convolutional Neural Networks now outperform traditional methods;age estimation task;estimating age;age groups;human eye gaze;performing age estimation","","1","","35","","11 Oct 2019","","","IEEE","IEEE Conferences"
"Unsupervised Learning of Eye Gaze Representation from the Web","N. Dubey; S. Ghosh; A. Dhall","Learning Affect and Semantic Image analysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India; Learning Affect and Semantic Image analysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India; Learning Affect and Semantic Image analysIs (LASII) Group, Indian Institute of Technology Ropar, Ropar, India","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","7","Automatic eye gaze estimation has interested researchers for a while now. In this paper, we propose an unsupervised learning based method for estimating the eye gaze region. To train the proposed network ""Ize-Net"" in self-supervised manner, we collect a large `in the wild' dataset containing 1,54,251 images from the web. For the images in the database, we divide the gaze into three regions based on an automatic technique based on pupil-centers localization and then use a feature-based technique to determine the gaze region. The performance is evaluated on the Tablet Gaze and CAVE datasets by fine-tuning results of Ize-Net for the task of eye gaze estimation. The feature representation learned is also used to train traditional machine learning algorithms for eye gaze estimation. The results demonstrate that the proposed method learns a rich data representation, which can be efficiently finetuned for any eye gaze estimation dataset.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8851961","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8851961","","Estimation;Feature extraction;Task analysis;Head;Iris;Neural networks;Lighting","gaze tracking;image representation;Internet;performance evaluation;unsupervised learning","feature-based technique;feature representation;unsupervised learning based method;automatic eye gaze estimation dataset;eye gaze representation;Web;Ize-Net network;pupil-center localization;CAVE dataset;Tablet Gaze datasets;data representatio","","1","","51","","30 Sep 2019","","","IEEE","IEEE Conferences"
"Improving the realism of synthetic images through a combination of adversarial and perceptual losses","C. Atapattu; B. Rekabdar","Department of Computer Science, Southern Illinois University, Carbondale, USA; Department of Computer Science, Southern Illinois University, Carbondale, USA","2019 International Joint Conference on Neural Networks (IJCNN)","30 Sep 2019","2019","","","1","7","In recent years, deep learning methods are becoming more widely used; however, large quantities of labeled training data are required for most models. Labeling large datasets is tedious, expensive, and time consuming. Generating large labeled synthetic datasets, on the other hand, is easier and less expensive since annotations are available. But there is usually a large gap between the distribution of the synthetic and real data. In this paper, we propose a novel method based on Generative Adversarial Networks (GANs) to improve the realism of the synthetic images while preserving the annotation information. In our work the inputs of the GANs are synthetic images instead of random vectors. Furthermore, we describe how a perceptual loss can be utilized while introducing the basic features and techniques from adversarial networks for obtaining better results. We evaluate our approach for appearance-based gaze direction classification on the MPIIGaze dataset. The results show that our generated refined images are more realistic and better preserve the annotation information than the refined images generated by the state-of-the-art methods.","2161-4407","978-1-7281-1985-4","10.1109/IJCNN.2019.8852449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8852449","deep learning;adversarial network;unsupervised learning;synthetic images;generative adversarial network","Annotations;Gallium nitride;Generative adversarial networks;Mathematical model;Training;Neural networks;Feature extraction","computer vision;feature extraction;gaze tracking;image classification;learning (artificial intelligence);neural nets","synthetic images;adversarial losses;perceptual losses;deep learning methods;labeled synthetic datasets;GANs;MPIIGaze dataset;generative adversarial networks;appearance-based gaze direction classification","","2","","27","","30 Sep 2019","","","IEEE","IEEE Conferences"
"Eye Gaze Region Estimation via Multi-scale Sparse Dictionary Learning","G. Yuan; Y. Wang; T. Zhao; X. Ding; Z. Mi; X. Fu","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","2019 14th IEEE Conference on Industrial Electronics and Applications (ICIEA)","16 Sep 2019","2019","","","1459","1463","Traditional model-based gaze estimation methods relying on pupil or iris detection is vulnerable under large head movement. To solve this problem, a novel head pose-free appearance-based eye gaze prediction method without gaze calibration is proposed, combining head pose information and multi-scale eye image sparse features to determine the gaze region. Multi-scale dictionaries are learned by sparse coding to represent eye appearance more effectively, in which whole eye images are employed as global scale while their corresponding distinct patch-based images as local scale. Supported by multiscale dictionaries, multi-scale eye image sparse features are the sparse coefficients encoding eye image by linear combinations of bases selected from the learned dictionaries. In order to handle gaze variation under large head movement, a multiclass classifier concatenates the known head pose information and multi-scale eye image sparse features to estimate the gaze region. The experimental results show that the proposed approach outperforms baseline methods on a public dataset.","2158-2297","978-1-5386-9490-9","10.1109/ICIEA.2019.8833727","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8833727","Gaze estimation;Dictionary learning;Appearance;Gaze region","Dictionaries;Head;Estimation;Feature extraction;Image coding;Image reconstruction;Machine learning","estimation theory;image classification;image coding;learning (artificial intelligence);prediction theory","multiscale sparse dictionary learning;multiscale eye image sparse features;distinct patch-based imaging;head pose-free appearance-based eye gaze prediction method;iris detection;traditional model-based eye gaze region estimation methods;gaze calibration;sparse coding;multiclass classifier concatenation","","","","9","","16 Sep 2019","","","IEEE","IEEE Conferences"
"Accurate and robust eye center localization via fully convolutional networks","Y. Xia; H. Yu; F. Wang","School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, UK; School of Creative Technologies, University of Portsmouth, Portsmouth, PO1 2DJ, UK; State Key Laboratory of Management and Control for Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing 100190, China","IEEE/CAA Journal of Automatica Sinica","3 Sep 2019","2019","6","5","1127","1138","Eye center localization is one of the most crucial and basic requirements for some human-computer interaction applications such as eye gaze estimation and eye tracking. There is a large body of works on this topic in recent years, but the accuracy still needs to be improved due to challenges in appearance such as the high variability of shapes, lighting conditions, viewing angles and possible occlusions. To address these problems and limitations, we propose a novel approach in this paper for the eye center localization with a fully convolutional network (FCN), which is an end-to-end and pixels-to-pixels network and can locate the eye center accurately. The key idea is to apply the FCN from the object semantic segmentation task to the eye center localization task since the problem of eye center localization can be regarded as a special semantic segmentation problem. We adapt contemporary FCN into a shallow structure with a large kernel convolutional block and transfer their performance from semantic segmentation to the eye center localization task by fine-tuning. Extensive experiments show that the proposed method outperforms the state-of-the-art methods in both accuracy and reliability of eye center localization. The proposed method has achieved a large performance improvement on the most challenging database and it thus provides a promising solution to some challenging applications.","2329-9274","","10.1109/JAS.2019.1911684","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823575","","Semantics;Task analysis;Hidden Markov models;Kernel;Estimation;Lighting;Computer vision","convolutional neural nets;eye;gaze tracking;human computer interaction;image resolution;image segmentation","eye tracking;fully convolutional network;eye center localization task;eye gaze estimation;robust eye center localization;human-computer interaction applications;end-to-end network;pixels-to-pixels network;semantic segmentation problem;kernel convolutional block","","10","","","","3 Sep 2019","","","IEEE","IEEE Journals"
"Deep Learning for Consumer Devices and Services 2—AI Gets Embedded at the Edge","P. Corcoran; J. Lemley; C. Costache; V. Varkarakis",National University of Ireland Galway (NUIG); National University of Ireland Galway (NUIG); National University of Ireland Galway (NUIG); National University of Ireland Galway (NUIG),"IEEE Consumer Electronics Magazine","2 Sep 2019","2019","8","5","10","19","The recent explosive growth of deep learning is enabling a new generation of intelligent consumer devices. Specialized deep learning inference now provides data analysis capabilities that once required an active cloud connection, while reducing latency and enhancing data privacy. This paper addresses current progress in Edge artificial intelligence (AI) technology in several consumer contexts including privacy, biometrics, eye gaze, driver monitoring systems, and more. New developments and challenges in edge hardware and emerging opportunities are identified. Our previous article, “Deep learning for consumer devices and services,” introduced many of the basics of deep learning and AI. In this paper, we explore the current paradigm shift of AI from the data center into CE devices-“Edge-AI”.","2162-2256","","10.1109/MCE.2019.2923042","Science Foundation Ireland and FotoNation, Ltd.(grant numbers:13/SPP/I2868); Next Generation Imaging for Smartphone and Embedded Platforms; Irish Research Council(grant numbers:EBPPG/2016/280); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8822518","","Cameras;Deep learning;Authentication;Iris recognition;Image edge detection;Intelligent systems","cloud computing;data analysis;data privacy;human factors;learning (artificial intelligence)","deep learning inference;consumer contexts;edge artificial intelligence technology;edge-AI;CE devices;edge hardware;data privacy;active cloud connection;data analysis capabilities;intelligent consumer devices","","1","","56","IEEE","2 Sep 2019","","","IEEE","IEEE Magazines"
"Low Cost Eye Gaze Tracker Using Web Camera","M. S. Mounica; M. Manvita; C. Jyotsna; J. Amudha","Dept. of Computer Science & Engineering, Amrita Vishwa Vidyapeetham, Bengaluru, India; Dept. of Computer Science & Engineering, Amrita Vishwa Vidyapeetham, Bengaluru, India; Dept. of Computer Science & Engineering, Amrita Vishwa Vidyapeetham, Bengaluru, India; Dept. of Computer Science & Engineering, Amrita Vishwa Vidyapeetham, Bengaluru, India","2019 3rd International Conference on Computing Methodologies and Communication (ICCMC)","29 Aug 2019","2019","","","79","85","The traditional gaze tracking systems are of invasive, expensive or not of a standard hardware. To address this problem, research has been focused onto systems with simple hardware with gaze tracking systems. We propose a system which uses web camera and the free open source Computer Vision Library Open CV. Gaze estimation plays a major role in predicting human attention and understanding human activities. It is also used in market analysis, gaze driven interactive displays, medical research, usability research, packaging research, gaming research, psychology research, and other human-machine interfaces. This paper describes how to develop a low cost eye gaze system using a simple web camera. The system captures real time video of the person to detect the eyes in the initial frames and extract the features of eyes. Once the features are extracted, the eyes are tracked in the subsequent frames and the gaze direction is estimated using the computational intelligence techniques.","","978-1-5386-7808-4","10.1109/ICCMC.2019.8819645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8819645","eye tracking;gaze estimation;human computer interaction;web camera;calibration","Gaze tracking;Calibration;Neural networks;Cameras;Streaming media;Matlab;Genetic algorithms","cameras;computer vision;gaze tracking;human computer interaction;user interfaces;video signal processing","low cost eye gaze tracker;traditional gaze tracking systems;free open source Computer Vision Library Open CV;gaze estimation;human attention;gaze driven interactive displays;medical research;usability research;packaging research;gaming research;psychology research;human-machine interfaces;low cost eye gaze system;simple web camera;gaze direction","","1","","14","","29 Aug 2019","","","IEEE","IEEE Conferences"
"Attention Monitoring and Hazard Assessment with Bio-Sensing and Vision: Empirical Analysis Utilizing CNNs on the KITTI Dataset","Siddharth; M. M. Trivedi","The Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, La Jolla, CA, 92093, USA; The Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, La Jolla, CA, 92093, USA","2019 IEEE Intelligent Vehicles Symposium (IV)","29 Aug 2019","2019","","","1673","1678","Assessing the driver's attention and detecting various hazardous and non-hazardous events during a drive are critical for driver's safety. Attention monitoring in driving scenarios has mostly been carried out using vision (camera-based) modality by tracking the driver's gaze and facial expressions. It is only recently that bio-sensing modalities such as Electroencephalogram (EEG) are being explored. But, there is another open problem which has not been explored sufficiently yet in this paradigm. This is the detection of specific events, hazardous and non-hazardous, during driving that affects the driver's mental and physiological states. The other challenge in evaluating multi-modal sensory applications is the absence of very large scale EEG data because of the various limitations of using EEG in the real world. In this paper, we use both of the above sensor modalities and compare them against the two tasks of assessing the driver's attention and detecting hazardous vs. non-hazardous driving events. We collect user data on twelve subjects and show how in the absence of very large-scale datasets, we can still use pre-trained deep learning convolution networks to extract meaningful features from both of the above modalities. We used the publicly available KITTI dataset for evaluating our platform and to compare it with previous studies. Finally, we show that the results presented in this paper surpass the previous benchmark set up in the above driver awareness-related applications.","2642-7214","978-1-7281-0560-4","10.1109/IVS.2019.8813874","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8813874","","","convolutional neural nets;electroencephalography;feature extraction;learning (artificial intelligence);medical signal processing","feature extraction;deep learning convolution networks;physiological states;mental states;electroencephalogram;CNN;attention monitoring;KITTI dataset;EEG data;facial expressions;vision modality;hazard assessment","","","","30","","29 Aug 2019","","","IEEE","IEEE Conferences"
"Deep Learning For Inter-Observer Congruency Prediction","A. BRUCKERT; Y. H. LAM; M. CHRISTIE; O. L. MEUR","Univ Rennes, IRISA, CNRS, France; Nokia Technologies, Tampere, Finland; Univ Rennes, IRISA, CNRS, France; Univ Rennes, IRISA, CNRS, France","2019 IEEE International Conference on Image Processing (ICIP)","26 Aug 2019","2019","","","3766","3770","According to the literature regarding visual saliency, observers may exhibit considerable variations in their gaze behaviors. These variations are influenced by aspects such as cultural background, age or prior experiences, but also by features in the observed images. The dispersion between the gaze of different observers looking at the same image is commonly referred as inter-observer congruency (IOC). Predicting this congruence can be of great interest when it comes to study the visual perception of an image. In this paper, we introduce a new method based on deep learning techniques to predict the IOC of an image. This is achieved by first extracting features from an image through a deep convolutional network. We then show that using such features to train a model with a shallow network regression technique significantly improves the precision of the prediction over existing approaches.","2381-8549","978-1-5386-6249-6","10.1109/ICIP.2019.8803596","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8803596","visual dispersion;gaze patterns;prediction;deep features","Observers;Feature extraction;Databases;Training;Visualization;Computational modeling;Computer architecture","convolutional neural nets;feature extraction;learning (artificial intelligence);regression analysis;visual perception","shallow network regression technique;inter-observer congruency prediction;visual saliency;gaze behaviors;cultural background;IOC;visual perception;deep learning techniques;deep convolutional network;feature extraction","","1","","24","","26 Aug 2019","","","IEEE","IEEE Conferences"
"Human Action Recognition in First Person Videos using Verb-Object Pairs","Z. Gökce; S. Pehlivan","TED Üniversitesi, Bilgisayar Mühendisliği Bölümü, Ankara, Türkiye; TED Üniversitesi, Bilgisayar Mühendisliği Bölümü, Ankara, Türkiye","2019 27th Signal Processing and Communications Applications Conference (SIU)","22 Aug 2019","2019","","","1","4","Human action recognition problem is important for distinguishing the rich variety of human activities in first-person videos. While there has been an improvement in egocentric action recognition, the space of action categories is large and it looks impractical to label training data for all categories. In this work, we decompose action models into verb and noun model pairs and propose a method to combine them with a simple fusion strategy. Particularly, we use 3 Dimensional Convolutional Neural Network model, C3D, for verb stream to model video-based features, and we use object detection model, YOLO, for noun stream to model objects interacting with human. We present experiments on the recently introduced large-scale EGTEA Gaze+ dataset with 106 action classes, and show that our model is comparable to the state-of-the-art action recognition models.","2165-0608","978-1-7281-1904-5","10.1109/SIU.2019.8806562","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8806562","action recognition;first-person video;deep learning","Videos;Activity recognition;Nanoelectromechanical systems;Training data;Convolutional neural networks;Feature extraction;Object detection","convolutional neural nets;feature extraction;image classification;image motion analysis;learning (artificial intelligence);object detection;pose estimation;video signal processing","first person videos;verb-object pairs;human action recognition problem;human activities;first-person videos;egocentric action recognition;action categories;training data;model pairs;simple fusion strategy;verb stream;detection model;noun stream;model objects;action recognition models;action classes;human action recognition;3-dimensional convolutional neural network model;object detection model;YOLO;large-scale EGTEA Gaze+ dataset","","","","","","22 Aug 2019","","","IEEE","IEEE Conferences"
"Mask-Guided Style Transfer Network for Purifying Real Images","T. Zhao; Y. Yan; J. Peng; H. Wang; X. Fu",Dalian Maritime University; Dalian Maritime University; Dalian Maritime University; Dalian Maritime University; Dalian Maritime University,"2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","15 Aug 2019","2019","","","429","434","Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. However, due to the different distribution of synthetic images compared with real images, the desired performance cannot be achieved. To solve this problem, the previous method learned a model to improve the realism of the synthetic images. Different from the previous methods, this paper try to purify real image by extracting discriminative and robust features to convert outdoor real images to indoor synthetic images. In this paper, we first introduce the segmentation masks to construct RGB-mask pairs as inputs, then we design a mask-guided style transfer network to learn style features separately from the attention and bkgd(background) regions and learn content features from full and attention region. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using mixed studies (qualitative and quantitative) methods to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on various public datasets, including LPW, COCO and MPIIGaze. Experimental results show that the proposed method is effective and achieves the state-of-the-art results.","","978-1-5386-9214-1","10.1109/ICMEW.2019.00080","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8795010","Gaze estimation , Style Transfer , Mask guided Style Transfer Network , Learning-by-synthesis","","feature extraction;image colour analysis;image segmentation;learning (artificial intelligence);masks","image segmentation masks;real image purification;cost reduction;material resources;synthetic image distribution;robust discriminative feature extraction;MPIIGaze public dataset;COCO public dataset;LPW public dataset;region-level task-guided loss;RGB-mask pairs;indoor synthetic images;learning-by-synthesis;mask-guided style transfer network","","","","29","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Gaze-based, Context-aware Robotic System for Assisted Reaching and Grasping","A. Shafti; P. Orlov; A. A. Faisal","Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, UK; Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, UK; Brain and Behaviour Lab, Imperial College London, London, SW7 2AZ, UK","2019 International Conference on Robotics and Automation (ICRA)","12 Aug 2019","2019","","","863","869","Assistive robotic systems endeavour to support those with movement disabilities, enabling them to move again and regain functionality. Main issue with these systems is the complexity of their low-level control, and how to translate this to simpler, higher level commands that are easy and intuitive for a human user to interact with. We have created a multi-modal system, consisting of different sensing, decision making and actuating modalities, leading to intuitive, human-in-the-loop assistive robotics. The system takes its cue from the user's gaze, to decode their intentions and implement low-level motion actions to achieve high-level tasks. This results in the user simply having to look at the objects of interest, for the robotic system to assist them in reaching for those objects, grasping them, and using them to interact with other objects. We present our method for 3D gaze estimation, and grammars-based implementation of sequences of action with the robotic system. The 3D gaze estimation is evaluated with 8 subjects, showing an overall accuracy of 4.68\pm 0.14cm. The full system is tested with 5 subjects, showing successful implementation of 100% of reach to gaze point actions and full implementation of pick and place tasks in 96%, and pick and pour tasks in 76% of cases. Finally we present a discussion on our results and what future work is needed to improve the system.","2577-087X","978-1-5386-6027-0","10.1109/ICRA.2019.8793804","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8793804","","Three-dimensional displays;Cameras;Robot kinematics;Robot vision systems;Grasping","gaze tracking;grammars;handicapped aids;human-robot interaction;medical robotics;mobile robots;patient rehabilitation","human-in-the-loop assistive robotics;low-level motion actions;3D gaze estimation;grammars-based implementation;gaze-based;context-aware robotic system;assistive robotic systems;movement disabilities;human user;multimodal system;assisted reaching;assisted grasping","","12","","32","","12 Aug 2019","","","IEEE","IEEE Conferences"
"Learning A 3D Gaze Estimator with Improved Itracker Combined with Bidirectional LSTM","X. Zhou; J. Lin; J. Jiang; S. Chen",Zhejiang University of Technology; Zhejiang University of Technology; Zhejiang University of Technology; Zhejiang University of Technology,"2019 IEEE International Conference on Multimedia and Expo (ICME)","5 Aug 2019","2019","","","850","855","Free-head 3D gaze estimation which outputs gaze vector in 3D space has wide application in human-computer interaction. In this paper, we propose a novel 3D gaze estimator by improving the Itracker and employing a many-to-one bidirectional LSTM (bi-LSTM). First, we improve the conventional Itracker by removing the face-grid and reducing one network branch via concatenating the two-eye region images to predict the subject's gaze of a single frame. Then, we employ the bi-LSTM to fit the temporal information between frames to estimate gaze vector for video sequence. Experimental results show that our improved Itracker obtains 11.6% significant improvement over the state-of-the-art methods on MPIIGaze dataset (single image frame) and has robust estimation accuracy for different image resolutions. Moreover, experimental results on EyeDiap dataset (video sequence) further bring 3% accuracy improvement by employing the bi-LSTM.","1945-788X","978-1-5386-9552-4","10.1109/ICME.2019.00151","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784770","Gaze estimation, Itracker, RNN, LSTM","Estimation;Face;Logic gates;Three-dimensional displays;Cameras;Computer architecture","computer vision;estimation theory;gaze tracking;image resolution;image sequences;learning (artificial intelligence);motion estimation;recurrent neural nets;stereo image processing;vectors;video signal processing","bidirectional LSTM;free-head 3D gaze estimation;human-computer interaction;two-eye region images;video sequence;gaze vector estimation;3D gaze estimator learning;Itracker;image resolutions","","2","","16","","5 Aug 2019","","","IEEE","IEEE Conferences"
"A Smart Eye Tracking System for Virtual Reality","B. Li; Y. Zhang; X. Zheng; X. Huang; S. Zhang; J. He","Northwest University, School of Information Science and Technology, Xi'an, Shaanxi, China; Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China; Sichuan University, College of Electrical Engineering and Information Technology, Chengdu, Sichuan, China; Northwestern Polytechnical University, School of Computer Science, Xi'an, Shaanxi, China; Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China; Xi'an Jiaotong University, School of Electronic and Information Engineering, Xi'an, Shaanxi, 710049, China","2019 IEEE MTT-S International Microwave Biomedical Conference (IMBioC)","29 Jul 2019","2019","1","","1","3","Virtual reality (VR) technology provides specific three-dimensional (3D) scenes for users, which could be benefit for the applications in the field of medical diagnosis, psychological analysis, cognitive researches and entertainments. The current VR device provides the virtual 3D images, but cannot synchronously detect the user's eye movements which could be significant for deriving the users' gaze points and interest regions in varied applications. In this paper, we proposed a smart eye tracking system for VR device. Firstly, a smart eye movement detection kit is mounted inside the VR device to capture the human eye movement images with the rate of 30 Hz. Based on the proposed eye detection kit, a gaze detection algorithm is then established for VR devices. Next, a gaze estimation model is proposed for human gaze estimation. The proposed smart eye tracking system can detect the user's eye movements in real time under VR stimulation. Moreover, it also provide a new human-VR interaction mode.","","978-1-5386-7395-9","10.1109/IMBIOC.2019.8777841","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8777841","","Cameras;Gaze tracking;Calibration;Virtual reality;Mathematical model;Detection algorithms;Estimation","eye;gaze tracking;psychology;virtual reality","smart eye tracking system;virtual reality;three-dimensional scenes;virtual 3D images;smart eye movement detection kit;human eye movement images;eye detection kit;gaze detection algorithm;VR stimulation;human-VR interaction mode;VR device;frequency 30.0 Hz","","1","","11","","29 Jul 2019","","","IEEE","IEEE Conferences"
"An Eye-Tracker-Based 3D Point-of-Gaze Estimation Method Using Head Movement","W. Pichitwong; K. Chamnongthai","Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand; Department of Electronics and Telecommunication Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand","IEEE Access","5 Aug 2019","2019","7","","99086","99098","Eye trackers are currently used to sense the positions of both the centers of the pupils and the point-of-gaze (POG) position on a screen, in keeping with the original objective for which they were designed; however, it remains difficult to measure the positions of three-dimensional (3D) POGs. This paper proposes a method for 3D gaze estimation by using head movement, pupil position data, and POGs on a screen. The method assumes that a person, usually unintentionally, moves his or her head a short distance such that multiple straight lines can be drawn from the center point between the two pupils to the POG. When the person is continuously focusing on a given 3D POG while moving, these lines represent the lines of sight that intersect at a 3D POG. That 3D POG can, therefore, be found from the intersection of several lines of sight formed by head movements. To evaluate the performance of the proposed method, experimental equipment was constructed, and experiments with five male and five female participants were performed in which the participants looked at nine test points in a 3D space for approximately 20 s each. The experimental results reveal that the proposed method can measure 3D POGs with average distance errors of 13.36 cm, 7.58 cm, 5.72 cm, 3.97 cm, and 3.52 cm for head movement distances of 1 cm, 2 cm, 3 cm, 4 cm, and 5 cm, respectively.","2169-3536","","10.1109/ACCESS.2019.2929195","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8764337","3D gaze estimation;gaze tracking;eye tracker","Three-dimensional displays;Cornea;Estimation;Two dimensional displays;Human computer interaction;Tracking;Navigation","biomechanics;biomedical optical imaging;brain;eye;gaze tracking;human computer interaction;optical tracking;vision","point-of-gaze position;three-dimensional POGs;3D gaze estimation;pupil position data;multiple straight lines;center point;head movement distances;eye-tracker-based 3D point-of-gaze estimation method;3D POG","","1","","30","CCBY","16 Jul 2019","","","IEEE","IEEE Journals"
"Saliency-Driven System With Deep Learning for Cell Image Classification","D. S. Ferreira; G. L. B. Ramalho; F. N. S. Medeiros; A. G. C. Bianchi; C. M. Carneiro; D. M. Ushizima","Berkeley Institute of Data Science, University of California, Berkeley, CA, USA; Ciência e Tecnologia do Ceará, Instituto Federal de Educação, Maracanáu, CE, Brazil; Eng. de Teleinformatica, Univ. Fed. do Ceara, Fortaleza, Brazil; Univ. Fed. de Ouro Preto, Ouro Preto, Brazil; Universidade Federal de Ouro Preto, Ouro Preto, MG, Brazil; Berkeley Institute of Data Science, University of California, Berkeley, CA, USA","2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)","11 Jul 2019","2019","","","1284","1287","This paper describes our automatic cell image classification algorithm that explores expert's eye tracking data combined to convolutional neural networks. Our framework selects regions of interest that attract cytologists attention, then it focuses computation on cell classification of these specific sub-images. Our contribution is to fuse deep learning to saliency maps from eye-tracking into an approach that bypasses segmentation to detect abnormal cells from Pap smear microscopy under real noisy conditions, artifacts and occlusion. Preliminary results show high classification accuracy of ~90% during tasks of locating and identifying critical cells within three levels: normal, low-risk disease and high-risk disease. We validate our results on 111 images containing 3,183 cells and obtained an average runtime of 4.5 seconds per image.","1945-8452","978-1-5386-3641-1","10.1109/ISBI.2019.8759398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759398","cervical cell images;visual attention;convnets;saliency prediction;eye-tracking;cell classification","Computer architecture;Diseases;Microprocessors;Visualization;Training;Prediction algorithms;Predictive models","biomedical optical imaging;cellular biophysics;convolutional neural nets;diseases;gaze tracking;image classification;image segmentation;learning (artificial intelligence);medical image processing","fuse deep learning;bypasses segmentation;high-risk disease;high classification accuracy;Pap smear microscopy;abnormal cells;eye-tracking;saliency maps;specific sub-images;convolutional neural networks;automatic cell image classification algorithm;saliency-driven system","","","","10","","11 Jul 2019","","","IEEE","IEEE Conferences"
"LPM: Learnable Pooling Module for Efficient Full-Face Gaze Estimation","R. Ogusu; T. Yamanaka","Department of Information and Communication Sciences, Sophia University, Tokyo, Japan; Department of Information and Communication Sciences, Sophia University, Tokyo, Japan","2019 14th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2019)","11 Jul 2019","2019","","","1","5","Gaze tracking is an important technology in many domains. Techniques such as Convolutional Neural Networks (CNNs) have allowed the invention of the gaze tracking method that relies only on commodity hardware such as the camera on a personal computer. It has been shown that the full-face region for gaze estimation can provide a better performance than the one obtained from eye image alone. However, a problem with using the full-face image is the heavy computation due to the larger image size. This study tackles this problem through compression of the input full-face image by removing redundant information using a novel learnable pooling module. The module can be trained end-to-end by backpropagation to learn the size of the grid in the pooling filter. The learnable pooling module keeps the resolution of valuable regions high and vice versa. This proposed method preserved the gaze estimation accuracy at a certain level when the image was reduced to a smaller size.","","978-1-7281-0089-0","10.1109/FG.2019.8756523","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756523","","","backpropagation;convolutional neural nets;data compression;face recognition;gaze tracking;image coding;image resolution","full-face gaze estimation;gaze tracking method;commodity hardware;full-face region;eye image;pooling filter;gaze estimation accuracy;image resolution;image compression;backpropagation;full-face image;LPM;learnable pooling module;image size;convolutional neural networks","","","","20","","11 Jul 2019","","","IEEE","IEEE Conferences"
"Gaze Estimation Using Residual Neural Network","E. T. Wong; S. Yean; Q. Hu; B. S. Lee; J. Liu; R. Deepu","Nanyang Technological University, School of Computer Science and Engineering), Singapore; Nanyang Technological University, School of Computer Science and Engineering), Singapore; Nanyang Technological University, School of Computer Science and Engineering), Singapore; Nanyang Technological University, School of Computer Science and Engineering), Singapore; Nanyang Technological University, School of Computer Science and Engineering), Singapore; Nanyang Technological University, School of Computer Science and Engineering), Singapore","2019 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)","6 Jun 2019","2019","","","411","414","Eye gaze tracking has become an prominent research topic in human-computer interaction and computer vision. It is due to its application in numerous fields, such as the market research, medical, neuroscience and psychology. Eye gaze tracking is implemented by estimating gaze (gaze estimation) for each individual frame in offline or real-time video captured. Therefore, in order to produce the secure the accurate tracking, especially in the emerging use in medical and community, innovation on the gaze estimation posts a challenge in research field. In this paper, we explored the use of the deep learning model, Residual Neural Network (ResNet-18), to predict the eye gaze on mobile device. The model is trained using the large-scale eye tracking public dataset called GazeCapture. We aim to innovate by incorporating methods/techniques of removing the blinking data, applying image histogram normalisation, head pose, and face grid features. As a result, we achieved 3.05cm average error, which is better performance than iTracker (4.11cm average error), the recent gaze tracking deep-learning model using AlexNet architecture. Upon observation, adaptive normalisation of the images was found to produce better results compared to histogram normalisation. Additionally, we found that head pose information was useful contribution to the proposed deep-learning network, while face grid information does not help to reduce test error.","","978-1-5386-9151-9","10.1109/PERCOMW.2019.8730846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8730846","eye track;mobile;ResNet;deep learning","Face;Estimation;Gaze tracking;Histograms;Training;Ear","computer vision;gaze tracking;image capture;learning (artificial intelligence);neural nets;pose estimation","Residual Neural Network;eye gaze tracking;human-computer interaction;computer vision;neuroscience;accurate tracking;deep learning model;deep-learning model;deep-learning network;gaze estimation;image histogram normalisation;head pose;face grid features","","2","","16","","6 Jun 2019","","","IEEE","IEEE Conferences"
"A Compact Wearable Eye Movement Measurement System for Support of Safe Driving","K. Hoshino; N. Ono","Graduate School of Systems and Information Engineering, University of Tsukuba, Tsukuba, 305-8573, Japan; Graduate School of Systems and Information Engineering, University of Tsukuba, Tsukuba, 305-8573, Japan","2019 IEEE International Conference on Mechatronics (ICM)","27 May 2019","2019","1","","225","231","If the eye movement of a driver who makes a long drive during the day and at night can be measured with less stress to the driver in order to estimate the following items for a long time with high accuracy, it is expected that the information will make a great contribution to safe driving: 1) What the driver is looking at or paying attention to (i.e. gaze), 2) A feeling or sign of unwellness such as dizziness, car sickness, and discomfort (i.e. rotational eye movement). In this study, the authors propose an approach in which an area of high intensity gradients in the white part of the eye, i.e. an area that includes a blood vessel with a distinctive shape, is automatically selected to estimate the gaze and calculate the angle of eye rotation, so that the target ocular blood vessel can be tracked with high accuracy even if blood vessels in the white part of the eye differ considerably between individuals in terms of shape, length, and thickness. An evaluation experiment yielded excellent results under various lighting conditions. Especially, extremely positive results were obtained for the estimation accuracy of rotational eye movement, with an estimation error of -0.69° ± 1.02° (mean and standard deviation).","","978-1-5386-6959-4","10.1109/ICMECH.2019.8722890","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8722890","gaze estimation;rotational eye movement measurement;vascular image tracking;intensity gradients","Blood vessels;Biomedical imaging;Feature extraction;Rotation measurement;Motion measurement;Vehicles;Light emitting diodes","biomedical measurement;biomedical optical imaging;blood vessels;eye;medical disorders","rotational eye movement;eye rotation;compact wearable eye movement measurement system;ocular blood vessel","","","","8","","27 May 2019","","","IEEE","IEEE Conferences"
"Residual-Network-Based Supervised Gaze Prediction for First-Person Videos","Y. Li; S. Ding; X. Li; B. Tan; A. Kanemura","National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Japan; School of Artificial Intelligence, Guilin University of Electronic Technology, Guilin, China; School of Computer Science and Engineering, The University of Aizu, Aizuwakamatsu, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba, Japan","IEEE Access","7 May 2019","2019","7","","56208","56216","Gaze prediction is a significant problem in efficiently processing and understanding a large number of incoming visual signals from first-person views (i.e., egocentric vision). Because many visual processes are expensive and human beings do not process the whole visual field, thus knowing the gaze position is an efficient way to understand the salient content of a video and what users pay attention to. However, current methods for gaze prediction are bottom-up methods and cannot incorporate information about user actions. We proposed a supervised gaze prediction framework based on a residual network, which takes the gaze of user action into consideration. Our model uses the features extracted from the VGG-16 deep neural network to predict the gaze position in FPV videos. The deep residual networks are introduced to combine with this model for learning the residual maps. Our proposed method attempts to obtain gaze prediction results with high accuracy. According to the experimental results, the performance of our proposed gaze prediction method is competitive with that of the state-of-the-art approaches.","2169-3536","","10.1109/ACCESS.2019.2913791","New Energy and Industrial Technology Development Organization; ImPACT Program of Council for Science, Technology and Innovation (Cabinet Office, Government of Japan); Japan Society for the Promotion of Science(grant numbers:18K18083,18KK0284); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8701440","Gaze prediction;first-person vision (FPV);saliency detection;convolution neural network (CNN);residual network","Feature extraction;Videos;Visualization;Biological neural networks;Saliency detection;Deep learning","feature extraction;neural nets;supervised learning;video signal processing","feature extraction;bottom-up methods;residual-network-based supervised gaze prediction;gaze prediction method;VGG-16 deep neural network;supervised gaze prediction framework;user action;gaze position;first-person views;incoming visual signals;first-person videos","","1","","39","OAPA","29 Apr 2019","","","IEEE","IEEE Journals"
"A Novel Remote Eye Gaze Tracking System Using Line Illumination Sources","J. O'Reilly; A. S. Khan; Z. Li; J. Cai; X. Hu; M. Chen; Y. Tong","Dept. of Comput. Sci. & Eng., Univ. of South Carolina, Columbia, SC, USA; Dept. of Comput. Sci. & Eng., Univ. of South Carolina, Columbia, SC, USA; Dept. of Comput. Sci. & Eng., Univ. of South Carolina, Columbia, SC, USA; Dept. of Comput. Sci. & Eng., Univ. of South Carolina, Columbia, SC, USA; Dept. of Comput. Sci. & Eng., Univ. of South Carolina, Columbia, SC, USA; Univ. of Washington, Bothell, WA, USA; Dept. of Comput. Sci. & Eng., Univ. of South Carolina, Columbia, SC, USA","2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)","25 Apr 2019","2019","","","449","454","This paper proposes a novel system to estimate the 3D point of gaze using observations of the pupil center and corneal reflections (glints). A mathematical model is developed with two solutions to estimate the corneal center efficiently using lines of LED lights. Differing from existing 3D approaches requiring associating light sources with glints, the model automatically associates glints and LED lines and can handle missing glints well. The new model also enables a user-friendly calibration process, allowing natural head movement. Experiments demonstrate that the proposed system can achieve accurate gaze estimation with natural head movement. The performance is impressive when using the natural calibration, requiring less user cooperation.","","978-1-7281-1198-8","10.1109/MIPR.2019.00090","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695323","Eye Tracking;Gaze Estimation;Robust;User Friendly Calibration","Light emitting diodes;Three-dimensional displays;Calibration;Germanium;Two dimensional displays;Light sources;Estimation","calibration;computational geometry;eye;feature extraction;gaze tracking;object tracking;solid modelling","pupil center;corneal reflections;corneal center;LED lights;line illumination sources;eye gaze tracking system;3D point of gaze;image feature detection;3D geometric-model based gaze estimation systems","","","","21","","25 Apr 2019","","","IEEE","IEEE Conferences"
"GazeVisual: A Practical Software Tool and Web Application for Performance Evaluation of Eye Tracking Systems","A. Kar; P. Corcoran","Department of Electrical and Electronic Engineering, National University of Ireland, Galway, Ireland; Department of Electrical and Electronic Engineering, National University of Ireland, Galway, Ireland","IEEE Transactions on Consumer Electronics","24 Jul 2019","2019","65","3","293","302","The concept and functionalities of a software tool developed for in depth performance evaluation of eye gaze estimation systems is presented. The software, GazeVisual has capabilities for quantitative, statistical, and visual analysis of eye gaze data as well as generation of static and dynamic visual stimuli for sample gaze data collection. This is a first of its kind cross-platform tool for gaze data analysis and evaluation. This software is made freely available to the eye gaze research and development community to provide a common framework for estimating the quality and reliability of data from eye tracking systems, especially those implemented in consumer electronics (CE) applications. The feasibility of using this software is tested through case studies which show that the software can handle eye gaze datasets obtained from several different consumer grade eye trackers. GazeVisual operates consistently, irrespective of the platform, algorithm or hardware of the eye trackers. In addition, the GazeVisual software capabilities are also made accessible via a Web-based application enabling performance evaluation of eye tracker data over a cloud-based platform.","1558-4127","","10.1109/TCE.2019.2912802","Science Foundation Ireland; Science Foundation Ireland(grant numbers:13/SPP/I2868); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695131","Eye tracking;eye gaze;performance evaluation;data quality;visualizations;Web-application","Software;Visualization;Graphical user interfaces;Gaze tracking;Data integrity;Data visualization;Tools","data analysis;eye;feature extraction;gaze tracking;human computer interaction;image motion analysis;Internet;software tools;video signal processing","Web application;eye tracking systems;eye gaze estimation systems;quantitative analysis;visual analysis;eye gaze data;static stimuli;dynamic visual stimuli;sample gaze data collection;gaze data analysis;eye gaze research;consumer electronics applications;eye gaze datasets;GazeVisual software capabilities;eye tracker data;statistical analysis;Web-based application;consumer grade eye trackers","","","","41","IEEE","22 Apr 2019","","","IEEE","IEEE Journals"
"Estimation of Gaze Region Using Two Dimensional Probabilistic Maps Constructed Using Convolutional Neural Networks","S. Jha; C. Busso","Multimodal Signal Processing (MSP) Laboratory, The University of Texas at Dallas, Richardson, TX, 75080, USA; Multimodal Signal Processing (MSP) Laboratory, The University of Texas at Dallas, Richardson, TX, 75080, USA","ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","17 Apr 2019","2019","","","3792","3796","Predicting the gaze of a user can have important applications in human computer interactions (HCI). They find applications in areas such as social interaction, driver distraction, human robot interaction and education. Appearance based models for gaze estimation have significantly improved due to recent advances in convolutional neural network (CNN). This paper proposes a method to predict the gaze of a user with deep models purely based on CNNs. A key novelty of the proposed model is that it produces a probabilistic map describing the gaze distribution (as opposed to predicting a single gaze direction). This approach is achieved by converting the regression problem into a classification problem, predicting the probability at the output instead of a single direction. The framework relies in a sequence of downsampling followed by upsampling to obtain the probabilistic gaze map. We observe that our proposed approach works better than a regression model in terms of prediction accuracy. The average mean squared error between the predicted gaze and the true gaze is observed to be 6.89° in a model trained and tested on the MSP-Gaze database, without any calibration or adaptation to the target user.","2379-190X","978-1-4799-8131-1","10.1109/ICASSP.2019.8683794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8683794","Convolutional neural networks;gaze estimation;regression by classification","","convolutional neural nets;gaze tracking;image classification;image sampling;image sequences;probability;regression analysis","human computer interactions;social interaction;driver distraction;human robot interaction;education;appearance based models;gaze estimation;deep models;gaze distribution;single gaze direction;probabilistic gaze map;regression model;prediction accuracy;MSP-Gaze database;target user;convolutional neural networks;gaze region estimation;two dimensional probabilistic maps;regression problem;classification problem","","1","","29","","17 Apr 2019","","","IEEE","IEEE Conferences"
"The Study of a Classification Technique for Numeric Gaze-Writing Entry in Hands-Free Interface","S. Yoo; D. K. Jeong; Y. Jang","Department of Computer Engineering, Sejong University, Seoul, South Korea; Department of Computer Engineering, Sejong University, Seoul, South Korea; Department of Computer Engineering, Sejong University, Seoul, South Korea","IEEE Access","19 Apr 2019","2019","7","","49125","49134","Recently, many applications are developed in numerous domains with various environments. Since some environments require hands-free applications, new technology is needed for the input interfaces other than the mouse and keyboard. Therefore, to meet the needs, many researchers have begun to investigate the gaze and voice for the input technology. In particular, there are many approaches to render virtual keyboards with the gaze. However, since the virtual keyboards hide the screen space, this technique can only be applied in limited environments. In this paper, we propose a classification technique for gaze-written numbers as the hands-free interface. Since the gaze-writing is less accurate compared to the virtual keyboard typing, we apply the convolutional neural network (CNN) deep learning algorithm to recognize the gaze-writing and improve the classification accuracy. Besides, we create new gaze-writing datasets for training, gaze MNIST (gMNIST), by modifying the MNIST data with features of the gaze movement patterns. For the evaluation, we compare our approach with the basic CNN structures using the original MNIST dataset. Our study will allow us to have more options for the input interfaces and expand our choices in hands-free environments.","2169-3536","","10.1109/ACCESS.2019.2909573","MSIT (Ministry of Science, ICT), South Korea, under the ITRC (Information Technology Research Center) support program(grant numbers:IITP-2019-2016-0-00312); Development of Complex Fast Stream Big Data Processing based on In-memory Technology in Distributed Environment; IITP (Institute for Information and communications Technology Promotion)(grant numbers:R0190-16-2016); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8682050","Gaze-writing;input technique;MNIST;Eye tracking;machine learning","Keyboards;Databases;Speech recognition;Data collection;Tools;Classification algorithms;Training","convolutional neural nets;gaze tracking;human computer interaction;image classification;image motion analysis;learning (artificial intelligence);neural nets","convolutional neural network deep learning algorithm;classification accuracy;gaze-writing datasets;gaze MNIST;gaze movement patterns;input interfaces;hands-free environments;classification technique;numeric gaze-writing entry;hands-free interface;mouse;input technology;virtual keyboards;gaze-written numbers;virtual keyboard typing;screen space;CNN deep learning algorithm;gaze MNIST training;gMNIST;MNIST dataset","","","","46","OAPA","9 Apr 2019","","","IEEE","IEEE Journals"
"Free-Head Appearance-Based Eye Gaze Estimation on Mobile Devices","L. Jigang; B. S. L. Francis; D. Rajan","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore","2019 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","21 Mar 2019","2019","","","232","237","Eye gaze tracking plays an important role in human-computer interaction applications. In recent years, many research have been performed to explore gaze estimation methods to handle free-head movement, most of which focused on gaze direction estimation. Gaze point estimation on the screen is another important application. In this paper, we proposed a two-step training network, called GazeEstimator, to improve the estimation accuracy of gaze location on mobile devices. The first step is to train an eye landmarks localization network on 300W-LP dataset [1], and the second step is to train a gaze estimation network on GazeCapture dataset [2]. Some processing operations are performed between the two networks for data cleaning. The first network is able to localize eye precisely on the image, while the gaze estimation network use only eye images and eye grids as inputs, and it is robust to facial expressions and occlusion.Compared with state-of-the-art gaze estimation method, iTracker, our proposed deep network achieves higher accuracy and is able to estimate gaze location even in the condition that the full face cannot be detected.","","978-1-5386-7822-0","10.1109/ICAIIC.2019.8669057","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669057","Eye gaze estimation;Deep learning;CNN;Eye localization;Gaze location","Estimation;Face;Mobile handsets;Computational modeling;Cameras;Gaze tracking","eye;face recognition;feature extraction;gaze tracking;human computer interaction;pose estimation","localization network;two-step training network;gaze point estimation;gaze direction estimation;free-head movement;gaze estimation methods;human-computer interaction applications;eye gaze tracking;mobile devices;free-head appearance-based eye gaze estimation;gaze location;deep network;gaze estimation network","","2","","25","","21 Mar 2019","","","IEEE","IEEE Conferences"
"Automatic Gaze Correction based on Deep Learning and Image Warping","M. Seo; T. Yamamoto; Y. Chen; T. Kitajima","College of Information Science and Engineering, Ritsumeikan University, Shiga, Japan; College of Information Science and Engineering, Ritsumeikan University, Shiga, Japan; College of Information Science and Engineering, Ritsumeikan University, Shiga, Japan; ET Center ET-2 Laboratory, Samsung R&D Institute Japan, Osaka, Japan","2019 IEEE International Conference on Consumer Electronics (ICCE)","7 Mar 2019","2019","","","1","4","When people take a selfie photo or talk through a video chat system, they tend to look at the screen. Since the position of the camera is usually different from that of the screen, the eyes of the person in the photograph or transmitted live video seem to make contact with something other than the viewer. This paper proposes a method to correct the gaze by synthesizing the image texture. The image synthesis uses feature points around the eyes as landmarks. A common convolutional neural network and long-term recurrent convolution network are used to extract or detect these feature points in a still image and video image, respectively. The deep learning-based feature point detections are very accurate compared to conventional methods. These feature points are used as landmarks, and the internal texture of these feature points are synthesized with the prepared template image in advance. Through these procedures, an automatic and natural gaze correction is realized. This method realized a natural gaze correction with little blur, even for a video image.","2158-4001","978-1-5386-7910-4","10.1109/ICCE.2019.8661836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8661836","gaze correction;image synthesis;convolutional neural network;long-term recurrent convolution networks","Feature extraction;Image generation;Active appearance model;Three-dimensional displays;Correlation;Cameras;Convolutional neural networks","cameras;convolutional neural nets;feature extraction;image texture;learning (artificial intelligence);recurrent neural nets;video signal processing","long-term recurrent convolution network;video image;deep learning-based feature point detections;automatic gaze correction;natural gaze correction;image warping;image texture;image synthesis;convolutional neural network;template image;still image","","","","13","","7 Mar 2019","","","IEEE","IEEE Conferences"
"Eyemotion: Classifying Facial Expressions in VR Using Eye-Tracking Cameras","S. Hickson; N. Dufour; A. Sud; V. Kwatra; I. Essa",NA; NA; NA; NA; NA,"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","7 Mar 2019","2019","","","1626","1635","One of the main challenges of social interaction in virtual reality settings is that head-mounted displays occlude a large portion of the face, blocking facial expressions and thereby restricting social engagement cues among users. We present an algorithm to automatically infer expressions by analyzing only a partially occluded face while the user is engaged in a virtual reality experience. Specifically, we show that images of the user's eyes captured from an IR gaze-tracking camera within a VR headset are sufficient to infer a subset of facial expressions without the use of any fixed external camera. Using these inferences, we can generate dynamic avatars in real-time which function as an expressive surrogate for the user. We propose a novel data collection pipeline as well as a novel approach for increasing CNN accuracy via personalization. Our results show a mean accuracy of 74% (F1 of 0.73) among 5 'emotive' expressions and a mean accuracy of 70% (F1 of 0.68) among 10 distinct facial action units, outperforming human raters.","1550-5790","978-1-7281-1975-5","10.1109/WACV.2019.00178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658392","","Cameras;Face;Resists;Avatars;Visualization;Headphones;Deep learning","avatars;cameras;emotion recognition;face recognition;helmet mounted displays","fixed external camera;expressive surrogate;facial expressions;eye-tracking cameras;social interaction;virtual reality settings;head-mounted displays;social engagement cues;partially occluded face;virtual reality experience;IR gaze-tracking camera;VR headset;facial action units;CNN accuracy;personalization;emotive expressions","","10","","44","","7 Mar 2019","","","IEEE","IEEE Conferences"
"Exploring Classification of Histological Disease Biomarkers From Renal Biopsy Images","P. Mathur; M. P. Ayyar; R. R. Shah; S. G. Sharma","MIDAS Lab., IIIT-Delhi, Delhi, India; MIDAS Lab, IIIT-DelhiDelhi, India; MIDAS Lab., IIIT-Delhi, Delhi, India; Arkana LaboratoriesArkansas, USA","2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","7 Mar 2019","2019","","","81","90","Identification of diseased kidney glomeruli and fibrotic regions remains subjective and time-consuming due to complete dependence on an expert kidney pathologist. In an attempt to automate the classification of glomeruli into normal and abnormal morphology and classification of fibrosis patches into mild, moderate and severe categories, we investigate three deep learning techniques: traditional transfer learning, pre-trained deep neural networks for feature extraction followed by supervised classification, and a novel Multi-Gaze Attention Network (MGANet) that uses multi-headed self-attention through parallel residual skip connections in a CNN architecture. Emperically, while the transfer learning models such as ResNet50, InceptionResNetV2, VGG19 and InceptionV3 acutely under-perform in the classification tasks, the Logistic Regression model augmented with features extracted from the InceptionResNetV2 shows promising results. Additionally, the experiments effectively ascertain that the proposed MGANet architecture outperforms both the former baseline techniques to establish the state of the art accuracy of 87.25% and 81.47% for glomerluli and fibrosis classification, respectively on the Renal Glomeruli Fibrosis Histopathological (RGFH) database.","1550-5790","978-1-7281-1975-5","10.1109/WACV.2019.00016","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658665","","Kidney;Feature extraction;Databases;Biopsy;Diseases;Medical diagnostic imaging","biological tissues;convolutional neural nets;diseases;feature extraction;image classification;kidney;learning (artificial intelligence);medical image processing;pattern classification;regression analysis","exploring classification;histological disease biomarkers;Renal biopsy;diseased kidney glomeruli;fibrotic regions;time-consuming;complete dependence;expert kidney pathologist;abnormal morphology;fibrosis patches;mild categories;moderate categories;severe categories;deep learning techniques;traditional transfer learning;pre-trained deep neural networks;feature extraction;supervised classification;parallel residual skip connections;CNN architecture;transfer learning models;ResNet50;InceptionResNetV2;VGG19;classification tasks;MGANet architecture;baseline techniques;fibrosis classification;InceptionV3;logistic regression model;renal glomeruli fibrosis histopathological database;multigaze attention network","","2","","40","","7 Mar 2019","","","IEEE","IEEE Conferences"
"Deep Attention Network for Egocentric Action Recognition","M. Lu; Z. -N. Li; Y. Wang; G. Pan","College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Computing Science, Simon Fraser University, Burnaby, Canada; Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China","IEEE Transactions on Image Processing","13 Jun 2019","2019","28","8","3703","3713","Recognizing a camera wearer's actions from videos captured by an egocentric camera is a challenging task. In this paper, we employ a two-stream deep neural network composed of an appearance-based stream and a motion-based stream to recognize egocentric actions. Based on the insight that human action and gaze behavior are highly coordinated in object manipulation tasks, we propose a spatial attention network to predict human gaze in the form of attention map. The attention map helps each of the two streams to focus on the most relevant spatial region of the video frames to predict actions. To better model the temporal structure of the videos, a temporal network is proposed. The temporal network incorporates bi-directional long short-term memory to model the long-range dependencies to recognize egocentric actions. The experimental results demonstrate that our method is able to predict attention maps that are consistent with human attention and achieve competitive action recognition performance with the state-of-the-art methods on the GTEA Gaze and GTEA Gaze+ datasets.","1941-0042","","10.1109/TIP.2019.2901707","Natural Sciences and Engineering Research Council of Canada(grant numbers:RGP36726); National Key Research and Development Program of China(grant numbers:2017YFB1002503); National Natural Science Foundation of China(grant numbers:61772460); Natural Science Foundation of Zhejiang Province(grant numbers:LR15F020001); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653357","Attention network;top-down attention;egocentric vision;action recognition;LSTM","Videos;Task analysis;Cameras;Streaming media;Predictive models;Data models;Head","cameras;gaze tracking;image motion analysis;neural nets;video signal processing","video frames;temporal network;deep attention network;egocentric action recognition;camera wearer;egocentric camera;two-stream deep neural network;appearance-based stream;motion-based stream;human action;gaze behavior;object manipulation tasks;spatial attention network;human gaze","Attention;Databases, Factual;Human Activities;Humans;Image Processing, Computer-Assisted;Neural Networks, Computer;Pattern Recognition, Automated;Video Recording","10","","52","IEEE","26 Feb 2019","","","IEEE","IEEE Journals"
"Appearance-Based Gaze Estimator for Natural Interaction Control of Surgical Robots","P. Li; X. Hou; X. Duan; H. Yip; G. Song; Y. Liu","School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Mechanical Engineering and Automation, Harbin Institute of Technology (Shenzhen), Shenzhen, China; School of Mechatronical Engineering, Beijing Institute of Technology, Beijing, China; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong; State Key Laboratory of Robotics, Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang, China; Department of Mechanical and Automation Engineering, The Chinese University of Hong Kong, Hong Kong","IEEE Access","5 Mar 2019","2019","7","","25095","25110","Robots are playing an increasingly important role in modern surgery. However, conventional human-computer interaction methods, such as joystick control and sound control, have some shortcomings, and medical personnel are required to specifically practice operating the robot. We propose a human-computer interaction model based on eye movement with which medical staff can conveniently use their eye movements to control the robot. Our algorithm requires only an RGB camera to perform tasks without requiring expensive eye-tracking devices. Two kinds of eye control modes are designed in this paper. The first type is the pick and place movement, with which the user uses eye gaze to specify the point where the robotic arm is required to move. The second type is user command movement, with which the user can use eye gaze to select the direction in which the user desires the robot to move. The experimental results demonstrate the feasibility and convenience of these two modes of movement.","2169-3536","","10.1109/ACCESS.2019.2900424","National Natural Science Foundation of China(grant numbers:U1613221); Shenzhen Fundamental Research(grant numbers:JCYJ20170307150346964); Guangdong Provincial Science and Technology Funds(grant numbers:2017A020211001); National Key Research and Development Program of China(grant numbers:2017YFB1302800); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8648437","Deep learning;surgical robot;gaze estimation;convolutional neural network","Convolutional neural networks;Computational modeling;Predictive models;Medical robotics;Task analysis;Prediction algorithms","cameras;gaze tracking;human-robot interaction;image colour analysis;manipulators;medical robotics;personnel;surgery","eye gaze;appearance-based gaze estimator;natural interaction control;surgical robots;medical personnel;human-computer interaction model;eye movement;medical staff;RGB camera;eye control modes;robotic arm;user command movement;surgery;eye-tracking devices;pick and place movement","","5","","41","OAPA","21 Feb 2019","","","IEEE","IEEE Journals"
"Convolutional Neural Network Implementation for Eye-Gaze Estimation on Low-Quality Consumer Imaging Systems","J. Lemley; A. Kar; A. Drimbarean; P. Corcoran","Department of Electrical and Electronic Engineering, National University of Ireland Galway, Galway, Ireland; Department of Electrical and Electronic Engineering, National University of Ireland Galway, Galway, Ireland; Vice President of Advanced Research, Fotonation Ltd., Galway, Ireland; Department of Electrical and Electronic Engineering, National University of Ireland Galway, Galway, Ireland","IEEE Transactions on Consumer Electronics","23 Apr 2019","2019","65","2","179","187","Accurate and efficient eye gaze estimation is important for emerging consumer electronic systems, such as driver monitoring systems and novel user interfaces. Such systems are required to operate reliably in difficult, unconstrained environments with low power consumption and at minimal cost. In this paper, a new hardware friendly, convolutional neural network (CNN) model with minimal computational requirements is introduced and assessed for efficient appearance-based gaze estimation. The model is tested and compared against existing appearance-based CNN approaches, achieving better eye gaze accuracy with significantly fewer computational requirements.","1558-4127","","10.1109/TCE.2019.2899869","Science Foundation Ireland(grant numbers:13/SPP/I2868); Irish Research Council(grant numbers:EBPPG/2016/280); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642826","Eye gaze;neural networks;deep learning","Estimation;Gaze tracking;Lighting;Solid modeling;Face;Three-dimensional displays","consumer electronics;convolutional neural nets;gaze tracking;image processing","appearance-based CNN approaches;appearance-based gaze estimation;convolutional neural network;eye gaze accuracy;minimal computational requirements;minimal cost;low power consumption;unconstrained environments;user interfaces;driver monitoring systems;consumer electronic systems;low-quality consumer imaging systems;eye-gaze estimation","","10","","47","IEEE","15 Feb 2019","","","IEEE","IEEE Journals"
"Predicting Human Saccadic Scanpaths Based on Iterative Representation Learning","C. Xia; J. Han; F. Qi; G. Shi","School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Automation, Northwestern Polytechnical University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China; School of Artificial Intelligence, Xidian University, Xi’an, China","IEEE Transactions on Image Processing","22 May 2019","2019","28","7","3502","3515","Visual attention is a dynamic process of scene exploration and information acquisition. However, existing research on attention modeling has concentrated on estimating static salient locations. In contrast, dynamic attributes presented by saccade have not been well explored in previous attention models. In this paper, we address the problem of saccadic scanpath prediction by introducing an iterative representation learning framework. Within the framework, saccade can be interpreted as an iterative process of predicting one fixation according to the current representation and updating the representation based on the gaze shift. In the predicting phase, we propose a Bayesian definition of saccade to combine the influence of perceptual residual and spatial location on the selection of fixations. In implementation, we compute the representation error of an autoencoder-based network to measure perceptual residuals of each area. Simultaneously, we integrate saccade amplitude and center-weighted mechanism to model the influence of spatial location. Based on estimating the influence of two parts, the final fixation is defined as the point with the largest posterior probability of gaze shift. In the updating phase, we update the representation pattern for the subsequent calculation by retraining the network with samples extracted around the current fixation. In the experiments, the proposed model can replicate the fundamental properties of psychophysics in visual search. In addition, it can achieve superior performance on several benchmark eye-tracking data sets.","1941-0042","","10.1109/TIP.2019.2897966","National Key R&D Program of China(grant numbers:2017YFB1002201); National Natural Science Foundation of China(grant numbers:61572387,61802314); China Postdoctoral Science Foundation(grant numbers:2017M623242); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637020","Visual attention;saccade;scanpath;deep learning;representation learning","Predictive models;Estimation;Feature extraction;Visualization;Brain modeling;Computational modeling;Data models","Bayes methods;biomechanics;eye;feature extraction;learning (artificial intelligence);object detection;probability;robot vision;visual perception","iterative representation learning framework;saccadic scanpath prediction;dynamic attributes;static salient locations;attention modeling;information acquisition;scene exploration;dynamic process;visual attention;predicting human saccadic scanpaths;current fixation;representation pattern;updating phase;final fixation;perceptual residuals;autoencoder-based network;representation error;spatial location;saccade;predicting phase;gaze shift","Adult;Algorithms;Attention;Databases, Factual;Deep Learning;Female;Humans;Image Processing, Computer-Assisted;Male;Models, Neurological;Saccades;Young Adult","3","","50","IEEE","7 Feb 2019","","","IEEE","IEEE Journals"
"Watch & Do: A Smart IoT Interaction System with Object Detection and Gaze Estimation","J. Kim; S. Choi; J. Jeong","Department of Computer Engineering, Kumoh National Institute of Technology, Gumi, South Korea; Department of Computer Engineering, Kumoh National Institute of Technology, Gumi, South Korea; Department of Computer Engineering, Kumoh National Institute of Technology, Gumi, South Korea","IEEE Transactions on Consumer Electronics","23 Apr 2019","2019","65","2","195","204","The Internet of Things (IoT) attempts to help people access Internet-connected devices, applications, and services anytime and anywhere. However, how providing an efficient and intuitive method of interaction between people and IoT devices is still an open challenge. In this paper, we propose a novel interaction system called Watch & Do, where users can control an IoT device by gazing at it and doing simple gestures. The proposed system mainly consists of: 1) object detection module; 2) gaze estimation module; 3) hand gesture recognition module; and 4) IoT controller module. The target device is identified by various deep learning-based gaze estimation and object detection techniques. Afterwards, hand gesture recognition is applied to generate an IoT device control command which is transmitted to the IoT platform. The experimental results and case studies demonstrate the feasibility of the proposed system and imply the future research directions.","1558-4127","","10.1109/TCE.2019.2897758","National Research Foundation of Korea; Ministry of Education(grant numbers:2016R1D1A1B03931672); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8634883","Deep learning;gaze estimation;Internet of Things;object detection;smart interaction","Internet of Things;Estimation;Object detection;Head;Gesture recognition;Gaze tracking;Object recognition","gesture recognition;Internet;Internet of Things;learning (artificial intelligence);object detection","deep learning-based gaze estimation;object detection techniques;hand gesture recognition;IoT device control command;smart IoT interaction system;Internet-connected devices;watch-and-do;Internet of Things","","2","","61","IEEE","5 Feb 2019","","","IEEE","IEEE Journals"
"An Accurate and Robust Gaze Estimation Method Based on Maximum Correntropy Criterion","B. Yang; X. Zhang; Z. Li; S. Du; F. Wang","National Engineering Laboratory for Visual Information Processing and Applications, Xi’an Jiaotong University, Xi’an, China; National Engineering Laboratory for Visual Information Processing and Applications, Xi’an Jiaotong University, Xi’an, China; National Engineering Laboratory for Visual Information Processing and Applications, Xi’an Jiaotong University, Xi’an, China; National Engineering Laboratory for Visual Information Processing and Applications, Xi’an Jiaotong University, Xi’an, China; National Engineering Laboratory for Visual Information Processing and Applications, Xi’an Jiaotong University, Xi’an, China","IEEE Access","4 Mar 2019","2019","7","","23291","23302","Accurately estimating the user's gaze is important in many applications, such as human-computer interaction. Due to great convenience, appearance-based methods for gaze estimation have been a popular subject of research for many years. However, the greatest challenges in the appearance-based gaze estimation in a desktop environment are how to simplify the calibration process and deal with other issues such as image noise and low resolution. To address the problems, we adopt a mapping relationship between the high-dimensional eye image features space and the low-dimensional gaze positions and propose a robust and accurate method for gaze estimation with a webcam. First, we utilize Kullback-Leibler divergence to reduce feature dimension and keep similarity between the feature space and the gaze space. Then, we construct the objective function using the maximum correntropy criterion instead of mean squared error, which can enhance the anti-noise ability, especially for outliers or pixel corruption. A regularization term is adopted to adaptively select the sparse training samples for gaze estimation. We conducted extensive experiments in a desktop environment, which verified that the proposed method was robust and efficient in dealing with sparse training samples, pixel corruption, and low-resolution problems in gaze estimation.","2169-3536","","10.1109/ACCESS.2019.2896303","National Basic Research Program of China (973 Program)(grant numbers:2017YFC0803905); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629993","Appearance-based method;human computer interaction;gaze estimation;maximum correntropy criterion","Estimation;Training;Manifolds;Feature extraction;Dimensionality reduction;Visualization;Three-dimensional displays","entropy;estimation theory;gaze tracking;image sampling;learning (artificial intelligence);mean square error methods","low-dimensional gaze positions;maximum correntropy criterion;appearance-based gaze estimation method;high-dimensional eye image feature space;human-computer interaction;calibration process;Kullback-Leibler divergence;mean squared error method;sparse training samples","","","","36","OAPA","30 Jan 2019","","","IEEE","IEEE Journals"
"A Method for Localizing the Eye Pupil for Point-of-Gaze Estimation","N. Panigrahi; K. Lavu; S. K. Gorijala; P. Corcoran; S. P. Mohanty","Indian Institute of Technology, Bombay, India; Vignan’s University, Vadlamudi, India; Vignan’s University, Vadlamudi, India; Engineering and Informatics, National University of Ireland Galway, Ireland; University of North Texas, Denton, United States","IEEE Potentials","28 Dec 2018","2019","38","1","37","42","The estimation of the point of gaze in a scene presented on a digital screen has many applications, such as fatigue detection and attention tracking. Some popular applications of eye tracking through gaze estimation are depicted in Fig. 1. When estimating the point of gaze, indentifying the visual focus of a person within a scene is required. This is known as the eye fix or point of fixation. Finding the point of gaze involves tracking different features of human eyes. Various methods are available for eye tracking, some of which use special contact lenses, whereas others focus on electrical potential measurements. Optical tracking is a nonintrusive technique that uses a sequence of image frames of eyes that have been recorded using video-capturing devices. This technique is popularly known as video oculography.","1558-1772","","10.1109/MPOT.2018.2850540","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8595416","","Eyes;Gaze tracking;Videos;Digital images","electro-oculography;eye;medical image processing;optical tracking;video signal processing","contact lenses;electrical potential measurements;video-capturing devices;video oculography;point-of-gaze estimation;eye pupil;optical tracking;human eyes;eye fix;eye tracking;attention tracking;fatigue detection","","2","","10","IEEE","28 Dec 2018","","","IEEE","IEEE Magazines"
"A Remote Free-Head Pupillometry Based on Deep Learning and Binocular System","Y. Ni; B. Sun","School of Electrical and Information Engineering, Tianjin University, Tianjin, China; School of Electrical and Information Engineering, Tianjin University, Tianjin, China","IEEE Sensors Journal","15 Feb 2019","2019","19","6","2362","2369","Objective: Pupillometer plays a key role in a variety of research areas, including disease diagnosis, human-machine interaction, and education. Here, we set out to leverage the deep learning theory to develop a remote binocular vision system for pupil diameter estimation. Approach: the system consists of three parts: eye detection, eye tracking, and pupil diameter estimation. We first train a convolutional neural network based on YOLO V2 to perform eye detection, leading to high accuracy and robustness under ambient light interference. By exploring the similarity of binocular camera images, we then propose a master-slave structure for eye tracking, surpassing the traditional parallel structure in tracking speed while keeping considerable accuracy. Furthermore, we develop a pupil diameter estimation algorithm based on binocular vision, avoiding the personal calibration procedure and reducing the measurement distortion error. Main results: Experimental results on real datasets reveal that our system exhibits the state-of-the-art performance with high eye detection accuracy (90.6%), fast eye tracking speed (<;11 ms per frame), low pupil diameter estimation error [(0.022±0.017) mm mean absolute error, and (0.6 ± 0.7)% percentage of the mean absolute error] and excellent flexibility. Significance: in contrast with previous pupillometers, which lead to pupil diameter measurement distortion error through a 2-D projection image on a single camera, our system measures pupil diameter in 3-D space without distortion influence, thus improving its robustness to head angle variation and making it more practical for real applications.","1558-1748","","10.1109/JSEN.2018.2885355","National Natural Science Foundation of China(grant numbers:61401303,51578189); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8565941","Pupil diameter (PD);deep learning;eye detection;master-slave structure;eye tracking;pupil detection;binocular vision","Cameras;Gaze tracking;Feature extraction;Estimation;Clustering algorithms;Calibration","biomedical equipment;biomedical measurement;calibration;cameras;diameter measurement;diseases;eye;learning (artificial intelligence);neural nets;user interfaces","high eye detection accuracy;low pupil diameter estimation error;mean absolute error;pupil diameter measurement distortion error;2-D projection image;system measures pupil diameter;remote free-head pupillometry;binocular system;disease diagnosis;human-machine interaction;deep learning theory;remote binocular vision system;eye tracking;convolutional neural network;YOLO V2;ambient light interference;binocular camera images;master-slave structure;traditional parallel structure;pupil diameter estimation algorithm","","1","","29","IEEE","6 Dec 2018","","","IEEE","IEEE Journals"
"Video Saliency Prediction Based on Spatial-Temporal Two-Stream Network","K. Zhang; Z. Chen","School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China; School of Remote Sensing and Information Engineering, Wuhan University, Wuhan, China","IEEE Transactions on Circuits and Systems for Video Technology","10 Dec 2019","2019","29","12","3544","3557","In this paper, we propose a novel two-stream neural network for video saliency prediction. Unlike some traditional methods based on hand-crafted feature extraction and integration, our proposed method automatically learns saliency related spatiotemporal features from human fixations without any pre-processing, post-processing, or manual tuning. Video frames are routed through the spatial stream network to compute static or color saliency maps for each of them. And a new two-stage temporal stream network is proposed, which is composed of a pre-trained 2D-CNN model (SF-Net) to extract saliency related features and a shallow 3D-CNN model (Te-Net) to process these features, for temporal or dynamic saliency maps. It can reduce the requirement of video gaze data, improve training efficiency, and achieve high performance. A fusion network is adopted to combine the outputs of both streams and generate the final saliency maps. Besides, a convolutional Gaussian priors (CGP) layer is proposed to learn the bias phenomenon in viewing behavior to improve the performance of the video saliency prediction. The proposed method is compared with state-of-the-art saliency models on two public video saliency benchmark datasets. The results demonstrate that our model can achieve advanced performance on video saliency prediction.","1558-2205","","10.1109/TCSVT.2018.2883305","National Key R&D Program of China(grant numbers:2017YFB1002202); National Natural Science Foundation of China(grant numbers:61471273,61771348); Wuhan Morning Light Plan of Youth Science and Technology(grant numbers:2017050304010302); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543830","Video saliency;spatial-temporal features;visual attention;deep learning","Feature extraction;Predictive models;Streaming media;Visualization;Spatiotemporal phenomena;Computational modeling","computer vision;convolutional neural nets;feature extraction;Gaussian processes;image colour analysis;image motion analysis;learning (artificial intelligence);object detection;video signal processing","video saliency prediction;hand-crafted feature extraction;two-stage temporal stream neural network;convolutional Gaussian priors;video gaze data;shallow 3D-CNN model;spatial-temporal two- stream network;public video saliency benchmark datasets;temporal saliency maps;2D-CNN model;color saliency maps;static saliency maps;spatiotemporal features","","11","","81","IEEE","25 Nov 2018","","","IEEE","IEEE Journals"
"Deep Visual Saliency on Stereoscopic Images","A. Nguyen; J. Kim; H. Oh; H. Kim; W. Lin; S. Lee","Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; Microsoft Research Asia, Beijing, China; Electronics and Telecommunications Research Institute, Daejeon, South Korea; Korea Institute of Science and Technology, Seoul, South Korea; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea","IEEE Transactions on Image Processing","11 Dec 2018","2019","28","4","1939","1953","Visual saliency on stereoscopic 3D (S3D) images has been shown to be heavily influenced by image quality. Hence, this dependency is an important factor in image quality prediction, image restoration and discomfort reduction, but it is still very difficult to predict such a nonlinear relation in images. In addition, most algorithms specialized in detecting visual saliency on pristine images may unsurprisingly fail when facing distorted images. In this paper, we investigate a deep learning scheme named Deep Visual Saliency (DeepVS) to achieve a more accurate and reliable saliency predictor even in the presence of distortions. Since visual saliency is influenced by low-level features (contrast, luminance, and depth information) from a psychophysical point of view, we propose seven low-level features derived from S3D image pairs and utilize them in the context of deep learning to detect visual attention adaptively to human perception. During analysis, it turns out that the low-level features play a role to extract distortion and saliency information. To construct saliency predictors, we weight and model the human visual saliency through two different network architectures, a regression and a fully convolutional neural networks. Our results from thorough experiments confirm that the predicted saliency maps are up to 70% correlated with human gaze patterns, which emphasize the need for the hand-crafted features as input to deep neural networks in S3D saliency detection.","1941-0042","","10.1109/TIP.2018.2879408","Samsung(grant numbers:SRFC-IT1702-08); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8520871","Saliency prediction;stereoscopic image;distorted image;convolutional neural network;deep learning","Feature extraction;Visualization;Distortion;Saliency detection;Two dimensional displays;Image color analysis","convolution;feature extraction;feedforward neural nets;image restoration;learning (artificial intelligence);neural net architecture;regression analysis;stereo image processing;visual perception","Deep Visual Saliency;S3D image pairs;saliency information;deep neural networks;S3D saliency detection;stereoscopic 3D images;image quality prediction;image restoration;distorted images;deep learning;distortion information;visual saliency prediction;regression network architecture;convolutional neural network architecture","","7","","55","IEEE","2 Nov 2018","","","IEEE","IEEE Journals"
"Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks","D. Lian; L. Hu; W. Luo; Y. Xu; L. Duan; J. Yu; S. Gao","School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Computer Science and Engineering, University of Electronic Science and Technology of China, Chengdu, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China","IEEE Transactions on Neural Networks and Learning Systems","18 Sep 2019","2019","30","10","3010","3023","Gaze estimation, which aims to predict gaze points with given eye images, is an important task in computer vision because of its applications in human visual attention understanding. Many existing methods are based on a single camera, and most of them only focus on either the gaze point estimation or gaze direction estimation. In this paper, we propose a novel multitask method for the gaze point estimation using multiview cameras. Specifically, we analyze the close relationship between the gaze point estimation and gaze direction estimation, and we use a partially shared convolutional neural networks architecture to simultaneously estimate the gaze direction and gaze point. Furthermore, we also introduce a new multiview gaze tracking data set that consists of multiview eye images of different subjects. As far as we know, it is the largest multiview gaze tracking data set. Comprehensive experiments on our multiview gaze tracking data set and existing data sets demonstrate that our multiview multitask gaze point estimation solution consistently outperforms existing methods.","2162-2388","","10.1109/TNNLS.2018.2865525","National Natural Science Foundation of China(grant numbers:61502304); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454246","Convolutional neural networks (CNNs);gaze tracking;multitask learning (MTL);multiview learning","Estimation;Gaze tracking;Head;Task analysis;Feature extraction;Cameras;Robustness","computer vision;convolutional neural nets;estimation theory;gaze tracking","multiview cameras;multiview gaze tracking data;convolutional neural networks architecture;multiview multitask gaze point estimation solution;multiview eye images;gaze direction estimation;deep convolutional neural networks","Attention;Fixation, Ocular;Humans;Multitasking Behavior;Neural Networks, Computer;Photic Stimulation","8","","53","IEEE","2 Sep 2018","","","IEEE","IEEE Journals"
"Toward Precise Gaze Estimation for Mobile Head-Mounted Gaze Tracking Systems","D. Su; Y. Li; H. Chen","Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong; Department of Mechanical Engineering, City University of Hong Kong, Hong Kong","IEEE Transactions on Industrial Informatics","3 May 2019","2019","15","5","2660","2672","The gaze estimation in the mobile scenario often suffers from the extrapolation and parallax errors. In this paper, we propose a novel calibration framework to achieve the precise gaze estimation for head-mounted gaze trackers. Our proposed framework consists of two steps to learn a point-to-point and a point-to-line relations, respectively. The aim of step I is to infer the relation between pupil centers and spatially constrained points of regard. By adopting the “CalibMe” gaze data acquisition method, a sparse Gaussian Process using pseudo-inputs is used to capture the smooth residual field unmodeled by the polynomial function. Meanwhile, a distraction detection criterion is introduced to identify the moment when user's attention is taken away from the calibration point thereby removing outliers. By combining with the point-to-point relation inferred in step I, the observed parallax errors are leveraged in step II to obtain a point-to-line relation, i.e., each pupil center will correspond to an epipolar line. Thus, the real image gaze point projected from different depths is predicted as the intersection of two epipolar lines inferred from binocular data. The simulation and experimental results show the effectiveness of our proposed calibration framework for head-mounted gaze trackers.","1941-0050","","10.1109/TII.2018.2867952","Research Grants Council of Hong Kong(grant numbers:11205015,11255716); National Natural Science Foundation of China(grant numbers:51335004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451912","Head-mounted gaze tracker;parallax error compensation;precise gaze estimation","Calibration;Estimation;Cameras;Gaze tracking;Extrapolation;Tracking;Head","calibration;cameras;data acquisition;Gaussian processes;gaze tracking;handicapped aids;image sensors;object tracking;regression analysis","mobile head-mounted gaze tracking systems;precise gaze estimation;calibration framework;sparse Gaussian Process;pseudo-inputs;smooth residual field;polynomial function;distraction detection criterion;image gaze point;epipolar line;observed parallax errors;point-to-point relation;calibration point;CalibMe gaze data acquisition method;pupil center;point-to-line relation;head-mounted gaze trackers","","5","","31","IEEE","30 Aug 2018","","","IEEE","IEEE Journals"
"MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation","X. Zhang; Y. Sugano; M. Fritz; A. Bulling","Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany; Graduate School of Information Science and Technology, Osaka University, Osaka, Japan; Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany; Max Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany","IEEE Transactions on Pattern Analysis and Machine Intelligence","4 Dec 2018","2019","41","1","162","175","Learning-based methods are believed to work well for unconstrained gaze estimation, i.e. gaze estimation from a monocular RGB camera without assumptions regarding user, environment, or camera. However, current gaze datasets were collected under laboratory conditions and methods were not evaluated across multiple datasets. Our work makes three contributions towards addressing these limitations. First, we present the MPIIGaze dataset, which contains 213,659 full face images and corresponding ground-truth gaze positions collected from 15 users during everyday laptop use over several months. An experience sampling approach ensured continuous gaze and head poses and realistic variation in eye appearance and illumination. To facilitate cross-dataset evaluations, 37,667 images were manually annotated with eye corners, mouth corners, and pupil centres. Second, we present an extensive evaluation of state-of-the-art gaze estimation methods on three current datasets, including MPIIGaze. We study key challenges including target gaze range, illumination conditions, and facial appearance variation. We show that image resolution and the use of both eyes affect gaze estimation performance, while head pose and pupil centre information are less informative. Finally, we propose GazeNet, the first deep appearance-based gaze estimation method. GazeNet improves on the state of the art by 22 percent (from a mean error of 13.9 degrees to 10.8 degrees) for the most challenging cross-dataset evaluation.","1939-3539","","10.1109/TPAMI.2017.2778103","Cluster of Excellence on Multimodal Computing and Interaction (MMCI); Saarland University; Alexander von Humboldt Postdoctoral Fellowship; JST CREST Research(grant numbers:JPMJCR14E1); Max Planck Institute for Informatics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122058","Unconstrained gaze estimation;cross-dataset evaluation;convolutional neural network;deep learning","Estimation;Head;Lighting;Cameras;Magnetic heads;Three-dimensional displays;Data models","cameras;computer vision;convolution;face recognition;gaze tracking;image colour analysis;image resolution;learning (artificial intelligence);motion estimation;neural nets;object detection;pose estimation","target gaze range;cross-dataset evaluation;face images;extensive evaluation;eye corners;cross-dataset evaluations;eye appearance;continuous gaze;experience sampling approach;corresponding ground-truth gaze positions;MPIIGaze dataset;multiple datasets;laboratory conditions;current gaze datasets;monocular RGB camera;unconstrained gaze estimation;learning-based methods;real-world dataset;deep appearance-based gaze estimation method;gaze estimation performance;facial appearance variation;illumination conditions","","50","","77","IEEE","28 Nov 2017","","","IEEE","IEEE Journals"
"Calibration-Free Gaze Zone Estimation Using Convolutional Neural Network","X. Cha; X. Yang; Z. Feng; T. Xu; X. Fan; J. Tian","University of Jinan,School of Information Science and Engineering,Jinan,China,250022; University of Jinan,School of Information Science and Engineering,Jinan,China,250022; University of Jinan,School of Information Science and Engineering,Jinan,China,250022; University of Jinan,School of Information Science and Engineering,Jinan,China,250022; University of Jinan,School of Information Science and Engineering,Jinan,China,250022; University of Jinan,School of Information Science and Engineering,Jinan,China,250022","2018 International Conference on Security, Pattern Analysis, and Cybernetics (SPAC)","23 Jan 2020","2018","","","481","484","In this paper we propose a gaze zone estimation method using deep learning. Compared with traditional method, our method does not need the procedure of calibration. In the proposed method, a Kinect is used to capture the video of a computer user, which is pre-processed to suppress illumination variations. After that, haar cascade classifier is adopted to detect the face region and eye region. Then, the eye region is used to estimate the gaze zone on the monitor via a trained CNN (Convolution Neural Network). Experimental results show that the proposed method has a high accuracy, which can be applied in human-computer interaction.","","978-1-7281-0551-2","10.1109/SPAC46244.2018.8965441","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8965441","Gaze estimation;eye tracking;convolutional neural network;deep learning","Estimation;Deep learning;Image color analysis;Convolutional neural networks;Information science;Face;Convolution","convolutional neural nets;face recognition;feature extraction;gaze tracking;learning (artificial intelligence);video signal processing","illumination variations;haar cascade classifier;face region;eye region;calibration-free gaze zone estimation;convolutional neural network;gaze zone estimation method;deep learning;computer user;human-computer interaction","","","","14","","23 Jan 2020","","","IEEE","IEEE Conferences"
"Simple and Accurate Iris Center Localization Method based on Geometric Relationship of Eyeball Model","J. Lin; X. Zhou; S. Chen; B. Lei","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; College of Computer and Information, China Three Gorges University, Yichang, China","2018 IEEE International Conference on Information and Automation (ICIA)","26 Aug 2019","2018","","","661","666","Precise iris localization plays a significant role on accurate gaze estimation. In this paper, a novel iris center localization method is proposed to deal with the iris contour change caused by the rotation of eyeball using the geometric relationship among iris center, eyeball center and iris contour. First, a face alignment method is employed to detect the face feature points that are used to initialize the iris center and eyeball locations. The three-dimensional rotation information of the eyeball is then described by the positional relationship between the centers of the eyeball and iris at the image level. The new obtained iris contour after the rotation is used to update the location of the iris center iteratively. To reduce the effect of illumination on image binarization, the cumulative histogram is introduced to adaptively get the threshold for binarization. Finally, experiments conducted on the BioID Face dataset and GI4E dataset as well as eight subjects demonstrate the superior performance of the proposed method.","","978-1-5386-8069-8","10.1109/ICInfA.2018.8812598","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812598","iris center;localization","Iris;Iris recognition;Face;Cameras;Feature extraction;Estimation;Lighting","eye;iris recognition;iterative methods","precise iris localization;accurate gaze estimation;iris center localization method;iris contour change;face alignment method;eyeball model geometric relationship;image binarization;iterative iris center location","","","","23","","26 Aug 2019","","","IEEE","IEEE Conferences"
"SmartEye: An Accurate Infrared Eye Tracking System for Smartphones","B. Brousseau; J. Rose; M. Eizenman","Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Electrical and Computer Engineering, Institute Biomedical Engineering University of Toronto, Toronto, Canada","2018 9th IEEE Annual Ubiquitous Computing, Electronics & Mobile Communication Conference (UEMCON)","15 Aug 2019","2018","","","951","959","The capability to estimate where a user is looking on a screen is known as gaze estimation or eye tracking. It has been used in medical applications including assessment of mood and learning disorders, and brain injury diagnosis. If accurate eye tracking could be integrated into commodity smartphones these diagnostics could be broadly deployed at very low cost. The highest accuracy and most robust eye tracking methods employ infrared cameras and illumination which are not yet available on all standard smartphones. In this paper, we present an accurate infrared eye tracking system on a smartphone, named SmartEye, on an industrial prototype phone equipped with an infrared camera and illumination. The system is accurate in the presence of head pose variation and device movements in the user's hands, and requires only a one-time calibration routine to measure specific parameters of the user's eye. Our system achieves a gaze estimation bias of 0.57<sup>°</sup> at a 20cm distance from the user, 5 times better than state-of-the art mobile device eye-tracking systems that do not use infrared illumination. Our system also allows for free head movements at distances between 20-40cm with a moderate increase in average gaze bias (to ~1<sup>°</sup>), and can operate at 12fps. This enhanced accuracy and increased mobility can expand significantly the range of eye-tracking applications that can be supported by smartphones.","","978-1-5386-7693-6","10.1109/UEMCON.2018.8796799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8796799","Eye Tracking; Gaze Estimation; Mobile Computing; Mobile Eye-Tracking;Gaze-Based Interaction","","calibration;cameras;gaze tracking;infrared imaging;mobile computing;object tracking;smart phones","commodity smartphones;infrared camera;gaze estimation bias;infrared illumination;eye-tracking applications;mobile device eye-tracking systems;SmartEye system;robust eye tracking methods;infrared eye tracking system;size 20.0 cm;size 20.0 cm to 40.0 cm","","2","","44","","15 Aug 2019","","","IEEE","IEEE Conferences"
"Deep Learning Frameworks for Off-Angle Iris Recognition","M. Karakaya","University of Central Arkansas, Conway, AR, 72035, USA","2018 IEEE 9th International Conference on Biometrics Theory, Applications and Systems (BTAS)","25 Apr 2019","2018","","","1","8","Although iris is known as one of the most accurate, distinctive, and reliable biometric identification, the accuracy of iris recognition depends on the image quality and it is negatively affected by several factors such as gaze angle, occlusion, and dilation. Since standoff iris recognition systems are much less constrained than traditional systems, the iris images captured are likely to be non-ideal, off-angle, and dilated. In this paper, we present a deep learning algorithm to improve performance of off-angle iris recognition in traditional and untraditional iris recognition frameworks. As a main contribution, this approach will allow us to study the effect of the gaze angle in ocular/periocular biometrics and fuse the information in different off-angle iris images. Using convolutional neural networks (CNNs), we first investigate traditional iris recognition framework with segmentation, normalization, and CNN based encoding and matching. In nontraditional framework, we use the iris images without segmentation and normalization to investigate the effect of periocular regions. We train and test our deep learning based approach with frontal and off-angle iris images in different sizes and types such as original, cropped, segmented, and masked images. Based on results from our real off-angle iris image dataset with 52 subjects, the proposed method improved the recognition performance compared with traditional off-angle iris recognition approaches.","2474-9699","978-1-5386-7180-1","10.1109/BTAS.2018.8698565","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698565","","Iris recognition;Deep learning;Image segmentation;Feature extraction;Biomedical imaging;Image recognition;Convolution","biometrics (access control);eye;feature extraction;image capture;image segmentation;image texture;iris recognition;learning (artificial intelligence);neural nets","off-angle iris recognition;standoff iris recognition;iris images;traditional iris recognition framework;deep learning based approach;off-angle iris image dataset;biometric identification;gaze angle effect;image quality;convolutional neural networks;cropped image;segmented image;masked image","","","","20","","25 Apr 2019","","","IEEE","IEEE Conferences"
"Deep Learning-Based Eye Gaze Controlled Robotic Car","D. Saha; M. Ferdoushi; M. T. Emrose; S. Das; S. M. M. Hasan; A. I. Khan; C. Shahnaz","Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology; Department of Electrical and Electronic Engineering, Bangladesh University of Engineering and Technology","2018 IEEE Region 10 Humanitarian Technology Conference (R10-HTC)","31 Jan 2019","2018","","","1","6","In recent years Eye gaze tracking (EGT) has emerged as an attractive alternative to conventional communication modes. Gaze estimation can be effectively used in human-computer interaction, assistive devices for motor-disabled persons, autonomous robot control systems, safe car driving, diagnosis of diseases and even in human sentiment assessment. Implementation in any of these areas however mostly depends on the efficiency of detection algorithm along with usability and robustness of detection process. In this context we have proposed a Convolutional Neural Network (CNN) architecture to estimate the eye gaze direction from detected eyes which outperforms all other state of the art results for Eye-Chimera dataset. The overall accuracies are 90.21% and 99.19% for Eye-Chimera and HPEG datasets respectively. This paper also introduces a new dataset EGDC for which proposed algorithm finds 86.93% accuracy. We have developed a real-time eye gaze controlled robotic car as a prototype for possible implementations of our algorithm.","2572-7621","978-1-5386-5051-6","10.1109/R10-HTC.2018.8629836","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8629836","Eye Gaze Estimation;Human-Computer Interaction;Eye Detection;Eye Tracking;Convolutional Neural Network;EAC","Automobiles;Estimation;Robots;Real-time systems;Feature extraction;Face;Support vector machines","control engineering computing;convolutional neural nets;diseases;gaze tracking;handicapped aids;human computer interaction;learning (artificial intelligence);mobile robots;neural net architecture;robot vision","gaze estimation;human-computer interaction;assistive devices;motor-disabled persons;autonomous robot control systems;safe car driving;human sentiment assessment;Eye-Chimera dataset;communication modes;eye gaze tracking;convolutional neural network architecture;deep learning;eye gaze controlled robotic car;EGT;diseases diagnosis;HPEG datasets","","1","","30","","31 Jan 2019","","","IEEE","IEEE Conferences"
"Efficient and Low-Cost Deep-Learning Based Gaze Estimator for Surgical Robot Control","P. Li; X. Hou; L. Wei; G. Song; X. Duan","Harbin Institute of Technology (ShenZhen), School of Mechanical Engineering and Automation, Shenzhen, China; Harbin Institute of Technology (ShenZhen), School of Mechanical Engineering and Automation, Shenzhen, China; Harbin Institute of Technology (ShenZhen), School of Mechanical Engineering and Automation, Shenzhen, China; Shenyang Institute of Automation, Chinese Academy of Sciences, State Key Laboratory of Robotics, Shenyang, China; Beijing Institute of Technology, School of Mechatronical Engineering, Beijing, China","2018 IEEE International Conference on Real-time Computing and Robotics (RCAR)","24 Jan 2019","2018","","","58","63","Surgical robots are playing more and more important role in modern operating room. However, operations by using surgical robot are not easy to handle by doctors. Vision based human-computer interaction (HCI) is a way to ease the difficulty to control surgical robots. While the problem of this method is that eyes tracking devices are expensive. In this paper, a low cost and robust deep-learning based on gaze estimator is proposed to control surgical robots. By this method, doctors can easily control the robot by specifying the starting point and ending point of the surgical robot using eye gazing. Surgical robots can also be controlled to move in 9 directions using controllers' eyes gazing information. A Densely Connected convolutional Neural Networks (Dense CNN) model for 9-direction/36-direction gaze estimation is built. The Dense CNN architecture has much more less trainable parameters compared to traditional CNN network architecture (AlexNet like/VGG like) which is more feasible to deploy on the Field-Programmable Gate Array (FPGA) and other hardware with limited memories.","","978-1-5386-6869-6","10.1109/RCAR.2018.8621810","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621810","Deep Learning;Minimally Invasive Surgery;Gaze estimation;Convolutional Neural Neural;Surgical robot","Medical robotics;Monitoring;Convolutional neural networks;Cameras;Indexes;Training","computer vision;control engineering computing;convolutional neural nets;gaze tracking;human computer interaction;learning (artificial intelligence);medical robotics;neural net architecture;surgery","surgical robot control;deep-learning based gaze estimator;densely connected convolutional neural networks;CNN architecture;vision based human-computer interaction","","2","","15","","24 Jan 2019","","","IEEE","IEEE Conferences"
"Prediction and Localization of Student Engagement in the Wild","A. Kaur; A. Mustafa; L. Mehta; A. Dhall","Learning Affect & Semantic Imaging Analysis Group, Indian Institute of Technology Ropar; Learning Affect & Semantic Imaging Analysis Group, Indian Institute of Technology Ropar; Learning Affect & Semantic Imaging Analysis Group, Indian Institute of Technology Ropar; Learning Affect & Semantic Imaging Analysis Group, Indian Institute of Technology Ropar","2018 Digital Image Computing: Techniques and Applications (DICTA)","17 Jan 2019","2018","","","1","8","Digital revolution has transformed the traditional teaching procedures, students are going online to access study materials. It is realised that analysis of student engagement in an e-learning environment would facilitate effective task accomplishment and learning. Well known social cues of engagement/disengagement can be inferred from facial expressions, body movements and gaze patterns. In this paper, student's response to various stimuli (educational videos) are recorded and cues are extracted to estimate variations in engagement level. We study the association of a subject's behavioral cues with his/her engagement level, as annotated by labelers. We have localized engaging/non-engaging parts in the stimuli videos using a deep multiple instance learning based framework, which can give useful insight into designing Massive Open Online Courses (MOOCs) video material. Recognizing the lack of any publicly available dataset in the domain of user engagement, a new `in the wild' dataset is curated. The dataset: Engagement in the Wild contains 264 videos captured from 91 subjects, which is approximately 16.5 hours of recording. Detailed baseline results using different classifiers ranging from traditional machine learning to deep learning based approaches are evaluated on the database. Subject independent analysis is performed and the task of engagement prediction is modeled as a weakly supervised learning problem. The dataset is manually annotated by different labelers and the correlation studies between annotated and predicted labels of videos by different classifiers are reported. This dataset creation is an effort to facilitate research in various e-learning environments such as intelligent tutoring systems, MOOCs, and others.","","978-1-5386-6602-9","10.1109/DICTA.2018.8615851","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8615851","Dataset for Student Engagement;Engagement Detection;Engagement Localization;E-learning Environment","Videos;Electronic learning;Task analysis;Databases;Atmospheric measurements;Particle measurements;Affective computing","educational courses;intelligent tutoring systems;learning (artificial intelligence);user interfaces;video signal processing","student engagement;e-learning environment;educational videos;publicly available dataset;user engagement;subject independent analysis;task accomplishment;social cues;deep learning;supervised learning;teaching procedures;massive open online courses video material;intelligent tutoring systems","","3","","36","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Eye Contact Detection Algorithms Using Deep Learning and Generative Adversarial Networks","Y. Mitsuzumi; A. Nakazawa","Grad. Sch. of Infomatics, Kyoto Univ., Kyoto, Japan; Grad. Sch. of Infomatics, Kyoto Univ., Kyoto, Japan","2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","17 Jan 2019","2018","","","3927","3931","Eye contact (mutual gaze) is a foundation of human communication and social interactions; therefore, it is studied in many fields such as psychology, social science, and medicine. Our group have been studied wearable vision-based eye contact detection techniques using a first person camera for the purpose of evaluating the gaze skills in the tender dementia care. In this work, we search for deep learning-based eye contact detection techniques from small number of labeled images. We implemented and tested two eye contact detection algorithms: naïve deep-learning-based algorithm and generative adversarial networks (GAN)-based semi supervised learning (SSL) algorithm. These methods are learned and verified by using Columbia Gaze Dataset, Facescrub and our original datasets. The results show the effectiveness and limitations of the deep-learning-based and GAN-based approaches. Interestingly, we found the bilateral difference of the accuracy of eye contact detection with respect to the facial pose with respect to the camera, which is expected to be caused by the learning datasets.","2577-1655","978-1-5386-6650-0","10.1109/SMC.2018.00666","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8616663","eye contact;mutual gaze;deep learning;generative adversarial network (GAN);semi supervised learning","Cameras;Generators;Generative adversarial networks;Gallium nitride;Detection algorithms;Dementia;Supervised learning","computer vision;face recognition;learning (artificial intelligence);visual databases","eye contact detection algorithms;mutual gaze;wearable vision-based eye contact detection techniques;deep learning-based eye contact detection techniques;generative adversarial networks-based semisupervised learning algorithm;Columbia Gaze Dataset;learning datasets;GAN","","","","10","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Classification of Eye Tracking Data Using a Convolutional Neural Network","Y. Yin; C. Juan; J. Chakraborty; M. P. McGuire","Dept. of Comput. & Inf. Sci., Towson Univ., Towson, MD, USA; Dept. of Comput. & Inf. Sci., Towson Univ., Towson, MD, USA; Dept. of Comput. & Inf. Sci., Towson Univ., Towson, MD, USA; Dept. of Comput. & Inf. Sci., Towson Univ., Towson, MD, USA","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","17 Jan 2019","2018","","","530","535","Historically, eye tracking analysis has been a useful approach to identify areas of interest (AOIs) where users have specific regions of the user interface (UI) in which they are interested. Many algorithms have been proposed to analyze eye tracking data in order to make user interfaces more effective. The objective of this study is to use convolutional neural networks (CNNs) to classify eye tracking data. First, a CNN was used to classify two different web interfaces for browsing news data. Then in a second experiment, a CNN was used to classify the nationalities of users. In addition, techniques of data-preprocessing and feature-engineering were applied. The algorithm used in this research is convolutional neural network (CNN), which is famous in deep learning field. Keras framework running on top of TensorFlow was used to define and train our CNN model. The purpose of this research is to explore how feature-engineering can affect evaluation metrics about our model. The results of the study show a number of interesting patterns and generally that deep learning shows promise in the analysis of eye tracking data.","","978-1-5386-6805-4","10.1109/ICMLA.2018.00085","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614110","eye tracking, CNN, deep learning, feature-engineering, gaze point, Keras, TensorFlow","Gaze tracking;Two dimensional displays;Data models;Deep learning;Arrays","convolutional neural nets;data analysis;learning (artificial intelligence);pattern classification;user interfaces","convolutional neural network;eye tracking analysis;user interface;CNN;eye tracking data classification;Web interfaces;news data;data-preprocessing;feature-engineering;deep learning field;Keras framework;TensorFlow;evaluation metrics","","2","","17","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Evaluation of a New Kernel-Based Classifier in Eye Pupil Detection","P. H. B. Monforte; G. Matos Araujo; A. Azevedo De Lima","Centro Fed. de Educ. Tecnolgica Celso Suckow da Fonseca, Nova Iguacu, Brazil; Centro Fed. de Educ. Tecnolgica Celso Suckow da Fonseca, Nova Iguacu, Brazil; Centro Fed. de Educ. Tecnolgica Celso Suckow da Fonseca, Nova Iguacu, Brazil","2018 17th IEEE International Conference on Machine Learning and Applications (ICMLA)","17 Jan 2019","2018","","","380","385","Accurate pupil location is paramount to applications such as gaze estimation, assistive technologies and several man-machine interfaces as the ones found in smartphones and VR applications. We introduce a new classifier stemmed from the Inner Product Detector and investigate its features on the challenging task of pupil localization. IPD (Inner Product Detector) is a classifier with high potential in facial landmarks detection. It is robust to variations in the desired pattern while maintaining good generalization and computational efficiency. However, one possible limitation is its linear behavior, which could be overcome by aggregating non-linear techniques, such as kernel methods. Although kernel classifiers have been exhaustively studied in the past two decades, it was not analyzed or applied with IPD, yet. The proposed KIPD achieves in the worst case an accuracy of 97.41% on the BioID dataset and 93.71% in LFPW dataset both at 10% of the interocular distance. In this paper the KIPD is compared to the state of the art methods, including the ones using deep learning, being competitive in terms of accuracy as well as computational complexity.","","978-1-5386-6805-4","10.1109/ICMLA.2018.00063","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8614088","kernel machines, correlation filtes, machine learning","Kernel;Training;Detectors;Face;Correlation;Biomedical imaging;Robustness","computational complexity;eye;face recognition;learning (artificial intelligence);pattern classification","Kernel-based classifier;eye pupil detection;accurate pupil location;gaze estimation;man-machine interfaces;VR applications;pupil localization;IPD;facial landmarks detection;computational efficiency;linear behavior;aggregating nonlinear techniques;kernel methods;kernel classifiers;inner product detector","","","","22","","17 Jan 2019","","","IEEE","IEEE Conferences"
"Comparative study of visual saliency maps in the problem of classification of architectural images with Deep CNNs","A. M. Obeso; J. Benois-Pineau; K. Guissous; V. Gouet-Brunet; M. S. García Vázquez; A. A. Ramírez Acosta","Université de Bordeaux, LaBRI, Bordeaux, France; Université de Bordeaux, LaBRI, Bordeaux, France; Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, F-94160, France; Univ. Paris-Est, LASTIG MATIS, IGN, ENSG, Saint-Mande, F-94160, France; Instituto Politécnico Nacional, CITEDI, Tijuana, México; MIRAL R&D&I, San Diego, United States","2018 Eighth International Conference on Image Processing Theory, Tools and Applications (IPTA)","13 Jan 2019","2018","","","1","6","Incorporating Human Visual System (HVS) models into building of classifiers has become an intensively researched field in visual content mining. In the variety of models of HVS we are interested in so-called visual saliency maps. Contrarily to scan-paths they model instantaneous attention assigning the degree of interestingness/saliency for humans to each pixel in the image plane. In various tasks of visual content understanding, these maps proved to be efficient stressing contribution of the areas of interest in image plane to classifiers models. In previous works saliency layers have been introduced in Deep CNNs, showing that they allow reducing training time getting similar accuracy and loss values in optimal models. In case of large image collections efficient building of saliency maps is based on predictive models of visual attention. They are generally bottom-up and are not adapted to specific visual tasks. Unless they are built for specific content, such as ""urban images""-targeted saliency maps we also compare in this paper. In present research we propose a ""bootstrap"" strategy of building visual saliency maps for particular tasks of visual data mining. A small collection of images relevant to the visual understanding problem is annotated with gaze fixations. Then the propagation to a large training dataset is ensured and compared with the classical GBVS model and a recent method of saliency for urban image content. The classification results within Deep CNN framework are promising compared to the purely automatic visual saliency prediction.","2154-512X","978-1-5386-6428-5","10.1109/IPTA.2018.8608125","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8608125","Saliency Maps;Deep Learning;Mexican Culture","Visualization;Task analysis;Windows;Image segmentation;Training;Semantics","architecture;convolutional neural nets;data mining;image classification","visual content mining;model instantaneous attention;image plane;visual content understanding;classifiers models;predictive models;visual attention;urban images;targeted saliency maps;visual data mining;visual understanding problem;urban image content;architectural image classification;deep CNNs;visual saliency maps;image collections;visual tasks;GBVS model;automatic visual saliency prediction;human visual system models;bootstrap strategy;gaze fixations","","2","","25","","13 Jan 2019","","","IEEE","IEEE Conferences"
"The RobotriX: An Extremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions","A. Garcia-Garcia; P. Martinez-Gonzalez; S. Oprea; J. A. Castro-Vargas; S. Orts-Escolano; J. Garcia-Rodriguez; A. Jover-Alvarez","University of Alicante, 3D Perception Lab, Spain; University of Alicante, 3D Perception Lab, Spain; University of Alicante, 3D Perception Lab, Spain; University of Alicante, 3D Perception Lab, Spain; University of Alicante, 3D Perception Lab, Spain; University of Alicante, 3D Perception Lab, Spain; University of Alicante, 3D Perception Lab, Spain","2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","6 Jan 2019","2018","","","6790","6797","Enter the RobotriX, an extremely photorealistic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems. The RobotriX consists of hyperrealistic indoor scenes which are explored by robot agents which also interact with objects in a visually realistic manner in that simulated world. Photorealistic scenes and robots are rendered by Unreal Engine into a virtual reality headset which captures gaze so that a human operator can move the robot and use controllers for the robotic hands; scene information is dumped on a per-frame basis so that it can be reproduced offline using UnrealCV to generate raw data and ground truth labels. By taking this approach, we were able to generate a dataset of 38 semantic classes across 512 sequences totaling 8M stills recorded at +60 frames per second with full HD resolution. For each frame, RGB-D and 3D information is provided with full annotations in both spaces. Thanks to the high quality and quantity of both raw information and annotations, the RobotriX will serve as a new milestone for investigating 2D and 3D robotic vision tasks with large-scale data-driven techniques.","2153-0866","978-1-5386-8094-0","10.1109/IROS.2018.8594495","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594495","","Robots;Three-dimensional displays;Trajectory;Deep learning;Image resolution;Rendering (computer graphics);Layout","control engineering computing;image colour analysis;image resolution;learning (artificial intelligence);rendering (computer graphics);robot vision;trajectory control;virtual reality","RobotriX;extremely photorealistic indoor dataset;very-large-scale indoor dataset;robot trajectories;deep learning techniques;robotic vision problems;hyperrealistic indoor scenes;robot agents;Unreal Engine;virtual reality headset;robotic hands;ground truth labels;3D robotic vision tasks;large-scale data-driven techniques;UnrealCV;full HD resolution;RGB-D;2D robotic vision tasks","","3","","30","","6 Jan 2019","","","IEEE","IEEE Conferences"
"Detection- Tracking for Efficient Person Analysis: The DetTA Pipeline","S. Breuers; L. Beyer; U. Rafi; B. Leibel","RWTH Aachen University, Visual Computing Institute; RWTH Aachen University, Visual Computing Institute; RWTH Aachen University, Visual Computing Institute; RWTH Aachen University, Visual Computing Institute","2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","6 Jan 2019","2018","","","48","53","In the past decade many robots were deployed in the wild, and people detection and tracking is an important component of such deployments. On top of that, one often needs to run modules which analyze persons and extract higher level attributes such as age and gender, or dynamic information like gaze and pose. The latter ones are especially necessary for building a reactive, social robot-person interaction. In this paper, we combine those components in a fully modular detection-tracking-analysis pipeline, called DetTA. We investigate the benefits of such an integration on the example of head and skeleton pose, by using the consistent track ID for a temporal filtering of the analysis modules' observations, showing a slight improvement in a challenging real-world scenario. We also study the potential of a so-called “free-flight” mode, where the analysis of a person attribute only relies on the filter's predictions for certain frames. Here, our study shows that this boosts the runtime dramatically, while the prediction quality remains stable. This insight is especially important for reducing power consumption and sharing precious (GPU-)memory when running many analysis components on a mobile platform, especially so in the era of expensive deep learning methods.","2153-0866","978-1-5386-8094-0","10.1109/IROS.2018.8594335","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594335","","Robots;Pipelines;Skeleton;Head;Estimation;Detectors;Trajectory","feature extraction;human-robot interaction;image filtering;learning (artificial intelligence);object detection;object tracking;pose estimation;robot vision","DetTA pipeline;people detection;dynamic information;social robot-person interaction;fully modular detection-tracking-analysis pipeline;temporal filtering;person attribute;track ID;person analysis;GPU-memory;power consumption;head pose;skeleton pose;deep learning methods","","3","","48","","6 Jan 2019","","","IEEE","IEEE Conferences"
"Human Gaze Following for Human-Robot Interaction","A. Saran; S. Majumdar; E. S. Short; A. Thomaz; S. Niekum","University of Texas at Austin, Department of Computer Science, Austin, TX, 78712, USA; University of Texas at Austin, Department of Electrical and Computer Engineering, Austin, TX, 78712, USA; University of Texas at Austin, Department of Electrical and Computer Engineering, Austin, TX, 78712, USA; University of Texas at Austin, Department of Electrical and Computer Engineering, Austin, TX, 78712, USA; University of Texas at Austin, Department of Computer Science, Austin, TX, 78712, USA","2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","6 Jan 2019","2018","","","8615","8621","Gaze provides subtle informative cues to aid fluent interactions among people. Incorporating human gaze predictions can signify how engaged a person is while interacting with a robot and allow the robot to predict a human's intentions or goals. We propose a novel approach to predict human gaze fixations relevant for human-robot interaction tasks-both referential and mutual gaze-in real time on a robot. We use a deep learning approach which tracks a human's gaze from a robot's perspective in real time. The approach builds on prior work which uses a deep network to predict the referential gaze of a person from a single 2D image. Our work uses an interpretable part of the network, a gaze heat map, and incorporates contextual task knowledge such as location of relevant objects, to predict referential gaze. We find that the gaze heat map statistics also capture differences between mutual and referential gaze conditions, which we use to predict whether a person is facing the robot's camera or not. We highlight the challenges of following a person's gaze on a robot in real time and show improved performance for referential gaze and mutual gaze prediction.","2153-0866","978-1-5386-8094-0","10.1109/IROS.2018.8593580","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8593580","","Task analysis;Real-time systems;Head;Cameras;Robot vision systems;Videos","control engineering computing;face recognition;gaze tracking;human-robot interaction;learning (artificial intelligence)","single 2D image;human gaze predictions;human-robot interaction tasks;fluent interactions;mutual gaze prediction;gaze heat map statistics;referential gaze;deep learning approach;human gaze fixations","","4","","28","","6 Jan 2019","","","IEEE","IEEE Conferences"
"A preliminary visual system for assistant diagnosis of ASD","Z. Wang; H. Liu","State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China; State Key Laboratory of Mechanical System and Vibration, Shanghai Jiao Tong University, Shanghai, China","2018 25th International Conference on Mechatronics and Machine Vision in Practice (M2VIP)","6 Jan 2019","2018","","","1","5","Autism spectrum disorder(ASD) is a kind of developmental disorder which attracted a lot of attention of researchers for its urgency and pervasiveness. The diagnosis and intervention of ASD is still complicated and hard to handle. The rapid development of technology has brought new methods to the auxiliary diagnosis of ASD, such as face detection, gaze estimation, action recognition, etc. The paper proposed a preliminary visual system for assistant diagnosis of ASD in a core clinical testing scenario-response to name. The eye center localization and gaze estimation were applied to measure the responses of the subject. The purpose of this paper is analyzing the feasibility of this system, and optimizing the sensing structure and the evaluation indicator.","","978-1-5386-7544-1","10.1109/M2VIP.2018.8600906","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8600906","ASD;Response to name;gaze estimation","Estimation;Face;Autism;Toy manufacturing industry;Cameras","diseases;eye;human-robot interaction;medical disorders;patient diagnosis","core clinical testing scenario-response;gaze estimation;preliminary visual system;assistant diagnosis;auxiliary diagnosis;autism spectrum disorder","","","","13","","6 Jan 2019","","","IEEE","IEEE Conferences"
"Discriminative Robust Gaze Estimation Using Kernel-DMCCA Fusion","S. Rabba; M. Kyan; L. Gao; A. Quddus; A. S. Zandi; L. Guan","Ryerson Univ., Toronto, ON, Canada; York Univ., Toronto, ON, Canada; Ryerson Univ., Toronto, ON, Canada; Alcohol Countermeasure Syst., Toronto, ON, Canada; Alcohol Countermeasure Syst., Toronto, ON, Canada; Alcohol Countermeasure Syst., Toronto, ON, Canada","2018 IEEE International Symposium on Multimedia (ISM)","6 Jan 2019","2018","","","291","298","The proposed framework employs discriminative analysis for gaze estimation using kernel discriminative multiple canonical correlation analysis (K-DMCCA), which represents different feature vectors that account for variations of head pose, illumination and occlusion. The feature extraction component of the framework includes spatial indexing, statistical and geometrical elements. Gaze estimation is constructed by feature aggregation and transforming features into a higher dimensional space using the RBF kernel γ and spread factor. The output of fused features through K-DMCCA is robust to illumination, occlusion and is calibration free. Our algorithm is validated on MPII, CAVE, ACS and EYEDIAP datasets. The two main contributions of the framework are the following: Enhancing the performance of DMCCA with the kernel and introducing quadtree as an iris region descriptor. Spatial indexing using quadtree is a robust method for detecting which quadrant the iris is situated, detecting the iris boundary and it is inclusive of statistical and geometrical indexing that are calibration free. Our method achieved an accurate gaze estimation of 4.8° using Cave, 4.6° using MPII, 5.1° using ACS and 5.9° using EYEDIAP datasets respectively. The proposed framework provides insight into the methodology of multi-feature fusion for gaze estimation.","","978-1-5386-6857-3","10.1109/ISM.2018.00064","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8603305","Gaze;head-pose;pupil;quadtree;Kernel-DMCCA","Estimation;Feature extraction;Iris;Head;Three-dimensional displays;Indexing;Iris recognition","correlation methods;feature extraction;image fusion;quadtrees","spread factor;quadtree;RBF kernel;feature vectors;discriminative analysis;Kernel-DMCCA fusion;discriminative robust gaze estimation;multifeature fusion;geometrical indexing;statistical indexing;robust method;EYEDIAP datasets;fused features;higher dimensional space;transforming features;feature aggregation;geometrical elements;statistical elements;spatial indexing;feature extraction component;illumination;K-DMCCA;kernel discriminative multiple canonical correlation analysis","","1","","37","","6 Jan 2019","","","IEEE","IEEE Conferences"
"Deep Learning Based Object Shape Identification from EOG Controlled Vision System","R. Roy; A. Kumar; M. Mahadevappa; C. S. Kumar",Advanced Technology Development Centre; Geology and Geophysics; School of Medical Science & Technology; Mechanical Engineering,"2018 IEEE SENSORS","27 Dec 2018","2018","","","1","4","Humans look at the object of interest prior to grasping it. This phenomenon leads to necessity of artificial vision system integrated with prosthetic hand for object shape identification. Individuals who use prosthetic arms are able to move their eyes to locate objects. The eye movements can easily be interpreted by Electrooculography (EOG) signal. In this study, a single webcam placed in a cap visor, synchronised to move in the same direction as per the user's eye gaze angle. The camera clicked the snapshot including the object immediately fixing the eye over the object. To recognise the object shape, Convolutional Neural Network (CNN) was used further. Using VGG16 architecture, objects belonging to whole hand grasps were classified into cylindrical, spherical, cubical and conical types to adhere to most of the daily used objects. Around 93.0% accuracy was achieved with the realtime objects or object views that were not included in the training set.","2168-9229","978-1-5386-4707-3","10.1109/ICSENS.2018.8589722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8589722","","Electrooculography;Shape;Webcams;Machine vision;Prosthetic hand","cameras;computer vision;convolutional neural nets;electro-oculography;eye;image classification;learning (artificial intelligence);manipulators;object detection;prosthetics","electrooculography signal;deep learning based object shape identification;convolutional neural network;CNN;VGG16 architecture;hand grasps;eye movements;prosthetic arms;prosthetic hand;artificial vision system;EOG controlled vision system;object shape identification;object views","","1","","12","","27 Dec 2018","","","IEEE","IEEE Conferences"
"Fine-Grained Head Pose Estimation Without Keypoints","N. Ruiz; E. Chong; J. M. Rehg","Georgia Inst. of Technol., Atlanta, GA, USA; Georgia Inst. of Technol., Atlanta, GA, USA; Georgia Inst. of Technol., Atlanta, GA, USA","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","2155","215509","Estimating the head pose of a person is a crucial problem that has a large amount of applications such as aiding in gaze estimation, modeling attention, fitting 3D models to video and performing face alignment. Traditionally head pose is computed by estimating some keypoints from the target face and solving the 2D to 3D correspondence problem with a mean human head model. We argue that this is a fragile method because it relies entirely on landmark detection performance, the extraneous head model and an ad-hoc fitting step. We present an elegant and robust way to determine pose by training a multi-loss convolutional neural network on 300W-LP, a large synthetically expanded dataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from image intensities through joint binned pose classification and regression. We present empirical tests on common in-the-wild pose benchmark datasets which show state-of-the-art results. Additionally we test our method on a dataset usually used for pose estimation using depth and start to close the gap with state-of-the-art depth pose methods. We open-source our training and testing code as well as release our pre-trained models.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00281","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575451","","Three-dimensional displays;Pose estimation;Face;Solid modeling;Computational modeling;Training","convolutional neural nets;face recognition;image classification;learning (artificial intelligence);pose estimation;regression analysis;solid modelling","landmark detection performance;extraneous head model;ad-hoc fitting step;multiloss convolutional neural network;gaze estimation;modeling attention;target face;mean human head model;pretrained models;face alignment;fine-grained head pose estimation;3D-to-3D correspondence problem;3D model fitting;common in-the-wild pose benchmark datasets;intrinsic Euler angle prediction;joint binned pose classification;joint binned pose regression","","69","","36","","16 Dec 2018","","","IEEE","IEEE Conferences"
"Light-Weight Head Pose Invariant Gaze Tracking","R. Ranjan; S. De Mello; J. Kautz","Univ. of Maryland, College Park, MD, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","16 Dec 2018","2018","","","2237","22378","Unconstrained remote gaze tracking using off-the-shelf cameras is a challenging problem. Recently, promising algorithms for appearance-based gaze estimation using convolutional neural networks (CNN) have been proposed. Improving their robustness to various confounding factors including variable head pose, subject identity, illumination and image quality remain open problems. In this work, we study the effect of variable head pose on machine learning regressors trained to estimate gaze direction. We propose a novel branched CNN architecture that improves the robustness of gaze classifiers to variable head pose, without increasing computational cost. We also present various procedures to effectively train our gaze network including transfer learning from the more closely related task of object viewpoint estimation and from a large high-fidelity synthetic gaze dataset, which enable our ten times faster gaze network to achieve competitive accuracy to its current state-of-the-art direct competitor.","2160-7516","978-1-5386-6100-0","10.1109/CVPRW.2018.00290","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575461","","Head;Magnetic heads;Estimation;Gaze tracking;Cameras;Training;Iris","cameras;gaze tracking;image classification;learning (artificial intelligence);neural nets;pose estimation;regression analysis","transfer learning;gaze network;machine learning regressors;variable head pose;branched CNN architecture;direct competitor;light-weight head pose invariant gaze tracking;high-fidelity synthetic gaze dataset;object viewpoint estimation;gaze classifiers;gaze direction;image quality;illumination;variable head;convolutional neural networks;appearance-based gaze estimation;off-the-shelf cameras;unconstrained remote gaze tracking","","11","","27","","16 Dec 2018","","","IEEE","IEEE Conferences"
"A Hierarchical Generative Model for Eye Image Synthesis and Eye Gaze Estimation","K. Wang; R. Zhao; Q. Ji","ECSE, Rensselaer Polytech. Inst., Troy, NY, USA; ECSE, Rensselaer Polytech. Inst., Troy, NY, USA; ECSE, Rensselaer Polytech. Inst., Troy, NY, USA","2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","16 Dec 2018","2018","","","440","448","In this work, we introduce a Hierarchical Generative Model (HGM) to enable realistic forward eye image synthesis, as well as effective backward eye gaze estimation. The proposed HGM consists of a hierarchical generative shape model (HGSM), and a conditional bidirectional generative adversarial network (c-BiGAN). The HGSM encodes eye geometry knowledge and relates eye gaze with eye shape, while c-BiGAN leverages on big data and captures the dependency between eye shape and eye appearance. As an intermediate component, eye shape connects knowledge-based model (HGSM) with data-driven model (c-BiGAN) and enables bidirectional inference. Through a top-down inference, the HGM can synthesize eye images consistent with the given eye gaze. Through a bottom-up inference, HGM can infer eye gaze effectively from a given eye image. Qualitative and quantitative evaluations on benchmark datasets demonstrate our model's effectiveness on both eye image synthesis and eye gaze estimation. In addition, the proposed model is not restricted to eye images only. It can be adapted to face images and any shape-appearance related fields.","2575-7075","978-1-5386-6420-9","10.1109/CVPR.2018.00053","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8578151","","Computer vision;Pattern recognition","Big Data;computer vision;eye;gaze tracking;inference mechanisms","HGM;realistic forward eye image synthesis;effective backward eye gaze estimation;hierarchical generative shape model;HGSM;conditional bidirectional generative adversarial network;eye geometry knowledge;eye shape;eye appearance;knowledge-based model;data-driven model;eye images;shape-appearance related fields;hierarchical generative model;bidirectional inference","","14","","43","","16 Dec 2018","","","IEEE","IEEE Conferences"
"Eye Gaze Correction Using Generative Adversarial Networks","T. Yamamoto; M. Seo; T. Kitajima; Y. Chen","Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan; Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan; Sumsung RD Institute Japan, Osaka, Japan; Graduate School of Information Science and Eng., Ritsumeikan University, Shiga, Japan","2018 IEEE 7th Global Conference on Consumer Electronics (GCCE)","13 Dec 2018","2018","","","276","277","Eye gaze correction is an important topic in video teleconference and video chart in order to keep the eye contact. In this paper, we propose to use a generative adversarial networks for eye gaze correction. We use pairs of front facial image (idea camera setting) and real facial image (real camera setting) to training the network. By using the trained network, we can generate a gaze corrected facial image (front facial image) for any real facial image. Experiments demonstrated the effectiveness of our proposed method.","2378-8143","978-1-5386-6309-7","10.1109/GCCE.2018.8574844","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574844","deep learning;image-to-image translation;gaze correction;Generative Adversarial Net(GAN);Conditional GAN","Cameras;Gallium nitride;Generative adversarial networks;Image generation;Training;Generators;Task analysis","cameras;face recognition;gaze tracking;teleconferencing;video communication","eye gaze correction;generative adversarial networks;video teleconference;video chart;eye contact;front facial image;trained network;rear facial image;real camera setting;idea camera setting","","1","","13","","13 Dec 2018","","","IEEE","IEEE Conferences"
"Probabilistic Estimation of the Gaze Region of the Driver using Dense Classification","S. Jha; C. Busso","Multimodal Signal Processing Lab, University of Texas, Dallas, Richardson, TX, 75080, USA; Multimodal Signal Processing Lab, University of Texas, Dallas, Richardson, TX, 75080, USA","2018 21st International Conference on Intelligent Transportation Systems (ITSC)","9 Dec 2018","2018","","","697","702","The ability to monitor the visual attention of a driver is a useful feature for smart vehicles to understand the driver's intents and behaviors. The gaze angle of the driver is not deterministically related to his/her head pose due to the interplay between head and eye movements. Therefore, this study aims to establish a probabilistic relationship using deep learning. While probabilistic regression techniques such as Gaussian process regression (GPR) has been previously used to predict the visual attention of a driver, the proposed deep learning framework is a more generic approach that does not make assumptions, learning the relationship between gaze and head pose from the data. In our formulation, the continuous gaze angles are converted into intervals and the grid of the quantized angles is treated as an image for dense prediction. We rely on convolutional neural networks (CNNs) with upsampling to map the six degrees of freedom of the orientation and position of the head into gaze angles. We train and evaluate the proposed network with data collected from drivers who were asked to look at predetermined locations inside a car during naturalistic driving recordings. The proposed model obtains very promising results, where the size of the gaze region with 95% accuracy is only 11.73% of a half sphere centered at the driver, which approximates his/her field of view. The architecture offers an appealing and general solution to convert regression problems into dense classification problems.","2153-0017","978-1-7281-0323-5","10.1109/ITSC.2018.8569709","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8569709","","Vehicles;Head;Visualization;Task analysis;Training;Probabilistic logic","convolution;driver information systems;feedforward neural nets;Gaussian processes;gaze tracking;learning (artificial intelligence);pattern classification;regression analysis","probabilistic estimation;gaze region;visual attention;probabilistic relationship;probabilistic regression techniques;Gaussian process regression;deep learning framework;continuous gaze angles;driver intent;dense classification;smart vehicles;driver behaviour;convolutional neural networks","","5","","30","","9 Dec 2018","","","IEEE","IEEE Conferences"
"A Rapid Webcam-Based Eye Tracking Method for Human Computer Interaction","C. Zheng; T. Usagawa","Kumamoto University, Graduate School of Science and Technology, Kumamoto, Japan; Kumamoto University, Graduate School of Science and Technology, Kumamoto, Japan","2018 International Conference on Control, Automation and Information Sciences (ICCAIS)","9 Dec 2018","2018","","","133","136","This study proposes a rapid eye tracking method, to respond to a situation that require a high processing speed but less accuracy. Unlike other studies, this study uses a webcam with a low resolution of 640 × 480, which decreased the cost of devices considerably. We also developed the corresponding algorithm to suit the low-quality image. We use an efficient algorithm to detect the pupils which is based on color intensity change to decrease the calculation load. The processing speed exceeds the requirement of eye tracking for saccade eyeball movement. The result of experiment shows that the proposed method is a fast and low-cost method for eye tracking.","2475-7896","978-1-5386-6020-1","10.1109/ICCAIS.2018.8570532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8570532","eye tracking;gaze estimation;webcam;OpenCV","Gaze tracking;Feature extraction;Image color analysis;Face detection;Face;Webcams","eye;gaze tracking;human computer interaction;object detection","rapid webcam-based eye tracking method;human computer interaction;high processing speed;low-quality image;saccade eyeball movement","","3","","7","","9 Dec 2018","","","IEEE","IEEE Conferences"
"Automatic Eye Gaze Estimation using Geometric & Texture-based Networks","S. Jyoti; A. Dhall","Learning Affect and SemantIc Imaging (LASII) Group, Indian Institute of Technology Ropar, India; Learning Affect and SemantIc Imaging (LASII) Group, Indian Institute of Technology Ropar, India","2018 24th International Conference on Pattern Recognition (ICPR)","29 Nov 2018","2018","","","2474","2479","Eye gaze estimation is an important problem in automatic human behavior understanding. This paper proposes a deep learning based method for inferring the eye gaze direction. The method is based on the use of ensemble of networks, which capture both the geometric and texture information. Firstly, a Deep Neural Network (DNN) is trained using the geometric features that are extracted from the facial landmark locations. Secondly, for the texture based features, three Convolutional Neural Networks (CNN) are trained i.e. for the patch around the left eye, right eye, and the combined eyes, respectively. Finally, the information from the four channels is fused with concatenation and dense layers are trained to predict the final eye gaze. The experiments are performed on the two publicly available datasets: Columbia eye gaze and TabletGaze. The extensive evaluation shows the superior performance of the proposed framework. We also evaluate the performance of the recently proposed swish activation function as compared to Rectified Linear Unit (ReLU) for eye gaze estimation.","1051-4651","978-1-5386-3788-3","10.1109/ICPR.2018.8545162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8545162","","Estimation;Feature extraction;Face;Neurons;Task analysis;Image resolution","convolution;estimation theory;face recognition;feature extraction;feedforward neural nets;gaze tracking;geometry;image texture;inference mechanisms;learning (artificial intelligence)","geometric information;deep neural network;convolutional neural networks;texture information;eye gaze direction inference;facial landmark locations;automatic human behavior understanding;automatic eye gaze estimation;texture based features;geometric features;deep learning based method","","1","","27","","29 Nov 2018","","","IEEE","IEEE Conferences"
"Gaze-Aided Eye Detection via Appearance Learning","L. Cao; C. Gou; K. Wang; G. Xiong; F. Wang","Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China; Institute of Automation, Chinese Academy of Sciences, Beijing, China","2018 24th International Conference on Pattern Recognition (ICPR)","29 Nov 2018","2018","","","1965","1970","Image based eye detection and gaze estimation have a wide range of potential applications, such as medical treatment, biometrics recognition, human-computer interaction. Though a large number of researchers have attempted to solve the two problems, they still exist some challenges due to the variation in appearance and lack of annotated images. In addition, most related work perform eye detection first, followed by gaze estimation via appearance learning. In this paper, we propose a unified framework to execute the gaze estimation and the eye detection simultaneously by learning the cascade regression models from appearance around the eye related key points. Intuitively, there is coupled relationship among location of eye center, shape of eye related key points, appearance representation and gaze information. To incorporate these information, at each cascade level, we first learn a model to map the shape and appearance around current eye related key points to the three dimension gaze update. Then, with the help of estimated gaze, we further learn a regression model to map the gaze, shape and appearance information to eye location update. By leveraging the power of cascade learning, the proposed method can alternatively optimize the two tasks of eye detection and gaze estimation. The experiments are conducted on benchmarks of GI4E and MPIIGaze. Experimental results show that our proposed method can achieve preferable results in gaze estimation and outperform the state-of-the-art methods in eye detection.","1051-4651","978-1-5386-3788-3","10.1109/ICPR.2018.8545635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8545635","","Estimation;Shape;Feature extraction;Lighting;Training;Solid modeling;Linear regression","computer vision;face recognition;feature extraction;gaze tracking;learning (artificial intelligence);object detection;regression analysis","cascade learning;eye location update;dimension gaze update;current eye;gaze information;appearance representation;eye related key points;eye center;image based eye detection;appearance learning;gaze-aided eye detection;gaze estimation","","2","","22","","29 Nov 2018","","","IEEE","IEEE Conferences"
"Eye Gazing Enabled Driving Behavior Monitoring and Prediction","X. Fan; F. Wang; Y. Lu; D. Song; J. Liu","School of Computing Science, Simon Fraser University, Canada; Department of Computer and Information Science, The University of Mississippi, USA; School of Computing Science, Simon Fraser University, Canada; School of Computing Science, Simon Fraser University, Canada; School of Computing Science, Simon Fraser University, Canada","2018 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","29 Nov 2018","2018","","","1","4","Automobiles have become one of the necessities of modern life, but also introduced numerous traffic accidents that threaten drivers and other road users. Most state-of-the-art safety systems are passively triggered, reacting to dangerous road conditions or driving behaviors only after they happen and are observed, which greatly limits the last chances for collision avoidances. Therefore, timely tracking and predicting the driving behaviors calls for a more direct interface beyond the traditional steering wheel/brake/gas pedal. In this paper, we argue that a driver's eyes are the interface, as it is the first and the essential window that gathers external information during driving. Our experiments suggest that a driver's gaze patterns appear prior to and correlate with the driving behaviors for driving behavior prediction. We accordingly propose GazMon, an active driving behavior monitoring and prediction framework for driving assistance applications. GazMon extracts the gaze information through a front-camera and analyzes the facial features, including facial landmarks, head pose, and iris centers, through a carefully constructed deep learning architecture. Our on-road experiments demonstrate the superiority of our GazMon on predicting driving behaviors. It is also readily deployable using RGB cameras and allows reuse of existing smartphones towards more safely driving.","","978-1-5386-4195-8","10.1109/ICMEW.2018.8551544","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8551544","Gaze;Driving Assistant;Mobile Computing;Deep Learning","Smart phones;Wheels;Vehicles;Roads;Cameras;Monitoring","automobiles;behavioural sciences computing;cameras;collision avoidance;driver information systems;face recognition;gaze tracking;learning (artificial intelligence);road accidents;road safety;road traffic;road vehicles;smart phones;steering systems;user interfaces","eye gazing;dangerous road conditions;driving behavior prediction;driving assistance applications;steering wheel;automobiles;traffic accidents;collision avoidances;drivers gaze patterns;facial features;head pose;iris centers;deep learning architecture;RGB cameras;smartphones","","1","","4","","29 Nov 2018","","","IEEE","IEEE Conferences"
"Image Purification through Controllable Neural Style Transfer","T. Zhao; Y. Yan; I. S. Shehu; H. Wei; X. Fu","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","2018 International Conference on Information and Communication Technology Convergence (ICTC)","18 Nov 2018","2018","","","466","471","Recently, progress in learning-by-synthesis proposed training models on synthetic images that can effectively reduce the cost of human and material resources. However, learning from synthetic images still cannot achieve the desired performance due to the different distribution of synthetic images compared to naturalistic images. Naturalistic images are composed of multiform light distribution, which is a characteristic of the outdoor scene. In an attempt to address this issue, previous methods learn a model to improve the realism of synthetic images. Differently from previous methods, this paper takes the first step towards purifying the naturalistic image to weaken the influence of light and convert the distribution of outdoor naturalistic image through a style transfer task to that of indoor synthetic image. This paper proposes therefore, a controlled neural style transfer network to preserve image structure, accelerate model convergence rate and adapt to multi-scale images. A mixed research approach (qualitative and quantitative) was adopted for the experiments carried out to demonstrate the possibility of purifying naturalistic images of complex distribution. Qualitatively, it compares the proposed method with baseline methods across several indoor and outdoor scenes of the LPW dataset. While quantitatively it evaluates the purified images by training models for gaze estimation on cross-dataset. Results show a significant improvement over using raw naturalistic images and when compared with the baseline methods.","2162-1233","978-1-5386-5041-7","10.1109/ICTC.2018.8539637","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8539637","Gaze estimation;Style Transfer;Image Purification;Learning-by-synthesis;Cross-subject.","Semantics;Estimation;Training;Image color analysis;Image segmentation;Iris;Information science","image denoising;image resolution;image restoration;image segmentation;learning (artificial intelligence);neural nets","image purification;training models;synthetic images;outdoor naturalistic image;indoor synthetic image;controlled neural style transfer network;image structure;multiscale images;purified images;raw naturalistic images","","","","22","","18 Nov 2018","","","IEEE","IEEE Conferences"
"Affective Computational Model to Extract Natural Affective States of Students With Asperger Syndrome (AS) in Computer-Based Learning Environment","A. Dawood; S. Turner; P. Perepa","Department of Computing, Faculty of Art, Science, and Technology, University of Northampton, Northampton, U.K.; Department of Computing, Faculty of Art, Science, and Technology, University of Northampton, Northampton, U.K.; Department of Special Education Needs and Inclusion, Faculty of Education and Humanities, University of Northampton, Northampton, U.K.","IEEE Access","28 Nov 2018","2018","6","","67026","67034","This paper was inspired by looking at the central role of emotion in the learning process, its impact on students’ performance; as well as the lack of affective computing models to detect and infer affective-cognitive states in real time for students with and without Asperger Syndrome (AS). This model overcomes gaps in other models that were designed for people with autism, which needed the use of sensors or physiological instrumentations to collect data. The model uses a webcam to capture students’ affective-cognitive states of confidence, uncertainty, engagement, anxiety, and boredom. These states have a dominant effect on the learning process. The model was trained and tested on a natural-spontaneous affective dataset for students with and without AS, which was collected for this purpose. The dataset was collected in an uncontrolled environment and included variations in culture, ethnicity, gender, facial and hairstyle, head movement, talking, glasses, illumination changes, and background variation. The model structure used deep learning (DL) techniques like convolutional neural network and long short-term memory. The DL is the-state-of-art tool that used to reduce data dimensionality and capturing non-linear complex features from simpler representations. The affective model provides reliable results with accuracy 90.06%. This model is the first model to detected affective states for adult students with AS without physiological or wearable instruments. For the first time, the occlusions in this model, like hand over face or head were considered an important indicator for affective states like boredom, anxiety, and uncertainty. These occlusions have been ignored in most other affective models. The essential information channels in this model are facial expressions, head movement, and eye gaze. The model can serve as an aided-technology for tutors to monitor and detect the behaviors of all students at the same time and help in predicting negative affective states during learning process.","2169-3536","","10.1109/ACCESS.2018.2879619","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8522016","Affective model;affective-cognitive states;autism;Asperger Syndrome;AS;CNN;deep learning;LSTM","Autism;Computational modeling;Instruments;Tools;Face recognition;Physiology","","","","2","","67","CCBY","4 Nov 2018","","","IEEE","IEEE Journals"
"Eye Tracking in Augmented Spaces: A Deep Learning Approach","J. Lemley; A. Kar; P. Corcoran","NUI Galway College of Engineering & Informatics, Center for Cognitive Connected & Computational Imaging, Galway, Ireland; NUI Galway College of Engineering & Informatics, Center for Cognitive Connected & Computational Imaging, Galway, Ireland; NUI Galway College of Engineering & Informatics, Center for Cognitive Connected & Computational Imaging, Galway, Ireland","2018 IEEE Games, Entertainment, Media Conference (GEM)","1 Nov 2018","2018","","","1","6","The use of deep learning for estimating eye gaze in augmented spaces is investigated in this work. There are two primary ways of interacting with augmented spaces. The first involves the use of AR/VR systems; the second involves devices that respond to the user's gaze directly. This domain can overlap with AR/VR environments but is not exclusive to them and contains its own unique set of issues. Deep learning methods for eye tracking that are capable of performing with minimal power consumption are investigated for both problems.","","978-1-5386-6304-2","10.1109/GEM.2018.8516529","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516529","Augmented reality;Virtual reality;gaze estimation;deep learning;convolutional neural networks;smart spaces","Gaze tracking;Estimation;Cameras;Head;Image resolution;Tracking","augmented reality;gaze tracking;learning (artificial intelligence)","eye tracking;augmented spaces;deep learning approach;eye gaze estimation;AR-VR systems;power consumption","","2","","39","","1 Nov 2018","","","IEEE","IEEE Conferences"
"Gaze Visual - A Graphical Software Tool for Performance Evaluation of Eye Gaze Estimation Systems","A. Kar; P. Corcoran","College of Engineering & Informatics NUI Galway, Center for Cognitive Connected & Computational Imaging, Galway, Ireland; College of Engineering & Informatics NUI Galway, Center for Cognitive Connected & Computational Imaging, Galway, Ireland","2018 IEEE Games, Entertainment, Media Conference (GEM)","1 Nov 2018","2018","","","1","9","The concept of an open source software developed for all round performance evaluation of gaze tracking systems is presented. The capabilities of this software towards quantitative, statistical and visual analysis of gaze data are discussed. Potential utilities of this software are towards understanding a gaze tracker's behavior, gaze data quality, and improving usability of gaze based applications that are currently very popular in augmented/virtual reality, gaming and multimedia domains.","","978-1-5386-6304-2","10.1109/GEM.2018.8516487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8516487","Performance evaluation;eye gaze tracking;user interface;accuracy metrics;visualizations;open source","Software;Gaze tracking;Data visualization;Performance evaluation;Visualization;Data analysis;Games","data analysis;gaze tracking;public domain software;software tools;statistical analysis;virtual reality","visual analysis;gaze data;potential utilities;gaze tracker;data quality;gaze based applications;gaze visual;graphical software tool;performance evaluation;eye gaze estimation systems;open source software;gaze tracking systems;quantitative analysis;statistical analysis;gaze trackers behavior;gaze data quality;augmented-virtual reality;multimedia domains;gaming domains","","1","","20","","1 Nov 2018","","","IEEE","IEEE Conferences"
"SDM-Based Means of Gradient for Eye Center Localization","Y. Xia; J. Lou; J. Dong; G. Li; H. Yu","Sch. of Creative Technol., Univ. of Portsmouth, Portsmouth, UK; Sch. of Creative Technol., Univ. of Portsmouth, Portsmouth, UK; Dept. of Comput. Sci. & Technol., Ocean Univ. of China, Qingdao, China; Key Lab. of Metall. Equip. & Control Technol., Wuhan Univ. of Sci. & Technol., Wuhan, China; Sch. of Creative Technol., Univ. of Portsmouth, Portsmouth, UK","2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)","28 Oct 2018","2018","","","862","867","For eye gaze estimation and eye tracking, localizing eye center is a crucial requirement. This task is challenging work because of the significant variability of eye appearance in illumination, shape, color and viewing angles. In this paper, we improve the performance of means of gradient method in low resolution images, which could locate the eye center more accurately. The proposed method applies Supervised Descent Method (SDM), which has remarkable achievement in the field of face alignment, to improve the traditional means of gradient method in localizing eye center. We extensively evaluate our method on BioID database which is very challenging and realistic for eye center localization. Moreover, we have compared our method with existing state of the art methods and the results of the experiment confirm that the proposed method is an attractive alternative for eye center localization.","","978-1-5386-7518-2","10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00-17","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8511989","Eye Gaze Estimation;Eye Center Localization;Means of Gradient.","Gradient methods;Face;Estimation;Lighting;Webcams;Measurement;Shape","eye;face recognition;gaze tracking;gradient methods;image resolution","BioID database;face alignment;SDM;supervised descent method;low resolution images;gradient method;viewing angles;eye appearance;eye tracking;eye gaze estimation;eye center localization","","1","","16","","28 Oct 2018","","","IEEE","IEEE Conferences"
"Image Purification Networks: Real-time Style Transfer with Semantics through Feed-forward Synthesis","T. Zhao; Y. Yan; I. S. Shehu; X. Fu","Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian, 116026, China","2018 International Joint Conference on Neural Networks (IJCNN)","14 Oct 2018","2018","","","1","7","Image synthesis has been widely accepted as a cost effective way to learn models because it provides training sets that are large, diverse and accurately labeled. However, the realism of the synthetic image is not enough, this affects generalization on naturalistic test image. In an attempt to address this issue, previous methods learn a model to improve the realism of synthetic image. Differently, from previous methods, we take the first step towards purifying the naturalistic image to weaken the influence of light and convert the distribution of an outdoor naturalistic image through a real-time style transfer task to that of indoor synthetic image. This paper proposes, therefore a real-time image purification networks that transfer style information with semantics through a feed-forward synthesis. Results from our experiments demonstrate that images purified through the proposed networks architecture trained models for gaze estimation more accurately on cross-datasets over using raw naturalistic images and when compared to baseline methods.","2161-4407","978-1-5090-6014-6","10.1109/IJCNN.2018.8489365","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489365","","Image segmentation;Semantics;Feature extraction;Iris;Training;Network architecture;Estimation","estimation theory;feature extraction;feedforward;gaze tracking;image processing;learning (artificial intelligence)","real-time style transfer;semantics;feed-forward synthesis;image synthesis;outdoor naturalistic image;indoor synthetic image;transfer style information;networks architecture trained models;image purification networks;gaze estimation","","","","17","","14 Oct 2018","","","IEEE","IEEE Conferences"
"Towards Gaze-Based Mobile Device Interaction for the Disabled","A. Wachner; J. Edinger; C. Becker","University of Mannheim, Schloss, Mannheim, 68161, Germany; University of Mannheim, Schloss, Mannheim, 68161, Germany; University of Mannheim, Schloss, Mannheim, 68161, Germany","2018 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)","7 Oct 2018","2018","","","397","402","Manual input is the dominant way to interact with mobile devices in everyday life. Innovations like multi-touch displays and virtual keyboards that allow user input by sliding a finger over the letters make this input more and more convenient. However, these systems assume that users have full control over their hand movements. This assumption excludes millions of people who suffer from loss of extremities, paralyses, or spasticities. In this paper, we motivate the urgent need for input mechanisms that make these technologies available for everybody. We focus on gaze-based interaction as eye movements can be tracked by built-in cameras of unmodified mobile devices without any further hardware requirements. As accurate point of gaze estimation on mobile devices is nontrivial, we propose a middleware that is not based on the absolute position of the pupils but rather takes eye movement patterns into account. This paper has four contributions. First, we motivate the need for alternative inputs methods besides manual input. Second, we discuss existing approaches and reveal their limitations. Third, we propose a middleware that allows gaze-based user interaction with mobile devices. Fourth, we provide a framework that allows developers to easily integrate gaze-based control into mobile applications.","","978-1-5386-3227-7","10.1109/PERCOMW.2018.8480159","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8480159","","Mobile handsets;Performance evaluation;Cameras;Manuals;Market research;Gaze tracking;Privacy","eye;handicapped aids;human computer interaction;middleware;mobile computing;tracking;user interfaces","gaze-based mobile device interaction;multitouch displays;user input;gaze-based interaction;eye movements;unmodified mobile devices;gaze estimation;eye movement patterns;alternative inputs methods;gaze-based user interaction;gaze-based control;mobile applications;disabled;middleware","","","","23","","7 Oct 2018","","","IEEE","IEEE Conferences"
"Real-time 3D Glint Detection in Remote Eye Tracking Based on Bayesian Inference","D. Geisler; D. Fox; E. Kasneci","Department of Perception Engineering, University of Tuebingen, Germany; Department of Computer Science & Engineering, University of Washington, Seattle, USA; Department of Perception Engineering, University of Tuebingen, Germany","2018 IEEE International Conference on Robotics and Automation (ICRA)","13 Sep 2018","2018","","","7119","7126","As human gaze provides information on our cognitive states, actions, and intentions, gaze-based interaction has the potential to enable a fluent and natural human-robot collaboration. In this work, we focus on reliable gaze estimation in remote eye tracking based on calibration-free methods. Although these methods work well in controlled settings, they fail when illumination conditions change or other objects induce noise. We propose a novel, adaptive method based on a probabilistic model, which reliably detects glints from stereo images and evaluate our method using a data set that contains different challenges with regarding to light and reflections.","2577-087X","978-1-5386-3081-5","10.1109/ICRA.2018.8460800","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460800","","Cameras;Gaze tracking;Feature extraction;Three-dimensional displays;Probabilistic logic;Solid modeling;Calibration","Bayes methods;gaze tracking;human-robot interaction;object detection;stereo image processing","remote eye tracking;Bayesian inference;human gaze;cognitive states;gaze-based interaction;human-robot collaboration;gaze estimation;3D glint detection","","1","","36","","13 Sep 2018","","","IEEE","IEEE Conferences"
"Making Third Person Techniques Recognize First-Person Actions in Egocentric Videos","S. Verma; P. Nagar; D. Gupta; C. Arora","IIIT, Delhi; IIIT, Delhi; IIIT, Delhi; IIIT, Delhi","2018 25th IEEE International Conference on Image Processing (ICIP)","6 Sep 2018","2018","","","2301","2305","We focus on first-person action recognition from egocentric videos. Unlike third person domain, researchers have divided first-person actions into two categories: involving hand-object interactions and the ones without, and developed separate techniques for the two action categories. Further, it has been argued that traditional cues used for third person action recognition do not suffice, and egocentric specific features, such as head motion and handled objects have been used for such actions. Unlike the state-of-the-art approaches, we show that a regular two stream Convolutional Neural Network (CNN) with Long Short-Term Memory (LSTM) architecture, having separate streams for objects and motion, can generalize to all categories of first-person actions. The proposed approach unifies the feature learned by all action categories, making the proposed architecture much more practical. In an important observation, we note that the size of the objects visible in the egocentric videos is much smaller. We show that the performance of the proposed model improves after cropping and resizing frames to make the size of objects comparable to the size of ImageNet's objects. Our experiments on the standard datasets: GTEA, EGTEA Gaze+, HUJI, ADL, UTE, and Kitchen, proves that our model significantly outperforms various state-of-the-art techniques.","2381-8549","978-1-4799-7061-2","10.1109/ICIP.2018.8451249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451249","Egocentric Videos;First-Person Action Recognition;Deep Learning","Videos;Cameras;Streaming media;Optical imaging;Training;Head;Adaptation models","feature extraction;feedforward neural nets;image motion analysis;image recognition;object detection;video signal processing","egocentric videos;first-person action recognition;hand-object interactions;egocentric specific features;third person action recognition;head motion;handled objects;convolutional neural network;long short-term memory architecture;ImageNet objects;GTEA datasets;EGTEA Gaze+ datasets;HUJI datasets;ADL datasets;UTE datasets;Kitchen datasets","","1","","28","","6 Sep 2018","","","IEEE","IEEE Conferences"
"Multiple RGB-D Camera-based User Intent Position and Object Estimation","K. HoonKwon; H. Oh; M. Y. Kim","School of Electronics Engineering, IT College, Kyungpook National University, 80 Daehak-ro, Buk-gu, Daegu, 41566, Korea; School of Electronics Engineering, IT College, Kyungpook National University, 80 Daehak-ro, Buk-gu, Daegu, 41566, Korea; School of Electronics Engineering, IT College, Kyungpook National University, Research Center for Neurosurgical Robotic System, Kyungpook National University, 80 Daehak-ro, Buk-gu, Daegu, 41566, Korea","2018 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM)","2 Sep 2018","2018","","","176","180","Human gaze represents the area of interests of the person. By analyzing a time-series of these areas, it is possible to obtain user behavioral pattern that can be used in various fields. Well-known techniques for estimating human gaze are inconvenient because they require a wearable device, or the measurement area is relatively narrow. In this paper, a method to implement gaze estimation system using 3D view tracking with multi RGB-D camera is proposed. Surround 3D cameras are used to extract the region of interest of the user from 3D gaze estimation without wearable device in living space. To implement proposed method, first, 3D space mapping through multiple RGB-D camera calibration is performed. The resulting 3D map is the measurement area, which depends on the number and specifications of the RGB-D cameras used for this purpose. Then, when a person enters the 3D map, the face region is detected using both 2D / 3D data, and 3D view tracking is implemented by detecting the gaze vector using the facial feature point and the head data center point extracted from the 3D map. Finally, when the gaze vector line intersects a specific point within the mapping space, the image coordinates corresponding to that point are extracted to implement user Intent position estimation. Applying object detection and classification algorithm to the extracted image can also estimate the intent object at that time.","2159-6255","978-1-5386-1854-7","10.1109/AIM.2018.8452320","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452320","","Three-dimensional displays;Cameras;Estimation;Tracking;Calibration;Face","calibration;cameras;face recognition;feature extraction;gaze tracking;image classification;image colour analysis;image sensors;object detection;pose estimation;stereo image processing;time series","object estimation;human gaze;time-series;user behavioral pattern;wearable device;3D gaze estimation;3D space mapping;multiple RGB-D camera calibration;RGB-D cameras;face region;3D view tracking;facial feature point;head data center point;gaze vector line;user Intent position estimation;object detection;classification algorithm;3D map","","","","9","","2 Sep 2018","","","IEEE","IEEE Conferences"
"Multimodal Computer Vision Framework for Human Assistive Robotics","E. Ivorra; M. Ortega; M. Alcañiz; N. Garcia-Aracil","Institute for Research and Innovation in Bioengineering Universitat Politècnica de València Valencia, Spain; Institute for Research and Innovation in Bioengineering Universitat Politècnica de València Valencia, Spain; Institute for Research and Innovation in Bioengineering Universitat Politècnica de València Valencia, Spain; Biomedical Neuroengineering Group Universidad Miguel Hernandez de Elche Elche, Spain","2018 Workshop on Metrology for Industry 4.0 and IoT","9 Aug 2018","2018","","","1","5","This paper presents a multimodal computer vision framework for human assistive robotics with the purpose of giving accessibility to persons with disabilities. The user is capable of interacting with the system just by staring. Specifically, it is possible to select the desired object as well as to indicate the intention to grasp it just by staring at it. This gaze information is provided by ©Tobii Glasses 2 that in combination with a deep learning algorithm gives the class id of the desirable object. Later, the object's pose is estimated using a RGB-D camera with a new developed technique. This technique mixes a template based algorithm with a deep learning algorithm giving a precise, realtime method for pose estimation. Once the pose is obtained, it is transformed to a grasping position in the coordinate system of the assistive robot that performs the grasping operation.","","978-1-5386-2497-5","10.1109/METROI4.2018.8428330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8428330","Object Pose Detection;Assistive Robotics;Eye Tracking;Deep Learning","","computer vision;handicapped aids;image colour analysis;learning (artificial intelligence);pose estimation;service robots","human assistive robotics;©Tobii Glasses 2;deep learning algorithm;multimodal computer vision framework;RGB-D camera;template based algorithm;pose estimation","","1","","23","","9 Aug 2018","","","IEEE","IEEE Conferences"
"Predicting Human Eye Fixations via an LSTM-Based Saliency Attentive Model","M. Cornia; L. Baraldi; G. Serra; R. Cucchiara","Department of Engineering “Enzo Ferrari,”, University of Modena and Reggio Emilia, Modena, Italy; Department of Engineering “Enzo Ferrari,”, University of Modena and Reggio Emilia, Modena, Italy; Department of Computer Science, Mathematics and Physics, University of Udine, Udine, Italy; Department of Engineering “Enzo Ferrari,”, University of Modena and Reggio Emilia, Modena, Italy","IEEE Transactions on Image Processing","20 Jul 2018","2018","27","10","5142","5154","Data-driven saliency has recently gained a lot of attention thanks to the use of convolutional neural networks for predicting gaze fixations. In this paper, we go beyond standard approaches to saliency prediction, in which gaze maps are computed with a feed-forward network, and present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms. The core of our solution is a convolutional long short-term memory that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map. In addition, to tackle the center bias typical of human eye fixations, our model can learn a set of prior maps generated with Gaussian functions. We show, through an extensive evaluation, that the proposed architecture outperforms the current state-of-the-art on public saliency prediction datasets. We further study the contribution of each key component to demonstrate their robustness on different scenarios.","1941-0042","","10.1109/TIP.2018.2851672","JUMP Project; Emilia-Romagna Region through the POR-FESR 2014-2020 Program; CultMedia Project(grant numbers:CTN02_00015_9852246); Italian Ministry of Education, Universities and Research; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8400593","Saliency;human eye fixations;convolutional neural networks;deep learning","Predictive models;Feature extraction;Computer architecture;Computational modeling;Task analysis;Deep learning;Visualization","eye;Gaussian processes;neural nets;object detection","Gaussian functions;neural attentive mechanisms;saliency maps;feed-forward network;gaze maps;standard approaches;gaze fixations;convolutional neural networks;data-driven saliency;LSTM-based saliency;public saliency prediction datasets;prior maps;human eye fixations;short-term memory","","98","","74","IEEE","29 Jun 2018","","","IEEE","IEEE Journals"
"OpenFace 2.0: Facial Behavior Analysis Toolkit","T. Baltrusaitis; A. Zadeh; Y. C. Lim; L. Morency","Microsoft, Cambridge, UK; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA","2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)","7 Jun 2018","2018","","","59","66","Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace 2.0 - a tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace 2.0 is an extension of OpenFace toolkit and is capable of more accurate facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, unlike a lot of modern approaches or toolkits, OpenFace 2.0 source code for training models and running them is freely available for research purposes.","","978-1-5386-2335-0","10.1109/FG.2018.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8373812","facial behavior analysis;landmark detection;head pose;eye gaze","Tools;Face;Training;Magnetic heads;Estimation;Real-time systems","computer vision;face recognition;gaze tracking;learning (artificial intelligence);object detection;pose estimation","automatic facial behavior analysis;affective computing community;OpenFace toolkit;facial action unit recognition;computer vision algorithms;OpenFace 2.0 source code;machine learning;facial landmark detection;Facial Behavior Analysis Toolkit;head pose estimation;eye-gaze estimation","","135","","85","","7 Jun 2018","","","IEEE","IEEE Conferences"
"Driver Gaze Zone Estimation Using Convolutional Neural Networks: A General Framework and Ablative Analysis","S. Vora; A. Rangesh; M. M. Trivedi","Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, La Jolla, CA, USA; Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, La Jolla, CA, USA; Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, La Jolla, CA, USA","IEEE Transactions on Intelligent Vehicles","24 Aug 2018","2018","3","3","254","265","Driver gaze has been shown to be an excellent surrogate for driver attention in intelligent vehicles. With the recent surge of highly autonomous vehicles, driver gaze can be useful for determining the handoff time to a human driver. While there has been significant improvement in personalized driver gaze zone estimation systems, a generalized system which is invariant to different subjects, perspectives, and scales is still lacking. We take a step toward this generalized system using convolutional neural networks (CNNs). We finetune four popular CNN architectures for this task, and provide extensive comparisons of their outputs. We additionally experiment with different input image patches, and also examine how the image size affects performance. For training and testing the networks, we collect a large naturalistic driving dataset comprising of 11 long drives, driven by ten subjects in two different cars. Our best performing model achieves an accuracy of 95.18% during cross-subject testing, outperforming current state-of-the-art techniques for this task. Finally, we evaluate our best performing model on the publicly available Columbia gaze dataset comprising of images from 56 subjects with varying head pose and gaze directions. Without any training, our model successfully encodes the different gaze directions on this diverse dataset, demonstrating good generalization capabilities.","2379-8904","","10.1109/TIV.2018.2843120","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370646","Computer vision, driver information systems, gaze tracking, image classification, driver assistance system, driver gaze region estimation, driver visual attention, facial feature extraction, gaze estimation, pattern recognition, driver distraction, gaze classification, intelligent systems, on-road study","Estimation;Cameras;Task analysis;Automobiles;Feature extraction;Testing","driver information systems;face recognition;gaze tracking;learning (artificial intelligence);neural nets;pose estimation;road vehicles","naturalistic driving dataset;autonomous vehicles;good generalization capabilities;cross-subject testing;convolutional neural networks;generalized system;personalized driver gaze zone estimation systems;human driver;intelligent vehicles;driver attention","","12","","37","IEEE","1 Jun 2018","","","IEEE","IEEE Journals"
"A High Performance Spelling System based on EEG-EOG Signals With Visual Feedback","M. -H. Lee; J. Williamson; D. -O. Won; S. Fazli; S. -W. Lee","Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; Nazarbayev University, Astana, Kazakhstan; Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea","IEEE Transactions on Neural Systems and Rehabilitation Engineering","6 Jul 2018","2018","26","7","1443","1459","In this paper, we propose a highly accurate and fast spelling system that employs multi-modal electroencephalography-electrooculography (EEG-EOG) signals and visual feedback technology. Over the last 20 years, various types of speller systems have been developed in brain-computer interface and EOG/eye-tracking research; however, these conventional systems have a tradeoff between the spelling accuracy (or decoding) and typing speed. Healthy users and physically challenged participants, in particular, may become exhausted quickly; thus, there is a need for a speller system with fast typing speed while retaining a high level of spelling accuracy. In this paper, we propose the first hybrid speller system that combines EEG and EOG signals with visual feedback technology so that the user and the speller system can act cooperatively for optimal decision-making. The proposed spelling system consists of a classic row-column event-related potential (ERP) speller, an EOG command detector, and visual feedback modules. First, the online ERP speller calculates classification probabilities for all candidate characters from the EEG epochs. Second, characters are sorted by their probability, and the characters with the highest probabilities are highlighted as visual feedback within the row-column spelling layout. Finally, the user can actively select the character as the target by generating an EOG command. The proposed system shows 97.6% spelling accuracy and an information transfer rate of 39.6 (±13.2) [bits/min] across 20 participants. In our extended experiment, we redesigned the visual feedback and minimized the number of channels (four channels) in order to enhance the speller performance and increase usability. Most importantly, a new weighted strategy resulted in 100% accuracy and a 57.8 (±23.6) [bits/min] information transfer rate across six participants. This paper demonstrates that the proposed system can provide a reliable communication channel for practical speller applications and may be used to supplement existing systems.","1558-0210","","10.1109/TNSRE.2018.2839116","Ministry of Science and ICT, South Korea, through the SW Starlab Support Program supervised by the Institute for Information and Communications Technology Promotion(grant numbers:IITP-2015-1107); Korean Government: Development of BCI based Brain and Cognitive Computing Technology for Recognizing User’s Intentions using Deep Learning(grant numbers:2017-0-00451); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8361842","Brain-computer interfaces (BCI);electroencephalography (EEG);electrooculogram (EOG);P300 speller;visual feedback","Electrooculography;Electroencephalography;Visualization;Task analysis;Decoding;Layout","bioelectric potentials;brain-computer interfaces;decision making;electroencephalography;electro-oculography;gaze tracking;medical signal processing;signal classification","typing speed;multimodal electroencephalography-electrooculography signals;brain-computer interface;spelling accuracy;optimal decision-making;classification probabilities;EEG epochs;hybrid speller system;EOG/eye-tracking research;EEG-EOG signals;high performance spelling system;practical speller applications;speller performance;row-column spelling layout;online ERP speller;visual feedback modules;EOG command detector;classic row-column event-related potential speller;visual feedback technology","Adult;Brain-Computer Interfaces;Calibration;Communication Aids for Disabled;Decision Making;Electroencephalography;Electrooculography;Event-Related Potentials, P300;Eye Movements;Feedback, Sensory;Female;Healthy Volunteers;Humans;Male;Reproducibility of Results;Young Adult","40","","72","OAPA","21 May 2018","","","IEEE","IEEE Journals"
"Saliency Prediction for Mobile User Interfaces","P. Gupta; S. Gupta; A. Jayagopal; S. Pal; R. Sinha",NA; NA; NA; NA; NA,"2018 IEEE Winter Conference on Applications of Computer Vision (WACV)","7 May 2018","2018","","","1529","1538","We introduce models for saliency prediction for mobile user interfaces. A mobile interface may include elements like buttons and text in addition to natural images which enable performing a variety of tasks. Saliency in natural images is a well studied topic. However, given the difference in what constitutes a mobile interface, and the usage context of these devices, we postulate that saliency prediction for mobile interface images requires a fresh approach. Mobile interface design involves operating on elements, the building blocks of the interface. We first collected eye-gaze data from mobile devices for a free viewing task. Using this data, we develop a novel autoencoder based multi-scale deep learning model that provides saliency prediction at the mobile interface element level. Compared to saliency prediction approaches developed for natural images, we show that our approach performs significantly better on a range of established metrics.","","978-1-5386-4886-5","10.1109/WACV.2018.00171","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8354275","","Predictive models;Task analysis;Mobile handsets;Feeds;Calibration;Face;Feature extraction","graphical user interfaces;learning (artificial intelligence);mobile computing;object detection","mobile interface design;mobile devices;saliency prediction;mobile interface element level;natural images;mobile user interfaces;mobile interface images;autoencoder based multiscale deep learning model;eye-gaze data;free viewing task","","2","","43","","7 May 2018","","","IEEE","IEEE Conferences"
"Deep neural network ensemble architecture for eye movements classification","M. Arsenovic; S. Sladojevic; D. Stefanovic; A. Anderla","Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia; Department of Industrial Engineering and Management, Faculty of Technical Sciences, Novi Sad, Serbia","2018 17th International Symposium INFOTEH-JAHORINA (INFOTEH)","26 Apr 2018","2018","","","1","4","Up to now, eye tracking technologies have been used for different purposes in various industries, from medical to gaming. Eye tracking methods could include predicting fixations, gaze mapping or movement classification. Recent advances in deep learning techniques provide possibilities for solving many computer vision tasks with high accuracy. Authors of this paper propose a novel deep learning based architecture for eye movement classification task. Proposed architecture is an ensemble approach which employs deep convolutional neural networks that run in parallel, for both eyes separately, for visual feature extractions along with recurrent layers for temporal information gathering. Dataset images for training and validation were gathered from standard web camera and pre-processed automatically using dedicated tools. Overall accuracy of developed classifier on the validation set was 92%. Proposed architecture uses relatively small networks which brings the possibility of real time usage (successfully tested on 15-20fps) on regular CPU. Classifier achieved overall accuracy of 88% on the real-time test, using standard laptop and web camera.","","978-1-5386-4907-7","10.1109/INFOTEH.2018.8345537","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8345537","data deep learning;eye tracking;image classification;time-series prediction;convolutional networks;recurrent networks","Gaze tracking;Computer architecture;Task analysis;Machine learning;Information filters;Visualization","cameras;computer vision;feature extraction;feedforward neural nets;gaze tracking;image classification;time series","visual feature extractions;temporal information gathering;standard web camera;standard laptop;neural network ensemble architecture;eye movements classification;eye tracking technologies;eye tracking methods;gaze mapping;deep learning techniques;computer vision tasks;deep learning based architecture;eye movement classification task;deep convolutional neural networks","","2","","18","","26 Apr 2018","","","IEEE","IEEE Conferences"
"Glimpse-gaze deep vision for Modular Rapidly Deployable Decision Support Agent in smart jungle","M. H. Abbasi; B. Majidi; M. T. Manzuri","Department of Computer Engineering, Khatam University, Tehran, Iran; Department of Computer Engineering, Khatam University, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran","2018 6th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS)","12 Apr 2018","2018","","","75","78","Visual interpretation of complex visual patterns in non-urban environments is necessary for many applications in smart rural community management, smart farming and smart jungles. In this paper, the Glimpse-Gaze framework for deep learning based visual interpretation of complex rural and jungle environment scenes is proposed. The proposed framework is used for decision support and navigation by a multi-agent robotic system singularly referred to as MOdular RApidly Deployable Decision Support Agent (MORAD DSA). A set of deep con-volutional neural networks are trained for fast and accurate interpretation of jungle scenes. Transfer learning and auxiliary pretraining on salient regions of the jungle scenes are investigated and the hyper parameter tuning and data augmentation for avoiding overfitting for the proposed model are explored. The experimental results show that the Glimpse-Gaze framework is capable of generating accurate visual cues for precise navigation and visual interpretation in the unstructured rural and jungle environments. A series of data sets for smart jungle applications are collected and the proposed framework is evaluated for applications such as detection of fire hazards and illegal grazing in the jungle.","","978-1-5386-2836-2","10.1109/CFIS.2018.8336635","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336635","Deep Learning;Convolutional Neural Networks;Robotics;Computer Vision;Intelligent Systems","Robots;Visualization;Roads;Machine vision;Convolutional neural networks;Navigation","belief networks;computer vision;decision support systems;learning (artificial intelligence);multi-agent systems;neural nets;object detection","glimpse-gaze deep vision;MORAD DSA;smart jungle applications;unstructured rural jungle;accurate visual cues;auxiliary pretraining;jungle scenes;accurate interpretation;fast interpretation;deep con-volutional neural networks;multiagent robotic system;navigation;deep learning;Glimpse-Gaze framework;smart farming;smart rural community management;nonurban environments;complex visual patterns;visual interpretation;modular rapidly deployable decision support agent","","15","","8","","12 Apr 2018","","","IEEE","IEEE Conferences"
"Driver's gaze zone estimation by transfer learning","I. R. Tayibnapis; M. Choi; S. Kwon","DGIST, Daegu, Republic of Korea; DGIST, Daegu, Republic of Korea; DGIST, Daegu, Republic of Korea","2018 IEEE International Conference on Consumer Electronics (ICCE)","29 Mar 2018","2018","","","1","5","Estimating driver's gaze zone has very important role to support advanced driver assistant system (ADAS). The gaze estimation can monitor the driver focus and indirectly control the user interface/user experience (UI/UX) on a windshield using augmented reality-head up display (AR-HUD). However, to train gaze zone estimator as a classification task, someone pays huge costs to gather a large amount of annotated dataset. To reduce the labor work, we used a transfer-learning method using pre-trained CNN model to project the gaze estimation task by regression on mobile devices that have large and reliable dataset into new classification task to overcome lack of annotated dataset for gaze zone estimation. We tested the proposed method to our own building simulation test bed. The result is shown in validation accuracy around 99.01 % and test accuracy with unseen driver around 60.25 % for estimating 10 gaze zones in-vehicle.","2158-4001","978-1-5386-3025-9","10.1109/ICCE.2018.8326308","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8326308","","Estimation;Tablet computers;Support vector machines;Automobiles;Mobile handsets;Feature extraction","augmented reality;driver information systems;gaze tracking;head-up displays;human computer interaction;learning (artificial intelligence);mobile computing;motion estimation;neural nets;pattern classification;regression analysis;traffic engineering computing;user interfaces","transfer learning;advanced driver assistant system;augmented reality-head up display;gaze zone estimator;classification task;user interface;user experience;driver gaze zone estimation;pre-trained CNN model;regression;mobile devices","","","","16","","29 Mar 2018","","","IEEE","IEEE Conferences"
"When Vehicles <italic>See</italic> Pedestrians With Phones: A Multicue Framework for Recognizing Phone-Based Activities of Pedestrians","A. Rangesh; M. M. Trivedi","Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA, USA; Laboratory for Intelligent and Safe Automobiles, University of California, San Diego, San Diego, CA, USA","IEEE Transactions on Intelligent Vehicles","23 May 2018","2018","3","2","218","227","The intelligent vehicle community has devoted considerable efforts to model driver behavior, and in particular, to detect and overcome driver distraction in an effort to reduce accidents caused by driver negligence. However, as the domain increasingly shifts toward autonomous and semiautonomous solutions, the driver is no longer integral to the decision-making process, indicating a need to refocus efforts elsewhere. To this end, we propose to study pedestrian distraction instead. In particular, we focus on detecting pedestrians who are engaged in secondary activities involving their cellphones and similar hand-held multimedia devices from a purely vision-based standpoint. To achieve this objective, we propose a pipeline incorporating articulated human pose estimation, followed by a soft object label transfer from an ensemble of exemplar support vector machines trained on the nearest neighbors in pose feature space. We additionally incorporate head gaze features and prior pose information to carry out cellphone related pedestrian activity recognition. Finally, we offer a method to reliably track the articulated pose of a pedestrian through a sequence of images using a particle filter with a Gaussian process dynamical model, which can then be used to estimate sequentially varying activity scores at a very low computational cost. The entire framework is fast (especially for sequential data) and accurate, and easily extensible to include other secondary activities and sources of distraction.","2379-8904","","10.1109/TIV.2018.2804170","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8286931","Articulated pose tracking;computer vision;deep learning;exemplar support vector machines (SVMs);highly autonomous vehicles;panoramic surround behavior analysis;pedestrian activity recognition","Legged locomotion;Intelligent vehicles;Pose estimation;Injuries;Head;Activity recognition","computer vision;driver information systems;feature extraction;Gaussian processes;image classification;learning (artificial intelligence);object detection;object recognition;object tracking;particle filtering (numerical methods);pedestrians;pose estimation;road safety;support vector machines","exemplar support vector machines;cellphone related pedestrian activity recognition;Gaussian process dynamical model;activity scores;secondary activities;phone;multicue framework;intelligent vehicle community;driver behavior;driver distraction;driver negligence;autonomous solutions;semiautonomous solutions;decision-making process;pedestrian distraction;human pose estimation;soft object label transfer;head gaze features","","4","","32","IEEE","8 Feb 2018","","","IEEE","IEEE Journals"
"Study of Mechanisms of Social Interaction Stimulation in Autism Spectrum Disorder by Assisted Humanoid Robot","M. Del Coco; M. Leo; P. Carcagnì; F. Famà; L. Spadaro; L. Ruta; G. Pioggia; C. Distante","National Research Council of Italy, Institute of Applied Sciences and Intelligent Systems, Lecce, Italy; National Research Council of Italy, Institute of Applied Sciences and Intelligent Systems, Lecce, Italy; National Research Council of Italy, Institute of Applied Sciences and Intelligent Systems, Lecce, Italy; National Research Council of Italy, Institute of Applied Sciences and Intelligent Systems, Messina, Italy; National Research Council of Italy, Institute of Applied Sciences and Intelligent Systems, Messina, Italy; National Research Council of Italy, Institute of Applied Sciences and Intelligent Systems, Messina, Italy; National Research Council of Italy, Institute of Applied Sciences and Intelligent Systems, Messina, Italy; National Research Council of Italy, Institute of Applied Sciences and Intelligent Systems, Lecce, Italy","IEEE Transactions on Cognitive and Developmental Systems","7 Dec 2018","2018","10","4","993","1004","Information and communication technologies (ICTs) have been proved to have a great impact in enhancing social, communicative, and language development in children with autism spectrum disorders (ASDs) as demonstrated by plenty of effective technological tools reported in the literature for diagnosis, assessment, and treatment of such neurological diseases. On the contrary, there are very few works exploiting ICT to study the mechanisms that trigger the behavioral patterns during the specialized sessions of treatment focused on social interaction stimulation. From the study of the literature it emerges that the behavioral outcomes are qualitatively evaluated by the therapists making this way impossible to assess, in a consistent manner, the worth of the supplied ASD treatments that should be based on quantitative metric not available for this purpose yet. Moreover, the rare attempts to use a methodological approach are limited to the study of one (of at least a couple) of the several behavioral cues involved. In order to fill this gap, in this paper a technological framework able to analyze and integrate multiple visual cues in order to capture the behavioral trend along an ASD treatment is introduced. It is based on an algorithmic pipeline involving face detection, landmark extraction, gaze estimation, head pose estimation and facial expression recognition and it has been used to detect behavioral features during the interaction among different children, affected by ASD, and a humanoid robot. Experimental results demonstrated the superiority of the proposed framework in the specific application context with respect to leading approaches in the literature, providing a reliable pathway to automatically build a quantitative report that could help therapists to better achieve either ASD diagnosis or assessment tasks.","2379-8939","","10.1109/TCDS.2017.2783684","Fondazione Cassa di Risparmio di Puglia(grant numbers:961_1102014054704); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8207589","Autism spectrum disorders (ASDs) diagnostic;behavioral imaging;human–machine interaction;optical metrology of behavior","Robots;Feature extraction;Face recognition;Autism;Information and communication technology","diseases;emotion recognition;face recognition;feature extraction;gaze tracking;humanoid robots;human-robot interaction;medical disorders;neurophysiology;patient treatment;pose estimation","social interaction stimulation;ASD treatment;ASD diagnosis;autism spectrum disorder;assisted humanoid robot;social language development;neurological diseases;behavioral patterns;face detection;behavioral feature detection;information and communication technologies;landmark extraction;gaze estimation;head pose estimation;facial expression recognition","","2","","46","IEEE","14 Dec 2017","","","IEEE","IEEE Journals"
"Photorealistic Monocular Gaze Redirection Using Machine Learning","D. Kononenko; Y. Ganin; D. Sungatullina; V. Lempitsky","Skolkovo Institute of Science and Technology, Moscow, Russia; Université de Montréal, Montréal, Quebec, Canada; Skolkovo Institute of Science and Technology, Moscow, Russia; Skolkovo Institute of Science and Technology, Moscow, Russia","IEEE Transactions on Pattern Analysis and Machine Intelligence","2 Oct 2018","2018","40","11","2696","2710","We propose a general approach to the gaze redirection problem in images that utilizes machine learning. The idea is to learn to re-synthesize images by training on pairs of images with known disparities between gaze directions. We show that such learning-based re-synthesis can achieve convincing gaze redirection based on monocular input, and that the learned systems generalize well to people and imaging conditions unseen during training. We describe and compare three instantiations of our idea. The first system is based on efficient decision forest predictors and redirects the gaze by a fixed angle in real-time (on a single CPU), being particularly suitable for the videoconferencing gaze correction. The second system is based on a deep architecture and allows gaze redirection by a range of angles. The second system achieves higher photorealism, while being several times slower. The third system is based on real-time decision forests at test time, while using the supervision from a “teacher” deep network during training. The third system approaches the quality of a teacher network in our experiments, and thus provides a highly realistic real-time monocular solution to the gaze correction problem. We present in-depth assessment and comparisons of the proposed systems based on quantitative measurements and a user study.","1939-3539","","10.1109/TPAMI.2017.2737423","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010348","Gaze redirection;machine learning;deep learning;random forest;weakly-supervised learning;image resynthesis","Real-time systems;Training;Cameras;Teleconferencing;Face;Magnetic heads","image motion analysis;learning (artificial intelligence);video signal processing","fixed angle;gaze correction problem;photorealistic monocular gaze redirection;gaze redirection problem;gaze directions;learning-based re-synthesis;monocular input;imaging conditions;machine learning;video conferencing gaze correction;deep network;monocular solution;decision forest predictors;images resynthesis","Databases, Factual;Decision Trees;Deep Learning;Eye Movements;Face;Facial Expression;Female;Fixation, Ocular;Humans;Image Processing, Computer-Assisted;Machine Learning;Male;Neural Networks, Computer;Videoconferencing","1","","50","IEEE","14 Aug 2017","","","IEEE","IEEE Journals"
"Motion-Based Rapid Serial Visual Presentation for Gaze-Independent Brain-Computer Interfaces","D. -O. Won; H. -J. Hwang; D. -M. Kim; K. -R. Müller; S. -W. Lee","Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; Department of Medical IT Convergence Engineering, Kumoh National Institute of Technology, Gumi, South Korea; Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea; Department of Brain and Cognitive Engineering, Korea University, Seoul, South Korea","IEEE Transactions on Neural Systems and Rehabilitation Engineering","9 Feb 2018","2018","26","2","334","343","Most event-related potential (ERP)-based brain-computer interface (BCI) spellers primarily use matrix layouts and generally require moderate eye movement for successful operation. The fundamental objective of this paper is to enhance the perceptibility of target characters by introducing motion stimuli to classical rapid serial visual presentation (RSVP) spellers that do not require any eye movement, thereby applying them to paralyzed patients with oculomotor dysfunctions. To test the feasibility of the proposed motion-based RSVP paradigm, we implemented three RSVP spellers: 1) fixed-direction motion (FM-RSVP); 2) random-direction motion (RM-RSVP); and 3) (the conventional) non-motion stimulation (NM-RSVP), and evaluated the effect of the three different stimulation methods on spelling performance. The two motion-based stimulation methods, FMand RM-RSVP, showed shorter P300 latency and higher P300 amplitudes (i. e., 360.4-379.6 ms; 5.5867- 5.7662 μV) than the NM-RSVP (i.e., 480.4 ms; 4.7426 μV). This led to higher and more stable performances for FMand RM-RSVP spellers than NM-RSVP speller (i. e., 79.06±6.45% for NM-RSVP, 90.60±2.98% for RM-RSVP, and 92.74±2.55% for FM-RSVP). In particular, the proposed motion-based RSVP paradigm was significantly beneficial for about half of the subjects who might not accurately perceive rapidly presented static stimuli. These results indicate that the use of proposed motion-based RSVP paradigm is more beneficial for target recognition when developing BCI applications for severely paralyzed patients with complex ocular dysfunctions.","1558-0210","","10.1109/TNSRE.2017.2736600","Institute for Information and Communications Technology Promotion (IITP) through the Korea Government (MSIT)(grant numbers:2017-0-00451); Development of BCI-based Brain and Cognitive Computing Technology for Recognizing User’s Intentions using Deep Learning) and the MSIT, South Korea, under the SW Starlab support program supervised by the IITP(grant numbers:IITP-2015-1107); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8008839","Brain-computer interface (BCI);gaze-independent;event-related potential (ERP);rapid serial visual presentation (RSVP)","Visualization;Color;Electroencephalography;Training;Target recognition;Shape;Character recognition","brain-computer interfaces;electroencephalography;eye;medical computing;neurophysiology;visual evoked potentials","gaze-independent brain-computer interfaces;event-related potential;brain-computer interface spellers;moderate eye movement;classical rapid serial visual presentation spellers;RSVP paradigm;fixed-direction motion;FM-RSVP;nonmotion stimulation;motion-based stimulation methods;FMand RM-RSVP spellers;NM-RSVP speller;motion-based rapid serial visual presentation;motion stimuli;paralyzed patients;oculomotor dysfunctions;random-direction motion;P300 latency;P300 amplitudes;target recognition","Adult;Brain-Computer Interfaces;Communication Aids for Disabled;Electroencephalography;Equipment Design;Event-Related Potentials, P300;Eye Movements;Female;Fixation, Ocular;Healthy Volunteers;Humans;Male;Paralysis;Psychomotor Performance;Young Adult","29","","60","OAPA","11 Aug 2017","","","IEEE","IEEE Journals"
"Unconstrained and Calibration-Free Gaze Estimation in a Room-Scale Area Using a Monocular Camera","K. Tamura; R. Choi; Y. Aoki","Graduate School of Integrated Design Engineering, Keio University, Kanagawa, Japan; Graduate School of Integrated Design Engineering, Keio University, Kanagawa, Japan; Graduate School of Integrated Design Engineering, Keio University, Kanagawa, Japan","IEEE Access","15 Mar 2018","2018","6","","10896","10908","Gaze estimation using monocular cameras has significant commercial applicability, and many studies have been undertaken on head pose-invariant and calibration-free gaze estimation. The head positions in existing data sets used in these studies are, however, limited to the vicinity of the camera, and methods trained on such data sets are not applicable when subjects are at greater distances from the camera. In this paper, we create a room-scale gaze data set with large variations in head poses to achieve robust gaze estimation across a broader range of widths and depths. The head positions are much farther from the camera, and the resolution of the eye image is lower than in conventional data sets. To address this issue, we propose a likelihood evaluation method based on edge gradients with dense particles for iris tracking, which achieves robust tracking at low-resolution eye images. Cross-validation experiments show that our proposed method is more accurate than conventional methods on all the individuals in our data set.","2169-3536","","10.1109/ACCESS.2017.2734168","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003269","Gaze estimation;iris tracking;particle filter;regression","Cameras;Estimation;Calibration;Image resolution;Face;Robustness","calibration;cameras;eye;gaze tracking;image resolution;iris recognition;pose estimation","unconstrained calibration-free gaze estimation;monocular camera;head pose-invariant;likelihood evaluation method;low-resolution eye images;room-scale gaze data set;edge gradients;iris tracking","","2","","44","OAPA","7 Aug 2017","","","IEEE","IEEE Journals"
"A Deep Learning Approach to Appearance-Based Gaze Estimation under Head Pose Variations","H. Sun; C. Yang; S. Lai","Comput. Sci. Dept., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Comput. Sci. Dept., Nat. Tsing Hua Univ., Hsinchu, Taiwan; Comput. Sci. Dept., Nat. Tsing Hua Univ., Hsinchu, Taiwan","2017 4th IAPR Asian Conference on Pattern Recognition (ACPR)","16 Dec 2018","2017","","","935","940","In this paper, we propose a deep learning based gaze estimation algorithm that estimates the gaze direction from a single face image. The proposed gaze estimation algorithm is based on using multiple convolutional neural networks (CNN) to learn the regression networks for gaze estimation from the eye images. The proposed algorithm can provide accurate gaze estimation for users with different head poses, since it explicitly includes the head pose information into the proposed gaze estimation framework. The proposed algorithm can be widely used for appearance-based gaze estimation in practice. Our experimental results show that the proposed gaze estimation system improves the accuracy of appearance-based gaze estimation under head pose variations compared to the previous methods.","2327-0985","978-1-5386-3354-0","10.1109/ACPR.2017.155","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8575948","Gaze estimation;deep learning;convolutional neural network","Estimation;Three-dimensional displays;Solid modeling;Face;Cameras;Data models","convolutional neural nets;eye;gaze tracking;learning (artificial intelligence);pose estimation;regression analysis","deep learning approach;appearance-based gaze estimation;head pose variations;deep learning based gaze estimation algorithm;gaze direction;gaze estimation framework;gaze estimation system;multiple convolutional neural networks;regression networks;eye images;head pose information","","","","22","","16 Dec 2018","","","IEEE","IEEE Conferences"
"Real Time Eye Gaze Estimation","S. Anwar; M. Milanova; Z. Svetleff; S. Abdulla","Comput. Sci. Dept., UA Little Rock, Little Rock, AR, USA; Comput. Sci. Dept., UA Little Rock, Little Rock, AR, USA; Dept. of Educ. Psychol. & Higher Eduaction, Univ. of Nevada, Las Vegas, NV, USA; Inf. Technolgy Dept., Polytech. Univ., Erbil, Iraq","2017 International Conference on Computational Science and Computational Intelligence (CSCI)","7 Dec 2018","2017","","","526","531","In this paper we used a set of searching techniques that allow to gain information about eye movement, its location and point of view in real time. The algorithm determines the point on the monitor at which the user is looking at. To obtain such data it is necessary to determine the relative position of the eye and the head. The first step is the initialization during which a head model is created. After initialization, the tracking phase is started using an Active Appearance Models (AAM) and Pose from Orthography and Scaling with ITerations (POSIT) algorithm for head position estimation. The purpose of eye gaze estimation or eye tracking can be used for testing the effectiveness of the text, game, or advertising message. The aim of this work is to develop and implement a system for real time eye gaze estimation using PC's webcam only without any additional hardware.","","978-1-5386-2652-8","10.1109/CSCI.2017.89","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8560846","eye gaze;gaze estimation;Active Appearance Model;AAM;POSIT","","eye;gaze tracking;iterative methods;pose estimation;tracking","eye movement;head model;Active Appearance Models;head position estimation;eye gaze estimation;AAM;pose from orthography and scaling with iterations;POSIT","","","","15","","7 Dec 2018","","","IEEE","IEEE Conferences"
"Gaze Tracking in 3D Space with a Convolution Neural Network “See What I See”","A. I. Adiba; S. Asatani; S. Tagawa; H. Niioka; J. Miyake","Department of Mechanical and Bioengineering, Osaka University, Osaka, Japan; Department of Mechanical and Bioengineering, Osaka University, Osaka, Japan; Department of Mechanical and Bioengineering, Osaka University, Osaka, Japan; Department of Mechanical and Bioengineering, Osaka University, Osaka, Japan; Department of Mechanical and Bioengineering, Osaka University, Osaka, Japan","2017 IEEE Applied Imagery Pattern Recognition Workshop (AIPR)","11 Sep 2018","2017","","","1","7","This paper presents integrated architecture to estimate gaze vectors under unrestricted head motions. Since previous approaches focused on estimating gaze toward a small planar screen, calibration is needed prior to use. With a Kinect device, we develop a method that relies on depth sensing to obtain robust and accurate head pose tracking and obtain the eye-in-head gaze direction information by training the visual data from eye images with a Neural Network (NN) model. Our model uses a Convolution Neural Network (CNN) that has five layers: two sets of convolution-pooling pairs and a fully connected-output layer. The filters are taken from the random patches of the images in an unsupervised way by k-means clustering. The learned filters are fed to a convolution layer, each of which is followed by a pooling layer, to reduce the resolution of the feature map and the sensitivity of the output to the shifts and the distortions. In the end, fully connected layers can be used as a classifier with a feed-forward-based process to obtain the weight. We reconstruct the gaze vectors from a set of head and eye pose orientations. The results of this approach suggest that the gaze estimation error is 5 degrees. This model is more accurate than a simple NN and an adaptive linear regression (ALR) approach.","2332-5615","978-1-5386-1235-4","10.1109/AIPR.2017.8457962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8457962","","Convolution;Head;Gaze tracking;Three-dimensional displays;Feature extraction;Biological neural networks","calibration;distortion;eye;gaze tracking;image classification;image filtering;image reconstruction;image resolution;pattern clustering;pose estimation;recurrent neural nets;regression analysis;unsupervised learning","convolution neural network model;fully connected-output layer;gaze vector estimation;3D space;head pose tracking;visual data training;k-means clustering;feature map resolution;learned filters;distortions;feedforward-based process;gaze vector reconstruction;adaptive linear regression approach;ALR approach;gaze tracking;gaze estimation error;fully connected layers;pooling layer;convolution layer;convolution-pooling pairs;eye images;eye-in-head gaze direction information;depth sensing;Kinect device;planar screen;unrestricted head motions;time 3.0 d;I","","","","31","","11 Sep 2018","","","IEEE","IEEE Conferences"
"Pholder: An Eye-Gaze Assisted Reading Application on Android","C. Wirawan; H. Qingyao; L. Yi; S. Yean; B. Lee; F. Ran","Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore; Sch. of Comput. Sci. & Eng., Nanyang Technol. Univ., Singapore, Singapore","2017 13th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)","12 Apr 2018","2017","","","350","353","Eye-gaze has been used extensively in human computer interface design, web layout design and as assistive technology. We successfully built a reading application with automatic scrolling, using the images captured by the in-build camera to determine the eye-gaze. The application, Pholder, uses the appearance-based method for gaze estimation and tracking of gaze movement directions for scrolling of the screen. We used an innovative technique, using the integration of pixel intensity, for gaze movement estimation which is more robust then other techniques.","","978-1-5386-4283-2","10.1109/SITIS.2017.64","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8334770","Gaze movement estimation;Saccade;Mobile;Appearance-based method","Face;Estimation;Robustness;Tracking;Libraries;Cameras;Switches","Android (operating system);gaze tracking;human computer interaction","web layout design;assistive technology;reading application;eye-gaze;Pholder;gaze estimation;tracking;gaze movement directions;gaze movement estimation;human computer interface design","","","","8","","12 Apr 2018","","","IEEE","IEEE Conferences"
"Pupil detection algorithm based on feature extraction for eye gaze","H. Mohsin; S. H. Abdullah","Computer Science, University of Technology, Baghdad, Iraq; Computer Science, University of Technology, Baghdad, Iraq","2017 6th International Conference on Information and Communication Technology and Accessibility (ICTA)","12 Apr 2018","2017","","","1","4","Exact real-time pupil tracking is an important step in a live eye gaze. Since pupil centre is a base point's reference, exact eye centre localization is essential for many applications, such as face recognition and eye gaze estimation. A new method proposed in this paper is to extract pupil eye features exactly within different intensity levels of eye images, mostly with localization of determined interest objects and where the human is looking. As application area, the eye localization in the frame of a video-sequence has been chosen with continuing in iris and pupil detection. This method is fast and has a high degree of accuracy to determine the eye gaze after the pupil is detected because it depends on the features in the human eye. The intensity increases in the centre of the eye, and these features are extracted using a multistage algorithm. Firstly, the feature-based algorithm detected the location of the face region and will be used to detect the pupil on the face. Secondly, use the pupil to determine where humans are looking. Proposed algorithm experiments results to the faces show that they are not only robust, but also relatively efficient. It has been tested on the Mackup database, which contains 500 images belonging to 108 females from the Asian region with different indoor illuminations. The image from a real-world indoor setting with lenses, and images from the Internet. The experiment results show 99%. This ratio shows very good robustness and accuracy.","2379-4402","978-1-5386-4460-7","10.1109/ICTA.2017.8336048","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336048","Pupil extraction;Features of human eye;Eye and pupil detection. Eye gaze detection","Feature extraction;Face;Image color analysis;Iris;Robustness;Iris recognition;Internet","face recognition;feature extraction;gaze tracking;image sequences;video signal processing","pupil eye features;eye images;eye localization;human eye;pupil detection algorithm;feature extraction;real-time pupil tracking;live eye gaze;exact eye centre localization;video sequence;multistage algorithm;face region location;Mackup database;Internet","","2","","18","","12 Apr 2018","","","IEEE","IEEE Conferences"
"Precise gaze estimation for mobile gaze trackers based on hybrid two-view geometry","D. Su; Y. F. Li; Y. Guo","Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Kowloon, Hong Kong; Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Kowloon, Hong Kong; Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Kowloon, Hong Kong","2017 IEEE International Conference on Robotics and Biomimetics (ROBIO)","26 Mar 2018","2017","","","302","307","In this paper, we propose a novel calibration framework for the gaze estimation of mobile gaze tracking systems. In our method, the user's eye and the eye camera are modeled as a central catadioptric camera. Thus the epipolar geometry of the mobile gaze tracker can be described by the hybrid two-view geometry. To calibrate this model, the user is asked to gaze at the calibration points distributed in 3-D space but not all located on one plane. In the light of binocular training data, we apply a 3×6 local hybrid-fundamental matrix to register pupil centers with epipolar lines in the scene image. Thus the image gaze point viewed from different depths can be uniquely determined as the intersection of two epipolar lines calculated by binocular data. The simulation and experimental results show the effectiveness of our proposed calibration framework for mobile gaze trackers.","","978-1-5386-3742-5","10.1109/ROBIO.2017.8324434","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8324434","","Calibration;Cameras;Geometry;Estimation;Training data;Mathematical model;Solid modeling","calibration;cameras;computer vision;eye;gaze tracking;geometry;image sensors;stereo image processing","precise gaze estimation;mobile gaze tracker;two-view geometry;novel calibration framework;mobile gaze tracking systems;eye camera;central catadioptric camera;epipolar geometry;calibration points;epipolar lines;image gaze point;local hybrid-fundamental matrix","","1","","18","","26 Mar 2018","","","IEEE","IEEE Conferences"
"Specialized gaze estimation for children by convolutional neural network and domain adaptation","W. Cui; J. Cui; H. Zha","Key Laboratory of Machine Perception(MOE), Peking University, Beijing 100871, China; Key Laboratory of Machine Perception(MOE), Peking University, Beijing 100871, China; Key Laboratory of Machine Perception(MOE), Peking University, Beijing 100871, China","2017 IEEE International Conference on Image Processing (ICIP)","22 Feb 2018","2017","","","3305","3309","Children's social gaze behavior modeling and evaluation has obtained increasing attentions in various research areas. In psychology research, eye gaze behavior is very important to developmental disorders diagnosis and assessment. In robotics area, gaze interaction between children and robots also draws more and more attention. However, there exists no specific gaze estimator for children in social interaction context. Current approaches usually use models trained with adults' data to estimate children's gaze. Since gaze behaviors and eye appearances of children are different from those of adults, the current approaches, especially those with free-calibration assumptions which are utilized in usual human-robot interaction systems, will result in big errors. Note that children data is difficult to collect and label, so directly learning from children data is hard to achieve. We propose a new system to solve this problem, which combines a CNN feature extractor trained from adult data and a domain adaptation unit using geodesic flow kernel to adapt the source domain (adults) classifier to the target domain (children). Our system performs well in children's gaze estimation.","2381-8549","978-1-5090-2175-8","10.1109/ICIP.2017.8296894","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296894","Gaze Tracking;Neural Network;Semi-Supervised Learning;Computer Vision for Automation","Head;Estimation;Feature extraction;Training;Three-dimensional displays;Convolutional neural networks;Kernel","feature extraction;feedforward neural nets;gaze tracking;learning (artificial intelligence);psychology","convolutional neural network;psychology research;eye gaze behavior;developmental disorders diagnosis;gaze interaction;social interaction context;domain adaptation unit;source domain classifier;CNN feature extractor;geodesic flow kernel;childrens gaze estimation","","2","","13","","22 Feb 2018","","","IEEE","IEEE Conferences"
"Feature extraction using gaze of participants for classifying gender of pedestrians in images","R. Matsumoto; H. Yoshimura; M. Nishiyama; Y. Iwai","Department of Information and Electronics, Graduate School of Engineering, Tottori University; Department of Information and Electronics, Graduate School of Engineering, Tottori University; Department of Information and Electronics, Graduate School of Engineering, Tottori University; Department of Information and Electronics, Graduate School of Engineering, Tottori University","2017 IEEE International Conference on Image Processing (ICIP)","22 Feb 2018","2017","","","3545","3549","Human participants look at informative regions when attempting to identify the gender of a pedestrian in images. In our preliminary experiment, participants mainly looked at the head and chest regions when classifying gender in these images. Thus, we hypothesized that the regions in which participants gaze locations were clustered would contain discriminative features for a gender classifier. In this paper, we discuss how to reveal and use gaze locations for the gender classification of pedestrian images. Our method acquired the distribution of gaze locations from various participants while they manually classified gender. We termed this distribution a gaze map. To extract discriminative features, we assigned large weights to regions with clusters of gaze locations in the gaze map. Our experiments show that this gaze-based feature extraction method significantly improved the performance of gender classification when combined with either a deep learning or a metric learning classifier.","2381-8549","978-1-5090-2175-8","10.1109/ICIP.2017.8296942","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8296942","Gender;Gaze;Feature","Feature extraction;Training;Atmospheric measurements;Particle measurements;Visualization;Task analysis;Kernel","feature extraction;image classification;learning (artificial intelligence)","human participants;informative regions;chest regions;discriminative features;gaze locations;gender classification;pedestrian images;gaze map;feature extraction method;metric learning classifier;head regions","","1","","15","","22 Feb 2018","","","IEEE","IEEE Conferences"
"Eyeball gesture controlled automatic wheelchair using deep learning","A. Rajesh; M. Mantur","Department of Electronics and Communication Enginnering, National Institute of Technology Karnataka; Department of Electronics and Communication Enginnering, National Institute of Technology Karnataka","2017 IEEE Region 10 Humanitarian Technology Conference (R10-HTC)","12 Feb 2018","2017","","","387","391","Traditional wheelchair control is very difficult for people suffering from quadriplegia and are hence, mostly restricted to their beds. Other alternatives include Electroencephalography (EEG) based and Electrooculography (EOG) based automatic wheelchairs which use electrodes to measure neuronal activity in the brain and eye respectively. These are expensive and uncomfortable, and are almost impossible to procure for someone from a backward economy. We present a wheelchair system that can be completely controlled with eye movements and blinks that uses deep convolutional neural networks for classification. We have developed a working prototype based on only a small video camera and a microprocessor that shows upwards of 99% accuracy. We also demonstrate the significant improvement in performance over traditional image processing algorithms for the same. This will allow such patients to be more independent in their day to day lives and significantly improve quality of life at an affordable cost.","2572-7621","978-1-5386-2175-2","10.1109/R10-HTC.2017.8288981","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288981","Deep Learning;Convolution Neural Networks;Eye controlled wheelchair;Hough Transform","Wheelchairs;Transforms;Training;Convolution;Kernel;Cameras;Feature extraction","computer vision;eye;feedforward neural nets;gaze tracking;handicapped aids;image sensors;learning (artificial intelligence);microprocessor chips;wheelchairs","image processing algorithms;microprocessor;video camera;deep convolutional neural networks;eye movements;backward economy;EOG;EEG;electrooculography;electroencephalography;quadriplegia;deep learning;Eyeball gesture controlled automatic wheelchair","","5","","8","","12 Feb 2018","","","IEEE","IEEE Conferences"
"Long range gaze estimation with multiple near-infrared emitters","Z. Ma; Z. Liu; M. Ho; J. Yen; Y. Chen","Department of Electrical Engineering, National Taiwan University, Taipei, TAIWAN, ROC; Department of Electrical Engineering, National Taiwan University, Taipei, TAIWAN, ROC; Department of Surgery, National Taiwan University, Taipei, TAIWAN, ROC; Department of Mechanical, National Taiwan University, Taipei, TAIWAN, ROC; Department of Electrical Engineering, National Taiwan University, Taipei, TAIWAN, ROC","2017 International Automatic Control Conference (CACS)","8 Feb 2018","2017","","","1","5","Gaze estimation is used to identify the eye foci of users for intention identification or as the input devi for computer/control systems. A novel long-range gai estimation algorithm is developed in this paper wi interpolations method and head motions compensation Among multiple near-infrared (NIR) emitters, particul sets of NIR's are chosen for different head positions provide the optimal gaze point estimations. Experimen were conducted showing satisfactory results for t method.","","978-1-5386-3900-9","10.1109/CACS.2017.8284270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8284270","","Calibration;Head;Interpolation;Gaze tracking;Reflection;Mathematical model;Estimation","gaze tracking;interpolation;motion compensation;motion estimation","head motions compensation;long-range gaze estimation algorithm;interpolation method;NIR emitters;near-infrared emitters;computer-control systems","","","","13","","8 Feb 2018","","","IEEE","IEEE Conferences"
"A simple but effective appearance-based gaze estimation method from massive synthetic eye images","Y. Wang; T. Zhao; X. Ding; T. Shen; J. Bian; X. Fu","School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian 116024, China; Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; Information Science and Technology College, Dalian Maritime University, Dalian 116026, China; School of Physics and Optoelectronic Engineering, Dalian University of Technology, Dalian 116024, China; Information Science and Technology College, Dalian Maritime University, Dalian 116026, China","2017 12th IEEE Conference on Industrial Electronics and Applications (ICIEA)","8 Feb 2018","2017","","","1184","1188","A novel method for appearance-based gaze estimation from massive synthetic eye images is proposed in this paper. This method is a combination of neighbor selection and gaze local regression for gaze mapping. First, a simple cascaded method using multiple k-NN(k-Nearest Neighbor) classifier is employed to select neighbors in feature space joint head pose, pupil center and eye appearance. Second, PLSR (Partial Least Square Regression) is applied to seek for a direct correlation between image feature and gaze angle. Experimental results demonstrate that the proposed method achieves state-of-the-art accuracy below 1 degree for with-in subject gaze estimation on public synthesis eye image dataset.","2158-2297","978-1-5090-6161-7","10.1109/ICIEA.2017.8283019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283019","Gaze estimation;Head pose;KNN;PLSR;Synthetic images","Head;Estimation;Feature extraction;Training;Computational modeling;Indexes;Image resolution","face recognition;feature extraction;gaze tracking;least squares approximations;nearest neighbour methods;pose estimation;regression analysis","massive synthetic eye images;neighbor selection;gaze local regression;gaze mapping;simple cascaded method;eye appearance;Partial Least Square Regression;image feature;gaze angle;public synthesis eye image dataset;appearance-based gaze estimation method;feature space joint head pose;pupile center;with-in subject gaze estimation","","","","11","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Vision interaction method based on visual attention mechanism","B. Yan; S. Yu; T. Pei; Y. Hu","School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, China; School of Reliability and Systems Engineering, Beihang University, Beijing, China","2017 12th IEEE Conference on Industrial Electronics and Applications (ICIEA)","8 Feb 2018","2017","","","930","935","This paper puts forward a calibration-free vision interaction system using Visual Attention Mechanism (VAM), which requires the user to watch a video clip in advance to alleviate the problem of cumbersome and unnatural calibration, which exists in traditional gaze estimators. Our method establishes the mapping between eye features and gaze points using Support Vector Machine (SVM), in which eye features are constructed using Pupil Center Cornea Reflection (PCCR) vectors and gaze points are found using saliency maps. The experimental results show an increase in gaze estimator accuracy and and the vision interaction applications show the applicability of the vision interaction system.","2158-2297","978-1-5090-6161-7","10.1109/ICIEA.2017.8282972","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8282972","gaze estimator;VAM;SVM;PCCR vector;saliency map","Calibration;Computational modeling;Visualization;Machine vision;Support vector machines;Human computer interaction","calibration;computer vision;eye;feature extraction;gaze tracking;support vector machines","vision interaction method;visual attention mechanism;video clip;unnatural calibration;traditional gaze estimators;gaze points;Support Vector Machine;Pupil Center Cornea Reflection vectors;gaze estimator accuracy;vision interaction applications;calibration-free vision interaction system;eye feature mapping;SVM;PCCR vectors","","1","","15","","8 Feb 2018","","","IEEE","IEEE Conferences"
"Eye-motion detection system for mnd patients","C. Xu; C. Lin","Department of Mechanical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Mechanical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan","2017 IEEE 4th International Conference on Soft Computing & Machine Intelligence (ISCMI)","5 Feb 2018","2017","","","99","103","This paper aims to develop an eye-motion based communication system for motor neuron disease (MND) patients to contact with care providers any time they want when they lie on the bed. This eye-motion detection system involves technical modules of eye-blink detection, gaze estimation and head pose estimation on MND patients. The system comprises a rotating arm with a camera, an infrared light source and a speaker. The arm with the camera will autonomously rotate to track the face of the patient with arbitrary body directions so that necessary eye detections can be properly conducted when it is called. When the patient needs to call out, only designated blinking and eye motions will trigger the call out action for the need of finding the care provider.","","978-1-5386-1314-6","10.1109/ISCMI.2017.8279606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8279606","eye-motion detection;motor neuron disease;blink detection;gaze estimation;head pose estimation","Face;Cameras;Ear;Face detection;Pose estimation;Feature extraction","cameras;diseases;eye;image motion analysis;medical image processing;neurophysiology;object detection;patient diagnosis;pose estimation","eye-motion detection system;MND patients;eye-motion based communication system;motor neuron disease patients;eye-blink detection;care providers;technical modules;gaze estimation;head pose estimation;rotating arm;camera;infrared light source;speaker;blinking motion","","2","","10","","5 Feb 2018","","","IEEE","IEEE Conferences"
"Real Time Eye Gaze Tracking with 3D Deformable Eye-Face Model","K. Wang; Q. Ji","ECSE Dept., Rensselaer Polytech. Inst., Troy, NY, USA; ECSE Dept., Rensselaer Polytech. Inst., Troy, NY, USA","2017 IEEE International Conference on Computer Vision (ICCV)","25 Dec 2017","2017","","","1003","1011","3D model-based gaze estimation methods are widely explored because of their good accuracy and ability to handle free head movement. Traditional methods with complex hardware systems (Eg. infrared lights, 3D sensors, etc.) are restricted to controlled environments, which significantly limit their practical utilities. In this paper, we propose a 3D model-based gaze estimation method with a single web-camera, which enables instant and portable eye gaze tracking. The key idea is to leverage on the proposed 3D eye-face model, from which we can estimate 3D eye gaze from observed 2D facial landmarks. The proposed system includes a 3D deformable eye-face model that is learned offline from multiple training subjects. Given the deformable model, individual 3D eye-face models and personal eye parameters can be recovered through the unified calibration algorithm. Experimental results show that the proposed method outperforms state-of-the-art methods while allowing convenient system setup and free head movement. A real time eye tracking system running at 30 FPS also validates the effectiveness and efficiency of the proposed method.","2380-7504","978-1-5386-1032-9","10.1109/ICCV.2017.114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237376","","Three-dimensional displays;Solid modeling;Estimation;Deformable models;Calibration;Gaze tracking;Two dimensional displays","calibration;cameras;eye;face recognition;gaze tracking;image motion analysis;pose estimation","real time eye gaze tracking;real time eye tracking system;2D facial landmarks;unified calibration algorithm;Web camera;personal eye parameters;deformable model;3D eye gaze;3D eye-face model;portable eye gaze tracking;gaze estimation method;free head movement;3D deformable eye-face model","","23","","33","","25 Dec 2017","","","IEEE","IEEE Conferences"
"Monocular Free-Head 3D Gaze Tracking with Deep Learning and Geometry Constraints","H. Deng; W. Zhu",NA; NA,"2017 IEEE International Conference on Computer Vision (ICCV)","25 Dec 2017","2017","","","3162","3171","Free-head 3D gaze tracking outputs both the eye location and the gaze vector in 3D space, and it has wide applications in scenarios such as driver monitoring, advertisement analysis and surveillance. A reliable and low-cost monocular solution is critical for pervasive usage in these areas. Noticing that a gaze vector is a composition of head pose and eyeball movement in a geometrically deterministic way, we propose a novel gaze transform layer to connect separate head pose and eyeball movement models. The proposed decomposition does not suffer from head-gaze correlation overfitting and makes it possible to use datasets existing for other tasks. To add stronger supervision for better network training, we propose a two-step training strategy, which first trains sub-tasks with rough labels and then jointly trains with accurate gaze labels. To enable good cross-subject performance under various conditions, we collect a large dataset which has full coverage of head poses and eyeball movements, contains 200 subjects, and has diverse illumination conditions. Our deep solution achieves state-of-the-art gaze tracking accuracy, reaching 5.6° cross-subject prediction error using a small network running at 1000 fps on a single CPU (excluding face alignment time) and 4.3° cross-subject error with a deeper network.","2380-7504","978-1-5386-1032-9","10.1109/ICCV.2017.341","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8237603","","Head;Gaze tracking;Training;Three-dimensional displays;Correlation;Predictive models;Transforms","eye;feature extraction;gaze tracking;image motion analysis;learning (artificial intelligence);pose estimation","monocular free-head 3D;geometry constraints;eye location;gaze vector;wide applications;driver monitoring;advertisement analysis;reliable cost monocular solution;low-cost monocular solution;pervasive usage;eyeball movement models;head-gaze correlation;network training;two-step training strategy;trains sub-tasks;accurate gaze labels;good cross-subject performance;head poses;state-of-the-art gaze;cross-subject prediction error;video surveillance;CPU","","18","","54","","25 Dec 2017","","","IEEE","IEEE Conferences"
"Appearance-based gaze block estimation via CNN classification","X. Wu; J. Li; Q. Wu; J. Sun","School of Information Science and Engineering, Shandong University, Jinan, China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan, China; School of Information Science and Engineering, Shandong University, Jinan, China; School of Information Science and Engineering, Shandong University, Jinan, China","2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)","30 Nov 2017","2017","","","1","5","Appearance-based gaze estimation methods have received increasing attention in the field of human-computer interaction (HCI). These methods tried to estimate the accurate gaze point via Convolutional Neural Network (CNN) model, but the estimated accuracy can't reach the requirement of gaze-based HCI when the regression model is used in the output layer of CNN. Given the popularity of button-touch-based interaction, we propose an appearance-based gaze block estimation method, which aims to estimate the gaze block, not the gaze point. In the proposed method, we relax the estimation from point to block, so that the gaze block can be estimated by CNN-based classification instead of the previous regression model. We divide the screen into square blocks to imitate the button-touch interface, and build an eye-image dataset, which contains the eye images labelled by their corresponding gaze blocks on the screen. We train the CNN model according to this dataset to estimate the gaze block by classifying the eye images. The experiments on 6- and 54-block classifications demonstrate that the proposed method has high accuracy in gaze block estimation without any calibration, and it is promising in button-touch-based interaction.","2473-3628","978-1-5090-3649-3","10.1109/MMSP.2017.8122270","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122270","Gaze estimation;Appearance-based;Gaze block;CNN;Button-Touch-Based Interaction","Estimation;Training;Feature extraction;Human computer interaction;Lighting;Videos;Information science","eye;gaze tracking;human computer interaction;image classification;learning (artificial intelligence);neural nets;user interfaces","human-computer interaction;Convolutional Neural Network model;gaze block estimation method;square blocks;CNN classification;eye imag classification","","6","1","20","","30 Nov 2017","","","IEEE","IEEE Conferences"
"Gaze estimation using 3-D eyeball model under HMD circumstance","S. Y. Han; S. H. Lee; N. I. Cho","Dept. of ECE, INMC Seoul National Univ., Seoul, Korea; Dept. of ECE, INMC Seoul National Univ., Seoul, Korea; Dept. of ECE, INMC Seoul National Univ., Seoul, Korea","2017 IEEE 19th International Workshop on Multimedia Signal Processing (MMSP)","30 Nov 2017","2017","","","1","4","This paper presents a gaze estimation algorithm using 3-D eyeball model and 2-D pupil center - inner eye corner(PC-IEC) vector. The conventional methods using feature points in the eye images need lots of calibration markers and long calibration time. However, since the pupil and gaze movements are closely related to the 3-D rotation of eyeball, the long and complicated calibrations are not necessary. This paper derives the relationship between the 3-D eyeball model and 2-D PC- IEC vector with a single reference calibration point which is located at the center of the screen. Also, the proposed algorithm compensates for the eyeball movements using the eyelid height against the inner eye corner. According to the experiment, the proposed method estimates the gaze within 2 degree error.","2473-3628","978-1-5090-3649-3","10.1109/MMSP.2017.8122263","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122263","","Calibration;Cameras;Eyelids;Solid modeling;Resists;Estimation;Gaze tracking","calibration;eye;face recognition;feature extraction;gaze tracking","complicated calibrations;3-D eyeball model;single reference calibration point;eyeball movements;gaze estimation;calibration markers;long calibration time;2-D PC-IEC vector;eyelid height;2-D pupil center-inner eye corner vector;feature points;gaze movements;pupil movements","","3","","10","","30 Nov 2017","","","IEEE","IEEE Conferences"
"Robust face model based approach to head pose estimation","K. Fornalczyk; A. Wojciechowski","Lodz University of Technology, Institute of Information Technology, Wolczanska 215, 90-924, Lodz, Poland; Lodz University of Technology, Institute of Information Technology, Wolczanska 215, 90-924, Lodz, Poland","2017 Federated Conference on Computer Science and Information Systems (FedCSIS)","13 Nov 2017","2017","","","1291","1295","Head pose estimation from camera images is a computational problem that may influence many sociological, cognitive, interaction and marketing researches. It is especially crucial in the process of visual gaze estimation which accuracy depends not only on eye region analysis, but head inferring as well. Presented method exploits a 3d head model for a user head pose estimation as it outperforms, in the context of performance, popular appearance based approaches and assures efficient face head pose analysis. The novelty of the presented approach lies in a default head model refinement according to the selected facial features localisation. The new method not only achieves very high precision (about 4°), but iteratively improves the reference head model. The results of the head pose inferring experiments were verified with professional Vicon motion tracking system and head model refinement accuracy was verified with high precision Artec structural light scanner.","","978-83-946253-7-5","10.15439/2017F425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104720","","Face;Magnetic heads;Pose estimation;Solid modeling;Three-dimensional displays;Cameras","face recognition;feature extraction;pose estimation","camera images;computational problem;visual gaze estimation;eye region analysis;head inferring;user head;default head model refinement;reference head model;inferring experiments;head model refinement accuracy;robust face model based approach;head pose estimation;3D head model;facial feature localisation;professional Vicon motion tracking system;high precision Artec structural light scanner","","1","","33","","13 Nov 2017","","","IEEE","IEEE Conferences"
"Learning from Simulated and Unsupervised Images through Adversarial Training","A. Shrivastava; T. Pfister; O. Tuzel; J. Susskind; W. Wang; R. Webb",NA; NA; NA; NA; NA; NA,"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","9 Nov 2017","2017","","","2242","2251","With recent progress in graphics, it has become more tractable to train models on synthetic images, potentially avoiding the need for expensive annotations. However, learning from synthetic images may not achieve the desired performance due to a gap between synthetic and real image distributions. To reduce this gap, we propose Simulated+Unsupervised (S+U) learning, where the task is to learn a model to improve the realism of a simulators output using unlabeled real data, while preserving the annotation information from the simulator. We develop a method for S+U learning that uses an adversarial network similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors. We make several key modifications to the standard GAN algorithm to preserve annotations, avoid artifacts, and stabilize training: (i) a self-regularization term, (ii) a local adversarial loss, and (iii) updating the discriminator using a history of refined images. We show that this enables generation of highly realistic images, which we demonstrate both qualitatively and with a user study. We quantitatively evaluate the generated images by training models for gaze estimation and hand pose estimation. We show a significant improvement over using synthetic images, and achieve state-of-the-art results on the MPIIGaze dataset without any labeled real data.","1063-6919","978-1-5386-0457-1","10.1109/CVPR.2017.241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8099724","","Training;Gallium nitride;Neural networks;Pose estimation;Computational modeling;Data models","pose estimation;realistic images;unsupervised learning","Generative Adversarial Networks;synthetic image distributions;Simulated+Unsupervised learning;gaze estimation;hand pose estimation;MPIIGaze dataset","","344","13","49","","9 Nov 2017","","","IEEE","IEEE Conferences"
"Predicting Salient Face in Multiple-Face Videos","Y. Liu; S. Zhang; M. Xu; X. He","Beihang Univ., Beijing, China; Beihang Univ., Beijing, China; Beihang Univ., Beijing, China; ShanghaiTech Univ., Shanghai, China","2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","9 Nov 2017","2017","","","3224","3232","Although the recent success of convolutional neural network (CNN) advances state-of-the-art saliency prediction in static images, few work has addressed the problem of predicting attention in videos. On the other hand, we find that the attention of different subjects consistently focuses on a single face in each frame of videos involving multiple faces. Therefore, we propose in this paper a novel deep learning (DL) based method to predict salient face in multiple-face videos, which is capable of learning features and transition of salient faces across video frames. In particular, we first learn a CNN for each frame to locate salient face. Taking CNN features as input, we develop a multiple-stream long short-term memory (M-LSTM) network to predict the temporal transition of salient faces in video sequences. To evaluate our DL-based method, we build a new eye-tracking database of multiple-face videos. The experimental results show that our method outperforms the prior state-of-the-art methods in predicting visual attention on faces in multiple-face videos.","1063-6919","978-1-5386-0457-1","10.1109/CVPR.2017.343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8099826","","Face;Videos;Conferences;Computer vision;Pattern recognition;Machine learning;Video sequences","convolution;face recognition;gaze tracking;image sequences;learning (artificial intelligence);neural nets;object detection;video databases;video signal processing","convolutional neural network;saliency prediction;multiple-stream long short-term memory;M-LSTM;static images;deep learning;DL based method;video sequences;eye-tracking database;video frames;multiple-face videos;salient face","","8","","43","","9 Nov 2017","","","IEEE","IEEE Conferences"
"Eye gaze detection system for impaired user GUI control","J. Hughes; S. Rhodes; B. E. Dunne","Grand Haven, MI 49417 USA; School of Engineering, Grand Valley State University, Grand Rapids, MI 49504 USA; School of Engineering, Grand Valley State University, Grand Rapids, MI 49504 USA","2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)","2 Oct 2017","2017","","","1348","1351","Gaze point, or point of regard, refers to the point in space where an individual's visual attention is focused. Estimating gaze point at a given time can provide a host of information pertaining to intention and perception of a scene. This information can be applied toward enhancement of Human-Computer Interaction (HCI), making such interfaces more smooth and efficient. For the population with limited or no mobility, a gaze point estimation system that accurately selects components of a computer application is extremely beneficial. Herein, a custom gaze point detection HW/SW system intended to allow the mobility hindered population the ability to control selection in a computer interface via gaze is presented. The principles of the image difference method for pupil detection, coupled with glint detection and calibration were implemented for an accurate, occlusion-immune estimation of gaze point in real-time. Under the ideal scenario of static head pose and lighting environment, the system was accurate to 1.05°. The gaze estimator tolerated small 1.5 inch head translations and over two orders of magnitude change in ambient illuminance, while sacrificing less than 1° of accuracy.","1558-3899","978-1-5090-6389-5","10.1109/MWSCAS.2017.8053181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053181","GUI control;eye gaze detection;impaired user;HCI","Light sources;Head;Lenses;Gaze tracking;Light emitting diodes;Finite impulse response filters;Cameras","calibration;eye;gaze tracking;graphical user interfaces;human computer interaction;pose estimation","lighting environment;static head pose;calibration;glint detection;pupil detection;image difference method;computer interface;HCI;custom gaze point detection HW-SW system;Human-Computer Interaction;impaired user GUI control;eye gaze detection system;gaze estimator;gaze point estimation system","","","","4","","2 Oct 2017","","","IEEE","IEEE Conferences"
"Pupil localization for gaze estimation using unsupervised graph-based model","S. Rabba; Y. He; M. Kyan; L. Guan","Ryerson University, Toronto, Canada; Ryerson University, Toronto, Canada; York University, Toronto, Canada; Ryerson University, Toronto, Canada","2017 IEEE International Symposium on Circuits and Systems (ISCAS)","28 Sep 2017","2017","","","1","4","In this paper, we propose a graph-based model for pupil localization, which is a step towards gaze detection. The proposed model can differentiate the key points located at the eyelashes, eyebrows and eye white regions. We first crop the eye region with an ellipse and then estimate the pupil center within the ellipse, thus reducing the computational complexity. We also consider the light reflections in the pupil region, which could lead to inaccuracy in pupil localization. We construct an undirected graph in the eye region based on the key points consisting of the corner points in the eye region, the centers of light reflection regions, and the multiple pixels with a high intensity in the pupil region. The pupil center is initially estimated as the weighted center of the revised graph after vertex/edge removal. In addition, we shift the initial pupil center to a revised position based on the line segments in the pupil region. We evaluate the proposed method on 850 eye images from a public database. The experimental results demonstrate that the proposed method can achieve a more accurate result compared to the existing work.","2379-447X","978-1-4673-6853-7","10.1109/ISCAS.2017.8050660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8050660","pupil localization;gaze estimation;graph model","Reflection;Eyelashes;Estimation;Eyebrows;Computational modeling;Computational complexity;Image segmentation","computational complexity;eye;gaze tracking;graph theory;image processing;light reflection","pupil localization;gaze estimation;unsupervised graph-based model;gaze detection;pupil center estimation;computational complexity;light reflection;vertex-edge removal","","1","","10","","28 Sep 2017","","","IEEE","IEEE Conferences"
"Using point cloud data to improve three dimensional gaze estimation","H. Wang; M. Antonelli; B. E. Shi","Department of Electronic and Computer Engineering at the Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering at the Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong; Department of Electronic and Computer Engineering at the Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong","2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)","14 Sep 2017","2017","","","795","798","This paper addresses the problem of estimating gaze location in the 3D environment using a remote eye tracker. Instead of relying only on data provided by the eye tracker, we investigate how to integrate gaze direction with the point-cloud-based representation of the scene provided by a Kinect sensor. The algorithm first combines the gaze vectors for the two eyes provided by the eye tracker into a single gaze vector emanating from a point in between the two eyes. The gaze target in the three dimensional environment is then identified by finding the point in the 3D point cloud that is closest to the gaze vector. Our experimental results demonstrate that the estimate of the gaze target location provided by this method is significantly better than that provided when considering gaze information alone. It is also better than two other methods for integrating point cloud information: (1) finding the 3D point closest to the gaze location as estimated by triangulating the gaze vectors from the two eyes, and (2) finding the 3D point with smallest average distance to the two gaze vectors considered individually. The proposed method has an average error of 1.7 cm in a workspace of 25 × 23 × 24 cm located at a distance of 60 cm from the user.","1558-4615","978-1-5090-2809-2","10.1109/EMBC.2017.8036944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036944","Eye tracker;3D gaze estimation;point cloud;human computer interaction","Three-dimensional displays;Calibration;Estimation;Standards;Two dimensional displays;Testing;Robots","eye;gaze tracking;human computer interaction;image sensors;medical computing","point cloud data;three-dimensional gaze estimation;gaze location;3D environment;remote eye tracker;gaze direction;point-cloud-based representation;Kinect sensor;gaze vectors;single gaze vector;3D point cloud;gaze target location","Algorithms;Eye;Fixation, Ocular","3","","17","","14 Sep 2017","","","IEEE","IEEE Conferences"
"An iterative representation learning framework to predict the sequence of eye fixations","C. Xia; F. Qi; G. Shi","School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, 710071, PR China; School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, 710071, PR China; School of Electronic Engineering, Xidian University, Xi'an, Shaanxi, 710071, PR China","2017 IEEE International Conference on Multimedia and Expo (ICME)","31 Aug 2017","2017","","","1530","1535","Visual attention is a dynamic search process of acquiring information. However, most previous studies have focused on the prediction of static attended locations. Without considering the temporal relationship of fixations, these models usually cannot explain the dynamic saccadic behavior well. In this paper, an iterative representation learning framework is proposed to predict the saccadic scanpath. Within the proposed framework, saccade can be explained as an iterative process of finding the most uncertain area and updating the representation of scenes. In implementation, a deep autoencoder is employed for representation learning. The current fixation is predicted to be the most salient pixel, with saliency estimated by the reconstruction residual of the deep network. Image patches around this fixation are then sampled to update the network for the selection of subsequent fixations. Compared with existing models, the proposed model shows the state-of-the-art performance on several public data sets.","1945-788X","978-1-5090-6067-2","10.1109/ICME.2017.8019396","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019396","Autoencoder;deep learning;representation learning;saliency;saccade","Image reconstruction;Visualization;Training;Computational modeling;Predictive models;Estimation;Data models","computer vision;eye;gaze tracking;iterative methods;learning (artificial intelligence);search problems","image patches;deep autoencoder;iterative process;saccadic scanpath prediction;dynamic saccadic behavior;fixation temporal relationship;static attended location prediction;dynamic search process;visual attention;eye fixation sequence prediction;iterative representation learning framework","","3","","24","","31 Aug 2017","","","IEEE","IEEE Conferences"
"It’s Written All Over Your Face: Full-Face Appearance-Based Gaze Estimation","X. Zhang; Y. Sugano; M. Fritz; A. Bulling","Perceptual User Interfaces Group, Max Planck Inst. for Inf., Saarbrucken, Germany; Grad. Sch. of Inf. Sci. & Technol., Osaka Univ., Suita, Japan; Scalable Learning & Perception Group, Max Planck Inst. for Inf., Saarbrucken, Germany; Perceptual User Interfaces Group, Max Planck Inst. for Inf., Saarbrucken, Germany","2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","24 Aug 2017","2017","","","2299","2308","Eye gaze is an important non-verbal cue for human affect analysis. Recent gaze estimation work indicated that information from the full face region can benefit performance. Pushing this idea further, we propose an appearance-based method that, in contrast to a long-standing line of work in computer vision, only takes the full face image as input. Our method encodes the face image using a convolutional neural network with spatial weights applied on the feature maps to flexibly suppress or enhance information in different facial regions. Through extensive evaluation, we show that our full-face method significantly outperforms the state of the art for both 2D and 3D gaze estimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on EYEDIAP for person-independent 3D gaze estimation. We further show that this improvement is consistent across different illumination conditions and gaze directions and particularly pronounced for the most challenging extreme head poses.","2160-7516","978-1-5386-0733-6","10.1109/CVPRW.2017.284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8015018","","Estimation;Three-dimensional displays;Two dimensional displays;Face;Cameras;Training","computer vision;convolution;eye;face recognition;gaze tracking;image enhancement;neural nets;pose estimation","full-face appearance;eye gaze;human affect analysis;computer vision;convolutional neural network;feature maps;information enhancement;MPIIGaze;EYEDIAP;3D gaze estimation;illumination conditions;head poses","","40","2","36","","24 Aug 2017","","","IEEE","IEEE Conferences"
"A Review and Analysis of Eye-Gaze Estimation Systems, Algorithms and Performance Evaluation Methods in Consumer Platforms","A. Kar; P. Corcoran","Department of Electrical and Electronic Engineering, Center for Cognitive, Connected, and Computational Imaging, National University of Ireland, Galway, Ireland; Department of Electrical and Electronic Engineering, Center for Cognitive, Connected, and Computational Imaging, National University of Ireland, Galway, Ireland","IEEE Access","6 Sep 2017","2017","5","","16495","16519","In this paper, a review is presented for the research on eye gaze estimation techniques and applications, which has progressed in diverse ways over the past two decades. Several generic eye gaze use-cases are identified: desktop, TV, head-mounted, automotive, and handheld devices. Analysis of the literature leads to the identification of several platform specific factors that influence gaze tracking accuracy. A key outcome from this review is the realization of a need to develop standardized methodologies for the performance evaluation of gaze tracking systems and achieve consistency in their specification and comparative evaluation. To address this need, the concept of a methodological framework for practical evaluation of different gaze tracking systems is proposed.","2169-3536","","10.1109/ACCESS.2017.2735633","Science Foundation Ireland through the Strategic Partnership Program; FotoNation Ltd., Next Generation Imaging for Smartphone and Embedded Platforms(grant numbers:13/SPP/I2868); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8003267","Eye gaze;gaze estimation;accuracy;error sources;performance evaluation;user platforms","Gaze tracking;Estimation;Visualization;Performance evaluation;Computers;Calibration;Imaging","gaze tracking;performance evaluation","gaze tracking systems;standardized methodologies;review;consumer platforms;performance evaluation;eye-gaze estimation","","40","","214","OAPA","7 Aug 2017","","","IEEE","IEEE Journals"
"Out-of-Sample Extension for Dimensionality Reduction of Noisy Time Series","H. Dadkhahi; M. F. Duarte; B. M. Marlin","Electrical and Computer Engineering Department, University of Massachusetts at Amherst, Amherst, MA, USA; Electrical and Computer Engineering Department, University of Massachusetts Amherst, MA, USA; College of Information and Computer Sciences, University of Massachusetts Amherst, MA, USA","IEEE Transactions on Image Processing","1 Sep 2017","2017","26","11","5435","5446","This paper proposes an out-of-sample extension framework for a global manifold learning algorithm (Isomap) that uses temporal information in out-of-sample points in order to make the embedding more robust to noise and artifacts. Given a set of noise-free training data and its embedding, the proposed framework extends the embedding for a noisy time series. This is achieved by adding a spatio-temporal compactness term to the optimization objective of the embedding. To the best of our knowledge, this is the first method for out-of-sample extension of manifold embeddings that leverages timing information available for the extension set. Experimental results demonstrate that our out-of-sample extension algorithm renders a more robust and accurate embedding of sequentially ordered image data in the presence of various noise and artifacts when compared with other timing-aware embeddings. Additionally, we show that an out-of-sample extension framework based on the proposed algorithm outperforms the state of the art in eye-gaze estimation.","1941-0042","","10.1109/TIP.2017.2735189","National Science Foundation(grant numbers:IIS-1239341,IIS-1319585,IIS-1350522); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000338","Manifold learning;dimensionality reduction;time series;out-of-sample extension","Manifolds;Training;Time series analysis;Principal component analysis;Sensors;Data models;Noise measurement","gaze tracking;image sequences;learning (artificial intelligence);rendering (computer graphics);time series","dimensionality reduction;noisy time series;global manifold learning algorithm;temporal information;noise-free training data;spatio-temporal compactness;optimization;manifold embeddings;timing information;out-of-sample extension algorithm;sequentially ordered image data;eye-gaze estimation","","1","","30","IEEE","2 Aug 2017","","","IEEE","IEEE Journals"
"Two-eye model-based gaze estimation from a Kinect sensor","X. Zhou; H. Cai; Y. Li; H. Liu","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computing, University of Portsmouth, Portsmouth, UK; Department of Mechanical and Biomedical Engineering, City University of Hong Kong, Hong Kong, China; School of Computing, University of Portsmouth, Portsmouth, UK","2017 IEEE International Conference on Robotics and Automation (ICRA)","24 Jul 2017","2017","","","1646","1653","In this paper, we present an effective and accurate gaze estimation method based on two-eye model of a subject with the tolerance of free head movement from a Kinect sensor. To accurately and efficiently determine the point of gaze, i) we employ two-eye model to improve the estimation accuracy; ii) we propose an improved convolution-based means of gradients method to localize the iris center in 3D space; iii) we present a new personal calibration method that only needs one calibration point. The method approximates the visual axis as a line from the iris center to the gaze point to determine the eyeball centers and the Kappa angles. The final point of gaze can be calculated by using the calibrated personal eye parameters. We experimentally evaluate the proposed gaze estimation method on eleven subjects. Experimental results demonstrate that our gaze estimation method has an average estimation accuracy around 1.99°, which outperforms many leading methods in the state-of-the-art.","","978-1-5090-4633-1","10.1109/ICRA.2017.7989194","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7989194","","Estimation;Iris;Calibration;Three-dimensional displays;Solid modeling;Head;Visualization","calibration;convolution;gaze tracking;gradient methods;iris recognition","two-eye model-based gaze estimation;Kinect sensor;free head movement tolerance;convolution-based gradients method;iris center localization;calibration method;Kappa angles","","12","","20","","24 Jul 2017","","","IEEE","IEEE Conferences"
"A new reconstruction method in gaze estimation with natural head movement","Y. Liu; B. Lee; M. McKeown","School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Medicine, University of British, Columbia, Canada","2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)","20 Jul 2017","2017","","","219","222","We present a novel reconstruction method for the appearance-based gaze estimation that allows inferring persons' gaze under natural head movement. We first study that the locally linear combination in the respective manifolds consisting of stable left and right eye appearances is efficient. The local structure of the manifolds is destroyed when there is head movement. This is due to the destruction of the intrinsic relations between the two eyes(left and right) when we do locally linear combinations. We then introduce a new combination of both eye appearances, which maintains the relation embedding into the reconstruction of the training stage. Through comparison with other well known methods, we show that the proposed method achieves an optimal performance with head pose variation.","","978-4-9011-2216-0","10.23919/MVA.2017.7986840","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986840","","Estimation;Face;Feature extraction;Manifolds;Tracking;Magnetic heads","estimation theory;gaze tracking;image reconstruction;pose estimation","gaze estimation;natural head movement;reconstruction method;left eye appearances;right eye appearances;locally linear combinations;head pose variation","","1","","15","","20 Jul 2017","","","IEEE","IEEE Conferences"
"Point of gaze estimation using corneal surface reflection and omnidirectional camera image","T. Ogawa; A. Nakazawa; T. Nishida",Kyoto University; Kyoto University; Kyoto University,"2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)","20 Jul 2017","2017","","","227","230","We present a human point of gaze estimation system using corneal surface reflection and omni-directional image taken by a fish eye. Only capturing an eye image, our system entables to find where a user is looking in 360° surrounding scene image. We first generates multiple perspective scene images from an equi-rectangular image and perform registration between corneal reflection and perspective images. We then compute the point of gaze using a 3D eye model and project the point to an omni-directional image. We evaluated the robustness of registration and accuracy of PoG estimations using two indoor and five outdoor scenes, and found that gaze mapping error was 5.526[deg] on average. This result shows the potential to the marketing and outdoor training system.","","978-4-9011-2216-0","10.23919/MVA.2017.7986842","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986842","","Cameras;Three-dimensional displays;Robustness;Calibration;Pose estimation","estimation theory;eye;gaze tracking;image registration","gaze estimation;corneal surface reflection;omnidirectional camera image;fish eye;multiple perspective scene images;equi-rectangular image;image registration;corneal reflection;3D eye model;PoG estimations","","","","7","","20 Jul 2017","","","IEEE","IEEE Conferences"
"Gaze Tracking and Object Recognition from Eye Images","L. E. Hafi; M. Ding; J. Takamatsu; T. Ogasawara","Grad. Sch. of Inf. Sci., Nara Inst. of Sci. & Technol. (NAIST), Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. & Technol. (NAIST), Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. & Technol. (NAIST), Nara, Japan; Grad. Sch. of Inf. Sci., Nara Inst. of Sci. & Technol. (NAIST), Nara, Japan","2017 First IEEE International Conference on Robotic Computing (IRC)","15 May 2017","2017","","","310","315","This paper introduces a method to identify the focused object in eye images captured from a single camera in order to enable intuitive eye-based interactions using wearable devices. Indeed, eye images allow to not only obtain natural user responses from eye movements, but also the scene reflected on the cornea without the need for additional sensors such as a frontal camera, thus making it more socially acceptable. The proposed method relies on a 3D eye model reconstruction to evaluate the gaze direction from the eye images. The gaze direction is then used in combination with deep learning algorithms to classify the focused object reflected on the cornea. Finally, the experimental results using a wearable prototype demonstrate the potential of the proposed method solely based on eye images captured from a single camera.","","978-1-5090-6724-4","10.1109/IRC.2017.44","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7926555","eye model;corneal image;gaze tracking;object recognition;wearable device","Cornea;Cameras;Visualization;Solid modeling;Optical imaging;Gaze tracking;Three-dimensional displays","gaze tracking;image sensors;learning (artificial intelligence);object tracking","object recognition;gaze tracking;eye images;single camera;wearable devices;eye movements;3D eye model reconstruction;gaze direction;deep learning algorithms;object reflection;wearable prototype","","","","10","","15 May 2017","","","IEEE","IEEE Conferences"
"Gaze Estimation Based on Eyeball-Head Dynamics","I. Mitsugami; Y. Okinaka; Y. Yagi","Osaka Univ., Suita, Japan; Osaka Univ., Suita, Japan; Osaka Univ., Suita, Japan","2017 IEEE Winter Applications of Computer Vision Workshops (WACVW)","27 Apr 2017","2017","","","48","52","Human's gaze direction is a useful cue to understand his/her attention and interest. There are many kinds of eye tracking devices, but they are usually unavailable for people observed in surveillance views because they are located too far to observe their eyeballs. If we could know the gaze direction from such surveillance views, it should be very effective for many applications. Our research objective is thus to estimate the gaze direction without any eye-trackers but by observing their behaviors. This paper proposes a new gaze estimation method based on a dynamical model emulating dynamic relation between eyeball and head. Experimental results using head-mounted and body-mounted camera images confirmed its effectiveness.","","978-1-5090-4941-7","10.1109/WACVW.2017.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912208","","Head;Mathematical model;Cameras;Estimation;Surveillance;Springs;Videos","approximation theory;behavioural sciences computing;cameras;gaze tracking","eye tracking devices;surveillance views;gaze direction;gaze estimation;dynamical model;dynamic relation;eyeball;head-mounted camera images;body-mounted camera images;eyeball-head dynamics;approximation model","","2","","14","","27 Apr 2017","","","IEEE","IEEE Conferences"
"A framework for polynomial model with head pose in low cost gaze estimation","S. Rattarom; N. Aunsri; S. Uttama","School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand","2017 International Conference on Digital Arts, Media and Technology (ICDAMT)","24 Apr 2017","2017","","","24","27","This paper presents the framework for low-cost gaze estimation system to find the polynomial model used in with head pose based system. This framework uses three features - center of pupil, glint, and inner eye corner to create the polynomial model. The pupil-glint vector used to estimate eye gaze while glint-inner eye corner vector used to estimate head pose and adding inner eye corner to the model could improve the accuracy and robustness of the system. The framework consists of five steps ranging from data acquisition to testing the accuracy steps. This process introduces the validation method in order to prove the efficiency of the model.","","978-1-5090-5210-3","10.1109/ICDAMT.2017.7904927","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7904927","head pose estimation;gaze tracking;interpolation based gaze estimation;mapping function;model validation","Mathematical model;Head;Cameras;Estimation;Light sources;Calibration;Gaze tracking","feature extraction;polynomials;pose estimation","polynomial model;head pose;low-cost gaze estimation system;center-of-pupil feature;glint feature;inner eye corner feature;data acquisition;validation method","","1","","15","","24 Apr 2017","","","IEEE","IEEE Conferences"
"Gaze estimation based on head movements in virtual reality applications using deep learning","A. M. Soccini","University of Torino, Italy","2017 IEEE Virtual Reality (VR)","6 Apr 2017","2017","","","413","414","Gaze detection in Virtual Reality systems is mostly performed using eye-tracking devices. The coordinates of the sight, as well as other data regarding the eyes, are used as input values for the applications. While this trend is becoming more and more popular in the interaction design of immersive systems, most visors do not come with an embedded eye-tracker, especially those that are low cost and maybe based on mobile phones. We suggest implementing an innovative gaze estimation system into virtual environments as a source of information regarding users intentions. We propose a solution based on a combination of the features of the images and the movement of the head as an input of a Deep Convolutional Neural Network capable of inferring the 2D gaze coordinates in the imaging plane.","2375-5334","978-1-5090-6647-6","10.1109/VR.2017.7892352","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7892352","Gaze;Virtual Reality;Deep Learning;Neural Networks;CNN;ConvNet","Head;Estimation;Neural networks;Resists;Virtual environments;Two dimensional displays","convolution;gaze tracking;learning (artificial intelligence);neural nets;virtual reality","gaze estimation;head movements;virtual reality applications;deep learning;gaze detection;eye-tracking devices;immersive system interaction design;visors;deep convolutional neural network;2D gaze coordinates","","7","1","4","","6 Apr 2017","","","IEEE","IEEE Conferences"
"Head pose-free eye gaze prediction for driver attention study","Yafei Wang; Tongtong Zhao; Xueyan Ding; Jiming Bian; X. Fu","School of Physics and Optoelectronic Engineering, Dalian University of Technology, 116024, China; Information Science and Technology College, Dalian Maritime University, 116026, China; Information Science and Technology College, Dalian Maritime University, 116026, China; School of Physics and Optoelectronic Engineering, Dalian University of Technology, 116024, China; Information Science and Technology College, Dalian Maritime University, 116026, China","2017 IEEE International Conference on Big Data and Smart Computing (BigComp)","20 Mar 2017","2017","","","42","46","Driver's gaze direction is an indicator of driver state and plays a significantly role in driving safety. Traditional gaze zone estimation methods based on eye model have disadvantages due to the vulnerability under large head movement. Different from these methods, an appearance-based head pose-free eye gaze prediction method is proposed in this paper, for driver gaze zone estimation under free head movement. To achieve this goal, a gaze zone classifier is trained with head vectors and eye image features by random forest. The head vector is calculated by Pose from Orthography and Scaling with ITerations (POSIT) where a 3D face model is combined with facial landmark detection. And the eye image features are derived from eye images which extracted through eye region localization. These features are presented as the combination of sparse coefficients by sparse encoding with eye image dictionary, having good potential to carry information of the eye images. Experimental results show that the proposed method is applicable in real driving environment.","2375-9356","978-1-5090-3015-6","10.1109/BIGCOMP.2017.7881713","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881713","Driver state;Head pose-free;Random forest;Gaze zone;Dictionary learning","Dictionaries;Estimation;Feature extraction;Magnetic heads;Face;Training","driver information systems;face recognition;feature extraction;gaze tracking;image classification;iterative methods;learning (artificial intelligence);object detection;pose estimation;solid modelling;vectors","appearance-based head pose-free eye gaze prediction method;driver attention study;driver state indicator;driving safety;eye model;free head movement;driver gaze zone estimation;gaze zone classifier;head vector calculation;random forest;pose-from-orthography-and-scaling-with-iterations;POSIT;3D face model;facial landmark detection;region localization;eye image feature extraction;sparse coefficients;sparse encoding;eye image dictionary","","4","","16","","20 Mar 2017","","","IEEE","IEEE Conferences"
"Appearance-Based Gaze Estimation via Uncalibrated Gaze Pattern Recovery","F. Lu; X. Chen; Y. Sato","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan","IEEE Transactions on Image Processing","24 Feb 2017","2017","26","4","1543","1553","Aiming at reducing the restrictions due to person/scene dependence, we deliver a novel method that solves appearance-based gaze estimation in a novel fashion. First, we introduce and solve an “uncalibrated gaze pattern” solely from eye images independent of the person and scene. The gaze pattern recovers gaze movements up to only scaling and translation ambiguities, via nonlinear dimension reduction and pixel motion analysis, while no training/calibration is needed. This is new in the literature and enables novel applications. Second, our method allows simple calibrations to align the gaze pattern to any gaze target. This is much simpler than conventional calibrations which rely on sufficient training data to compute person and scene-specific nonlinear gaze mappings. Through various evaluations, we show that: 1) the proposed uncalibrated gaze pattern has novel and broad capabilities; 2) the proposed calibration is simple and efficient, and can be even omitted in some scenarios; and 3) quantitative evaluations produce promising results under various conditions.","1941-0042","","10.1109/TIP.2017.2657880","Joint Funds of NSFC-CARFC(grant numbers:U1533129); NSFC(grant numbers:61602020,61532003,61325011); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833091","Gaze estimation;uncalibrated gaze pattern;dimension reduction","Calibration;Training;Training data;Estimation;Cameras;Computational modeling;Head","computer vision;gaze tracking","appearance-based gaze estimation;uncalibrated gaze pattern recovery;nonlinear dimension reduction;pixel motion analysis;scene-specific nonlinear gaze mappings","Algorithms;Eye;Fixation, Ocular;Humans;Image Processing, Computer-Assisted;Pattern Recognition, Automated","12","","38","IEEE","25 Jan 2017","","","IEEE","IEEE Journals"
"Automated Online Exam Proctoring","Y. Atoum; L. Chen; A. X. Liu; S. D. H. Hsu; X. Liu","Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Department of Physics and Astronomy, Michigan State University, East Lansing, MI, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA","IEEE Transactions on Multimedia","15 Jun 2017","2017","19","7","1609","1624","Massive open online courses and other forms of remote education continue to increase in popularity and reach. The ability to efficiently proctor remote online examinations is an important limiting factor to the scalability of this next stage in education. Presently, human proctoring is the most common approach of evaluation, by either requiring the test taker to visit an examination center, or by monitoring them visually and acoustically during exams via a webcam. However, such methods are labor intensive and costly. In this paper, we present a multimedia analytics system that performs automatic online exam proctoring. The system hardware includes one webcam, one wearcam, and a microphone for the purpose of monitoring the visual and acoustic environment of the testing location. The system includes six basic components that continuously estimate the key behavior cues: user verification, text detection, voice detection, active window detection, gaze estimation, and phone detection. By combining the continuous estimation components, and applying a temporal sliding window, we design higher level features to classify whether the test taker is cheating at any moment during the exam. To evaluate our proposed system, we collect multimedia (audio and visual) data from $\text{24}$ subjects performing various types of cheating while taking online exams. Extensive experimental results demonstrate the accuracy, robustness, and efficiency of our online exam proctoring system.","1941-0077","","10.1109/TMM.2017.2656064","Michigan State University Targeted Support Grants for Technology Development program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7828141","Covariance feature;gaze estimation;online exam proctoring (OEP);phone detection;speech detection;text detection;user verification","Feature extraction;Monitoring;Multimedia communication;Webcams;Visualization;Sensors","audio signal processing;computer aided instruction;distance learning;object detection","automated online exam proctoring;massive open online courses;remote education;remote online examinations;human proctoring;multimedia analytics system;webcam;wearcam;microphone;visual environment;acoustic environment;user verification;text detection;voice detection;active window detection;gaze estimation;phone detection","","38","","36","IEEE","20 Jan 2017","","","IEEE","IEEE Journals"
"A Regression-Based User Calibration Framework for Real-Time Gaze Estimation","N. M. Arar; H. Gao; J. Thiran","Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland; Signal Processing Laboratory (LTS5), École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland","IEEE Transactions on Circuits and Systems for Video Technology","11 Dec 2017","2017","27","12","2623","2638","Eye movements play a very significant role in human-computer interaction (HCI) as they are natural and fast, and contain important cues for human cognitive state and visual attention. Over the last two decades, many techniques have been proposed to accurately estimate the gaze. Among these, video-based remote eye trackers have attracted much interest, since they enable nonintrusive gaze estimation. To achieve high estimation accuracies for remote systems, user calibration is inevitable in order to compensate for the estimation bias caused by person-specific eye parameters. Although several explicit and implicit user calibration methods have been proposed to ease the calibration burden, the procedure is still cumbersome and needs further improvement. In this paper, we present a comprehensive analysis of regression-based user calibration techniques. We propose a novel weighted least squares regression-based user calibration method together with a real-time cross-ratio based gaze estimation framework. The proposed system enables to obtain high estimation accuracy with minimum user effort, which leads to user-friendly HCI applications. Experimental results conducted on both simulations and user experiments show that our framework achieves a significant performance improvement over the state-of-the-art user calibration methods when only a few points are available for the calibration.","1558-2205","","10.1109/TCSVT.2016.2595322","Logitech; Swiss Commission for Technology and Innovation (CTI)(grant numbers:13594.1 PFFLR-ES); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7524006","Calibration;computer vision;gaze tracking;human computer interaction;image processing;machine vision;real-time systems;regression analysis","Calibration;Image processing;Human computer interaction;Cameras;Computer vision;Real-time systems;Regression analysis;Machine vision","calibration;eye;gaze tracking;human computer interaction;regression analysis","user calibration framework;real-time gaze estimation;eye movements;human-computer interaction;human cognitive state;visual attention;video-based remote eye trackers;nonintrusive gaze estimation;estimation bias;person-specific eye parameters;user calibration techniques;weighted least squares regression;user calibration method;cross-ratio based gaze estimation framework;user-friendly HCI applications;user calibration methods","","3","","41","IEEE","27 Jul 2016","","","IEEE","IEEE Journals"
"Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization Analysis Tasks","S. R. Gomez; R. Jianu; R. Cabeen; H. Guo; D. H. Laidlaw","Department of Computer Science, Brown University, Providence, RI; School of Computing and Information Sciences, Florida International University, Miami, FL; Department of Computer Science, Brown University, Providence, RI; Department of Computer Science, Brown University, Providence, RI; Department of Computer Science, Brown University, Providence, RI","IEEE Transactions on Visualization and Computer Graphics","29 Dec 2016","2017","23","2","1042","1055","We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.","1941-0506","","10.1109/TVCG.2016.2532331","US National Science Foundation (NSF)(grant numbers:IIS-10-16623); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7414495","Eye tracking;crowdsourcing;focus window;information visualization;visual analysis;user studies","Data visualization;Visualization;Crowdsourcing;Gaze tracking;Presses;Hardware;Layout","computer vision;crowdsourcing;gaze tracking","crowdsourcing gaze location estimation;Fauxvea;visualization analysis tasks;user behaviors;gaze fixations;cursor interactions;eye-tracking hardware;eye-tracking data;Amazon Mechanical Turk;MTurk;crowdsourcing visual analysis tasks;static information visualizations","Adult;Attention;Crowdsourcing;Eye Movement Measurements;Female;Fixation, Ocular;Humans;Internet;Male;Task Performance and Analysis","7","","31","IEEE","19 Feb 2016","","","IEEE","IEEE Journals"
"A New Calibration-Free Gaze Tracking Algorithm Based on DE-SLFA","S. Wang; J. Wang; H. Peng; S. Gao; D. He","Sch. of Telecommun. Eng., Xidian Univ., Xi'an, China; Sch. of Telecommun. Eng., Xidian Univ., Xi'an, China; Sch. of Telecommun. Eng., Xidian Univ., Xi'an, China; Sch. of Math. & Stat., Xidian Univ., Xi'an, China; Sch. of Telecommun. Eng., Xidian Univ., Xi'an, China","2016 8th International Conference on Information Technology in Medicine and Education (ITME)","13 Jul 2017","2016","","","380","384","Advanced remote gaze estimation systems use automatic calibration procedure without requiring active user involving into the estimation of subject-specific eye parameters. Though automatic calibration process can simplify the difficulty of calibration task, it still needs time to collect information for completing the eye parameters of users before the gaze tracking system is used. This paper proposes a novel method, free of calibration procedure to extract subject-specific eye parameters. To estimate the real-time angles between the optical and visual axes of each eye before calculating the direction of the visual axes of the both the left and right eyes, differential evolution and Shuffled Frog-leaping Algorithm (DE-SLFA) is used to minimize the distance between the intersections of the visual axes of the left and right eyes with the surface of a display while subjects look naturally at the display. As a consequence, the inconvenient calibration procedure which may produce possible calibration errors can be eliminated. Computer simulation have been performed to confirm the proposed method.","2474-3828","978-1-5090-3906-7","10.1109/ITME.2016.0091","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976505","Gaze tracking;Calibration-free","Calibration;Visualization;Gaze tracking;Adaptive optics;Three-dimensional displays;Cornea;Biomedical optical imaging","computer vision;estimation theory;evolutionary computation;eye;gaze tracking;minimisation","calibration-free gaze tracking algorithm;remote gaze estimation systems;automatic calibration procedure;subject-specific eye parameter estimation;subject-specific eye parameter extraction;real-time angle estimation;optical axes;visual axes;differential evolution and shuffled frog-leaping algorithm;DE-SLFA;distance minimization;computer simulation","","","","13","","13 Jul 2017","","","IEEE","IEEE Conferences"
"Crowd analytics via one shot learning and agent based inference","P. Tu; M. Chang; T. Gao",GE Global Research; GE Global Research; GE Global Research,"2016 IEEE Global Conference on Signal and Information Processing (GlobalSIP)","24 Apr 2017","2016","","","1181","1185","For the purposes of inferring social behavior in crowded conditions, three concepts have been explored: 1) the GE Sherlock system which makes use of computer vision algorithms for the purposes of the opportunistic capture of various of social cues, 2) a one shot learning paradigm where behaviors can be identified based on as few as a single example and 3) an agent based approach to inference where generative models become the basis for social behavior recognition. The Sherlock system makes use of tracking, facial analysis, gaze estimation and upper body motion analysis. The one-shot learning paradigm makes use of semantically meaningful affects as descriptors. The agent based inference methods allows for the incorporation of cognitive models as a basis for inference.","","978-1-5090-4545-7","10.1109/GlobalSIP.2016.7906028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7906028","Tracking;Expression;Agent;Inference;Learning;Crowds","Signal generators;Tracking;Cameras;Computational modeling;Face;Analytical models;Streaming media","behavioural sciences computing;computer vision;face recognition;gaze tracking;learning (artificial intelligence)","crowd analytics;agent based inference;GE Sher- lock system;computer vision;social behavior recognition;facial analysis;gaze estimation;upper body motion analysis;one-shot learning paradigm","","","","12","","24 Apr 2017","","","IEEE","IEEE Conferences"
"User interest profiling using tracking-free coarse gaze estimation","F. Bartoli; G. Lisanti; L. Seidenari; A. Del Bimbo","Media Integration and Communication Center, Università degli Studi di Firenze, Italy; Media Integration and Communication Center, Università degli Studi di Firenze, Italy; Media Integration and Communication Center, Università degli Studi di Firenze, Italy; Media Integration and Communication Center, Università degli Studi di Firenze, Italy","2016 23rd International Conference on Pattern Recognition (ICPR)","24 Apr 2017","2016","","","1839","1844","Understanding where people attention focuses is a challenging and extremely valuable task that can be solved using computer vision technologies. In this paper we address this problem on surveillance-like scenarios, where head and body imagery are usually low resolution. We propose a method to profile the attention of people moving in a known space. We exploit coarse gaze estimation and a novel model based on optical flow to improve attention prediction without the need of a tracker. Removing the tracker dependency makes the method applicable also on highly crowded scenarios. The proposed method is able to obtain comparable performance with respect to state of the art solutions in terms of Mean Average Angular Error (MAAE) on the TownCentre dataset. We also test our approach on the publicly available MuseumVisitors dataset showing an improvement both in terms of MAAE and in terms of accuracy in the estimation of visitors' profile.","","978-1-5090-4847-2","10.1109/ICPR.2016.7899904","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899904","","Head;Estimation;Feature extraction;Tracking;Cameras;Image resolution;Reliability","computer vision;gaze tracking;image sequences","user interest profiling;tracking-free coarse gaze estimation;computer vision technologies;optical flow;mean average angular error;MAAE;TownCentre dataset","","1","","16","","24 Apr 2017","","","IEEE","IEEE Conferences"
"Gaze estimation using EEG signals for HCI in augmented and virtual reality headsets","J. M. F. Montenegro; V. Argyriou","Kingston University, London, United Kingdom; Kingston University, London, United Kingdom","2016 23rd International Conference on Pattern Recognition (ICPR)","24 Apr 2017","2016","","","1159","1164","Augmented and virtual reality have evolved significantly over the last few years providing new ways of entertainment and interaction with the environment. Although many systems and solutions are currently available, still there is much left unsettled and some technologies are missing from many VR/AR devices, such as foveated rendering and HCI. In this paper, a novel approach for coarse gaze estimation using EEG sensors with applications in items selection for HCI or foveated rendering for VR/AR devices is proposed. The suggested method requires only few electroencephalogram sensors that can be easily added to the current virtual and augmented reality headsets. A supervised machine leaning approach was suggested utilising novel features, based on quaternions allowing gaze estimation. Experiments were performed to evaluate the proposed method and a new dataset was designed and captured. Finally, the introduced learning framework was compared with other similar techniques demonstrating further the gain of the proposed descriptors.","","978-1-5090-4847-2","10.1109/ICPR.2016.7899793","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7899793","","Quaternions;Electroencephalography;Sensors;Feature extraction;Tracking;Gaze tracking","augmented reality;electroencephalography;gaze tracking;human computer interaction;learning (artificial intelligence);sensors","EEG signals;gaze estimation;HCI;virtual reality headsets;augmented reality headsets;VR-AR devices;EEG sensors;item selection;electroencephalogram sensors;supervised machine leaning approach;quaternions","","3","","26","","24 Apr 2017","","","IEEE","IEEE Conferences"
"3D eye model-based gaze estimation from a depth sensor","X. Zhou; H. Cai; Z. Shao; H. Yu; H. Liu","College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Computing, University of Portsmouth, UK; College of Computer Science and Technology, Zhejiang University of Technology, Hangzhou, China; School of Creative Technologies, University of Portsmouth, UK; School of Computing, University of Portsmouth, UK","2016 IEEE International Conference on Robotics and Biomimetics (ROBIO)","2 Mar 2017","2016","","","369","374","In this paper, we address the 3D eye gaze estimation problem using a low-cost, simple-setup, and non-intrusive consumer depth sensor (Kinect sensor). We present an effective and accurate method based on 3D eye model to estimate the point of gaze of a subject with the tolerance of free head movement. To determine the parameters involved in the proposed eye model, we propose i) an improved convolution-based means of gradients iris center localization method to accurately and efficiently locate the iris center in 3D space; ii) a geometric constraints-based method to estimate the eyeball center under the constraints that all the iris center points are distributed on a sphere originated from the eyeball center and the sizes of two eyeballs of a subject are identical; iii) an effective Kappa angle calculation method based on the fact that the visual axes of both eyes intersect at a same point with the screen plane. The final point of gaze is calculated by using the estimated eye model parameters. We experimentally evaluate our gaze estimation method on five subjects. The experimental results show the good performance of the proposed method with an average estimation accuracy of 3.78°, which outperforms several state-of-the-arts.","","978-1-5090-4364-4","10.1109/ROBIO.2016.7866350","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866350","","Three-dimensional displays;Solid modeling;Iris;Estimation;Visualization;Face","computer vision;convolution;estimation theory;eye;gaze tracking;gradient methods;iris recognition;pose estimation;sensor placement;sensors","3D eye model-based gaze estimation;depth sensor;Kinect sensor;free head movement tolerance;convolution-based means;gradients iris center localization method;iris center location;3D space;geometric constraints-based method;eyeball center estimation;Kappa angle calculation method;eye visual axes;screen plane","","3","1","16","","2 Mar 2017","","","IEEE","IEEE Conferences"
"Hybrid LASSO and Neural Network estimator for gaze estimation","S. D. Iyer; H. Ramasangu","Department of Electronic and Communication Engineering, MSRUAS, Bengaluru, Karnataka 560058, India; Department of Electronic and Communication Engineering, MSRUAS, Bengaluru, Karnataka 560058, India","2016 IEEE Region 10 Conference (TENCON)","9 Feb 2017","2016","","","2579","2582","Gaze estimation has wide applications in drowsiness detection, security, and biomedical domains. The challenges in estimating the gaze angle include varying light conditions and subtle movements of the gaze. The Convolutional Neural Network (CNN) has recently been suggested as a potential method for gaze estimation. In this present work, we have proposed a gaze estimator combining a neural network and Least Absolute Shrinkage and Selection Operator (LASSO). The features considered are both eye and head features. The combined estimator neural network-LASSO (NN-LASSO) outperforms the individual performance of neural network and LASSO estimator. The results are validated using MPII Gaze dataset and it has been shown that the proposed NN-LASSO estimator outperforms CNN in mean error sense.","2159-3450","978-1-5090-2597-8","10.1109/TENCON.2016.7848503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7848503","","Estimation;Artificial neural networks;Feature extraction;Biological neural networks;Face","feature extraction;gaze tracking;neural nets","NN-LASSO estimator;MPII gaze dataset;NN-LASSO;neural network-LASSO;least absolute shrinkage and selection operator;CNN;convolutional neural network;gaze estimation;neural network estimator;hybrid LASSO","","2","1","15","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Towards a smart selection of resources for low-energy multimedia processing","S. A. Mahmoudi","Faculty of Engineering, University of Mons, Computer Science department, 20, Place du Parc., Belgium","2016 2nd International Conference on Cloud Computing Technologies and Applications (CloudTech)","9 Feb 2017","2016","","","238","245","Today, one can find images and videos everywhere, they can come from cameras, mobile phones or from other devices. These images and videos are used to illustrate different objects in a large number of situations (airports, hospitals, public areas, sport events, etc.). This makes the task of processing images and videos a necessary tool that can be used for various domains related to computer vision. The performance of these algorithms have been so reduced due to their high intensive computation and energy consumption. In this work, we propose a new framework that allows users to select in a smart and efficient way the computing units (CPU or/and GPU) in case of processing single image, multiple images, multiple videos or single video in real time. This framework enables to affect the CPU or/and GPU units for calculation depending on the type of media to process and the algorithm complexity. The framework provides several image and video functions on GPU, such as silhouette extraction, points of interest extraction, edges detection, sparse and dense optical flow estimation. These functions are exploited in different applications such as vertebra segmentation in X-ray and MR images, gaze estimation, event detection and localization in real time. Experimental results have been obtained by applying the framework for different use case applications showing a speedup ranging from 5 to 116×, by comparison with sequential CPU implementations. In addition to these performance, the parallel and heterogeneous implementation offered lower power consumption as a result for the fast treatment.","","978-1-4673-8894-8","10.1109/CloudTech.2016.7847705","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7847705","GPU;Heterogeneous architectures;Image and video processing;Medical imaging;Motion tracking","Biomedical imaging;Performance evaluation;Estimation;Cameras;Image edge detection;Hardware;Graphics processing units","computer vision;multimedia computing;video signal processing","smart resource selection;low-energy multimedia processing;image processing;video processing;computer vision;CPU units;GPU units;silhouette extraction;points of interest extraction;edge detection;sparse optical flow estimation;dense optical flow estimation","","2","","26","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Towards the development of a standardized performance evaluation framework for eye gaze estimation systems in consumer platforms","A. Kar; P. Corcoran","Center for Cognitive, Connected & Computational Imaging, College of Engineering & Informatics, NUI Galway, Ireland; Center for Cognitive, Connected & Computational Imaging, College of Engineering & Informatics, NUI Galway, Ireland","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","9 Feb 2017","2016","","","002061","002066","There is a need to standardize the performance of eye gaze estimation (EGE) methods in various platforms for human computer interaction (HCI). Because of lack of consistent schemes or protocols for summative evaluation of EGE systems, performance results in this field can neither be compared nor reproduced with any consistency. In contemporary literature, gaze tracking accuracy is measured under non-identical sets of conditions, with variable metrics and most results do not report the impact of system meta-parameters that significantly affect tracking performances. In this work, the diverse nature of these research outcomes and system parameters which affect gaze tracking in different platforms is investigated and their error contributions are estimated quantitatively. Then the concept and development of a performance evaluation framework is proposed- that can define design criteria and benchmark quality measures for the eye gaze research community.","","978-1-5090-1897-0","10.1109/SMC.2016.7844543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844543","","Cameras;Gaze tracking;Head;Tracking;Magnetic heads;Image resolution;Estimation","consumer behaviour;gaze tracking;human computer interaction;performance evaluation","standardized performance evaluation;eye gaze estimation systems;consumer platforms;human computer interaction;HCI;EGE systems;gaze tracking;system meta-parameters;eye gaze research community","","4","","40","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Comparison of two techniques for gaze estimation system using the direction of eyes and head","K. Sakurai; M. Yan; H. Tamura; K. Tanno","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Japan; Faculty of Engineering, University of Miyazaki, Japan; Faculty of Engineering, University of Miyazaki, Japan; Faculty of Engineering, University of Miyazaki, Japan","2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)","9 Feb 2017","2016","","","002466","002471","Binocular eye-gaze tracking can be used to estimate gaze of a subject in space using the vergence of the eyes. We analyze eye movement and head movement. As for the eye movement, we used the TalkEye which is eye tracking device. The head angle determined by template matching using a camera attached to TalkEye. A purpose of this study is to compare the proposal method using TalkEye with the conventional method to perform a high-precision gaze estimate. Conventional method is a method of using the Electrooculogram (EOG) signal and RGB-D sensor. Experiments were performed in both indoor and outdoor. The subjects could move eyes and a head freely in wide space. From the experiments, we show the result of 30 degrees, 60 degrees, -30 degrees, and -60 degrees gaze estimation is possible with high precision. In addition, we compared proposal technique and the conventional technique using a regression analysis and considered the effectiveness.","","978-1-5090-1897-0","10.1109/SMC.2016.7844609","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7844609","Gaze Estimation;Eye Tracker;Template Matching;Electrooculogram Signal;RGB-D Sensor","Electrooculography;Head;Estimation;Cameras;Tracking;Proposals;Eye protection","gaze tracking;image matching","gaze estimation system;TalkEye;eye tracking device;template matching;binocular eye-gaze tracking;eye movement analysis;head movement analysis","","1","","11","","9 Feb 2017","","","IEEE","IEEE Conferences"
"Gaze estimation using 3-D eyeball model and eyelid shapes","S. Y. Han; I. Hwang; S. H. Lee; N. I. Cho","Institute of New Media and Communications, Department of Electrical and Computer Engineering of Seoul National University, Seoul, Republic of Korea; Institute of New Media and Communications, Department of Electrical and Computer Engineering of Seoul National University, Seoul, Republic of Korea; Institute of New Media and Communications, Department of Electrical and Computer Engineering of Seoul National University, Seoul, Republic of Korea; Institute of New Media and Communications, Department of Electrical and Computer Engineering of Seoul National University, Seoul, Republic of Korea","2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","19 Jan 2017","2016","","","1","4","This paper proposes a gaze estimation algorithm using 3-D eyeball model and eyelid shape. The gaze estimation suffers from differences of eye shapes and individual behaviors, and requires user-specific gaze calibration. The proposed method exploits the usual 3-D eyeball model and shapes of the eyelid to estimate gaze without user-specific calibration and learning. Since the gaze is closely related to the 3-D rotation of eyeball, this paper first derives the relation between 2-D pupil location extracted in the eye image and 3-D rotation of eyeball. This paper also models the shapes of the eyelid to adjust gaze based on the observation that the shapes of the eyelid are deformed with respect to the gaze. This paper models the curvature of eyelid curve to compensate for the gaze. According to the various experiments, the proposed method shows good results in gaze estimation. The proposed method does not need user-specific calibration or gaze learning since the general 3-D eyeball and eyelid models are exploited in the localized eye region. Therefore, it is expected that the proposed gaze estimation algorithm is suitable for various applications such as VR/AR devices, driver gaze tracking, gaze-based interfaces, and so on.","","978-9-8814-7682-1","10.1109/APSIPA.2016.7820784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820784","","Eyelids;Estimation;Shape;Solid modeling;Image edge detection;Calibration;Gaze tracking","calibration;gaze tracking;shape recognition","gaze estimation;3D eyeball model;eyelid shape;user-specific gaze calibration;eyeball 3D rotation;2D pupil location extraction;eye image","","2","","6","","19 Jan 2017","","","IEEE","IEEE Conferences"
"Towards gaze-based video annotation","M. Soliman; H. R. Tavakoli; J. Laaksonen",Aalto University; Aalto University; Aalto University,"2016 Sixth International Conference on Image Processing Theory, Tools and Applications (IPTA)","19 Jan 2017","2016","","","1","5","This paper presents our efforts towards a framework for video annotation using gaze. In computer vision, video annotation (VA) is an essential step in providing a ground truth for the evaluation of object detection and tracking techniques. VA is a demanding element in the development of video processing algorithms, where each object of interest should be manually labelled. Although the community has handled VA for a long time, the size of new data sets and the complexity of the new tasks pushes us to revisit it. A barrier towards automated video annotation is the recognition of the object of interest and tracking it over image sequences. To tackle this problem, we employ the concept of visual attention for enhancing video annotation. In an image, human attention naturally grasps interesting areas that provide valuable information for extracting the objects of interest, which can be exploited to annotate videos. Under task-based gaze recording, we utilize an observer's gaze to filter seed object detector responses in a video sequence. The filtered boxes are then passed to an appearance-based tracking algorithm. We evaluate the gaze usefulness by comparing the algorithm with gaze and without it. We show that eye gaze is an influential cue for enhancing the automated video annotation, improving the annotation significantly.","2154-512X","978-1-4673-8910-5","10.1109/IPTA.2016.7821028","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7821028","Video annotation;Video processing;Visual attenstion;Eye gaze;Object detection and tracking","Detectors;Observers;Face;Object detection;Manuals;Motion pictures;Pipelines","computer vision;gaze tracking;image annotation;image filtering;image sequences;object detection;object recognition;object tracking;video signal processing","appearance-based tracking algorithm;video sequence;task-based gaze recording;object extraction;video annotation enhancement;visual attention concept;image sequence;object recognition;video processing algorithm;object tracking technique;object detection;VA;computer vision;gaze-based video annotation","","2","","29","","19 Jan 2017","","","IEEE","IEEE Conferences"
"Developing an application using eye tracker","D. Venugopal; J. Amudha; C. Jyotsna","Dept. of Computer Science & Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Amrita University, India; Dept. of Computer Science & Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Amrita University, India; Dept. of Computer Science & Engineering, Amrita School of Engineering, Bengaluru, Amrita Vishwa Vidyapeetham, Amrita University, India","2016 IEEE International Conference on Recent Trends in Electronics, Information & Communication Technology (RTEICT)","9 Jan 2017","2016","","","1518","1522","Eye tracking measures where the eye is focused or the movement of eye with respect to the head. The eye tracker will track the eye positions and eye movement for the visual stimulus presented on the computer system. Various features like gaze point, pupil size and mouse position can be extracted and it can be represented using visualization techniques such as fixation, saccade, scanpath and heat map. The features obtained from eye tracker can be extended to real life applications. Using this technology companies could be able to analyze thousands of customer's eye patterns in real-time, and make decisions on marketing based on the data. The technology can analyze the stress level of patients, employees in IT, BPO, accounting, banking, front office etc. Here we are illustrating the advantages and applications of eye tracking, its usability and how to develop an application using a commercial eye tracker.","","978-1-5090-0774-5","10.1109/RTEICT.2016.7808086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7808086","eye-tracking;gaze estimation;calibration;feature extraction;visualization","Gaze tracking;Calibration;Data visualization;Tracking;Feature extraction;Mice;Computers","data visualisation;feature extraction;gaze tracking;image motion analysis","eye tracker;eye position tracking;eye movement;visual stimulus;computer system;gaze point;pupil size;mouse position;feature extraction;visualization techniques;fixation;saccade;scanpath;heat map;eye patterns","","14","","15","","9 Jan 2017","","","IEEE","IEEE Conferences"
"An accurate eye pupil localization approach based on adaptive gradient boosting decision tree","D. Tian; G. He; J. Wu; H. Chen; Y. Jiang","School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Dongchuan Road 800, Minhang District, Shanghai, 200240, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Dongchuan Road 800, Minhang District, Shanghai, 200240, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Dongchuan Road 800, Minhang District, Shanghai, 200240, China; Electric Power Research Institute, State Grid Shanghai Electric Power Company Shanghai, 200093, China; Electric Power Research Institute, State Grid Shanghai Electric Power Company Shanghai, 200093, China","2016 Visual Communications and Image Processing (VCIP)","5 Jan 2017","2016","","","1","4","Eye pupil localization is an important part in computer vision applications such as face recognition, gaze estimation and so on. In this paper, we propose an improved method for precise and fast eye pupil localization. Based on gradient boosting decision tree(GBDT) algorithm, a more accurate localization is achieved by increasing the weight of the training samples with larger errors in a moderate rate. Furthermore, a pruning strategy is utilized to avoid overfitting and reduce the localization time without accuracy loss. Experimental results show that the improved method achieves an accuracy of 92.39% at a speed as fast as 1.7ms to locate in the range of eye pupil on BioID database. The proposed method outperforms most state-of-the-art methods in terms of localization accuracy and consumed time.","","978-1-5090-5316-2","10.1109/VCIP.2016.7805483","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7805483","computer vision;eye pupil localization;gradient boosting;adaptive;pruning method","Training;Boosting;Prediction algorithms;Databases;Decision trees;Feature extraction;Face","computer vision;decision trees;gaze tracking","eye pupil localization;computer vision applications;gradient boosting decision tree algorithm;GBDT algorithm;training samples;pruning strategy;BioID database","","8","","12","","5 Jan 2017","","","IEEE","IEEE Conferences"
"Low-cost eye-tracking glasses with real-time head rotation compensation","Y. Wang; H. Zeng; J. Liu","School of Instrument Science and Engineering, Southeast University, Nanjing 210096, China; School of Instrument Science and Engineering, Southeast University, Nanjing 210096, China; Jiangsu Collaborative Innovation Center of Atmospheric Environment and Equipment Technology (CICAEET), Nanjing University of Information Science & Technology, Jiangsu 210044","2016 10th International Conference on Sensing Technology (ICST)","26 Dec 2016","2016","","","1","5","Nowadays capturing the image of eyeball with camera and estimating gaze points using pupil positions is the most common method for eye-tracking devices. Head-mounted eye-tracking equipment has a wide range of application because of its flexibility and high gaze estimation accuracy. But the user's head should keep still using a chin rest or other tools, or will lead to enormous gaze estimation errors. In this paper, a head rotation compensation method is proposed for head-mounted eye-tracking glasses. The whole cost of our system is 38 USD. According to our preliminary result, the mean error of gaze estimation is reduced from 8.0162° to 0.5748°, which shows that our method can compensate head rotation effectively. The data in the task based experiment reveal that our system can be used as an alternative input device to mouse.","2156-8073","978-1-5090-0796-7","10.1109/ICSensT.2016.7796336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7796336","eye-tracking;head-rotation compensation;human-computer interface;gaze estimation","Head;Cameras;Estimation;Geometry;Calibration;Hardware;Glass","gaze tracking;human computer interaction;motion compensation;user interfaces","human computer interface;gaze estimation;head-mounted eye-tracking glasses;real-time head rotation compensation;low-cost eye-tracking glasses","","1","","13","","26 Dec 2016","","","IEEE","IEEE Conferences"
"Analyzing the relationship between head pose and gaze to model driver visual attention","S. Jha; C. Busso","Multimodal Signal Processing Lab at the University of Texas at Dallas, Richardson, 75080, USA; Multimodal Signal Processing Lab at the University of Texas at Dallas, Richardson, 75080, USA","2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)","26 Dec 2016","2016","","","2157","2162","Monitoring driver behavior is crucial in the design of advanced driver assistance systems (ADAS) that can detect driver actions, providing necessary warnings when not attentive to driving tasks. The visual attention of a driver is an important aspect to consider, as most driving tasks require visual resources. Previous work has investigated algorithms to detect driver visual attention by tracking the head or eye movement. While tracking pupil can give an accurate direction of visual attention, estimating gaze on vehicle environment is a challenging problem due to changes in illumination, head rotations, and occlusions (e.g. hand, glasses). Instead, this paper investigates the use of the head pose as a coarse estimate of the driver visual attention. The key challenge is the non-trivial relation between head and eye movements while glancing to a target object, which depends on the driver, the underlying cognitive and visual demand, and the environment. First, we evaluate the performance of a state-of-the-art head pose detection algorithm over natural driving recordings, which are compared with ground truth estimations derived from AprilTags attached to a headband. Then, the study proposes regression models to estimate the drivers' gaze based on the head position and orientation, which are built with data from natural driving recordings. The proposed system achieves high accuracy over the horizontal direction, but moderate/low performance over the vertical direction. We compare results while our participants were driving, and when the vehicle was parked.","2153-0017","978-1-5090-1889-5","10.1109/ITSC.2016.7795905","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7795905","","Visualization;Cameras;Magnetic heads;Head;Roads;Automobiles","driver information systems;gaze tracking;image motion analysis;object detection;pose estimation;regression analysis","driver visual attention;driver behavior monitoring;advanced driver assistance systems;ADAS;driver actions detection;driving tasks;visual resources;head movements;eye movements;cognitive demand;visual demand;head pose detection;regression models;driver gaze estimation;head position;head orientation;natural driving recordings","","11","","21","","26 Dec 2016","","","IEEE","IEEE Conferences"
"On driver gaze estimation: Explorations and fusion of geometric and data driven approaches","B. Vasli; S. Martin; M. M. Trivedi","Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, La Jolla, 92092, USA; Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, La Jolla, 92092, USA; Laboratory for Intelligent and Safe Automobiles (LISA), University of California San Diego, La Jolla, 92092, USA","2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)","26 Dec 2016","2016","","","655","660","Gaze direction is important in a number of applications such as active safety and driver's activity monitoring. However, there are challenges in estimating gaze robustly in real world driving situations. While performance of personalized gaze estimation models has improved significantly, performance improvement of universal gaze estimation is lagging behind; one reason being, learning based methods do not exploit the physical constraints of the car. In this paper, we propose a system to estimate driver's gaze from head and eye cues projected on a multi-plane geometrical environment and a system which fuses the geometric with data driven learning method. Evaluations are conducted on naturalistic driving data containing different drivers in different vehicles in order to test the generalization of the methods. Systematic evaluations on this data set are presented for the proposed geometric based gaze estimation method and geometric plus learning based hybrid gaze estimation framework, where exploiting the geometrical constraints of the car shows promising results of generalization.","2153-0017","978-1-5090-1889-5","10.1109/ITSC.2016.7795623","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7795623","In Cabin Activity Analysis;Human-vehicle Interaction;Gaze Estimation;Take-over;Highly Automated Vehicles","Estimation;Automobiles;Head;Learning systems;Solid modeling;Mirrors","automobiles;driver information systems;gaze tracking;geometry;image motion analysis;learning (artificial intelligence);road safety;road traffic","driver gaze estimation;geometric approaches;gaze direction;active safety;driver activity monitoring;real world driving situations;personalized gaze estimation models;universal gaze estimation;multiplane geometrical environment;data driven learning;naturalistic driving data;geometric based gaze estimation;geometric plus learning;hybrid gaze estimation;geometrical constraints;car","","9","","18","","26 Dec 2016","","","IEEE","IEEE Conferences"
"Person-Independent 3D Gaze Estimation Using Face Frontalization","L. A. Jeni; J. F. Cohn","Carnegie Mellon Univ., Pittsburgh, PA, USA; Univ. of Pittsburgh, Pittsburgh, PA, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","19 Dec 2016","2016","","","792","800","Person-independent and pose-invariant estimation of eye-gaze is important for situation analysis and for automated video annotation. We propose a fast cascade regression based method that first estimates the location of a dense set of markers and their visibility, then reconstructs face shape by fitting a part-based 3D model. Next, the reconstructed 3D shape is used to estimate a canonical view of the eyes for 3D gaze estimation. The model operates in a feature space that naturally encodes local ordinal properties of pixel intensities leading to photometric invariant estimation of gaze. To evaluate the algorithm in comparison with alternative approaches, three publicly-available databases were used, Boston University Head Tracking, Multi-View Gaze and CAVE Gaze datasets. Precision for head pose and gaze averaged 4 degrees or less for pitch, yaw, and roll. The algorithm outperformed alternative methods in both datasets.","2160-7516","978-1-5090-1437-8","10.1109/CVPRW.2016.104","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7789594","","Three-dimensional displays;Face;Shape;Estimation;Two dimensional displays;Solid modeling","face recognition;gaze tracking;image reconstruction;shape recognition","person-independent 3D gaze estimation;face frontalization;pose-invariant estimation;eye-gaze;automated video annotation;situation analysis;3D shape reconstruction;Boston University head tracking;multiview gaze datasets","","11","","51","","19 Dec 2016","","","IEEE","IEEE Conferences"
"Learning Reconstruction-Based Remote Gaze Estimation","P. Yu; J. Zhou; Y. Wu","Northwestern Univ., Evanston, IL, USA; Northwestern Univ., Evanston, IL, USA; Northwestern Univ., Evanston, IL, USA","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","12 Dec 2016","2016","","","3447","3455","It is a challenging problem to accurately estimate gazes from low-resolution eye images that do not provide fine and detailed features for eyes. Existing methods attempt to establish the mapping between the visual appearance space to the gaze space. Different from the direct regression approach, the reconstruction-based approach represents appearance and gaze via local linear reconstruction in their own spaces. A common treatment is to use the same local reconstruction in the two spaces, i.e., the reconstruction weights in the appearance space are transferred to the gaze space for gaze reconstruction. However, this questionable treatment is taken for granted but has never been justified, leading to significant errors in gaze estimation. This paper is focused on the study of this fundamental issue. It shows that the distance metric in the appearance space needs to be adjusted, before the same reconstruction can be used. A novel method is proposed to learn the metric, such that the affinity structure of the appearance space under this new metric is as close as possible to the affinity structure of the gaze space under the normal Euclidean metric. Furthermore, the local affinity structure invariance is utilized to further regularize the solution to the reconstruction weights, so as to obtain a more robust and accurate solution. Effectiveness of the proposed method is validated and demonstrated through extensive experiments on different subjects.","1063-6919","978-1-4673-8851-1","10.1109/CVPR.2016.375","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780744","","Estimation;Image reconstruction;Training;Measurement;Head;Visualization;Erbium","gaze tracking;image reconstruction;image resolution;learning (artificial intelligence)","reconstruction-based remote gaze estimation learning;low-resolution eye images;visual appearance space;gaze space;appearance representation;gaze representation;local linear reconstruction;local reconstruction;reconstruction weights;appearance space;distance metric;affinity structure;normal Euclidean metric;local affinity structure invariance","","2","","25","","12 Dec 2016","","","IEEE","IEEE Conferences"
"A Deeper Look at Saliency: Feature Contrast, Semantics, and Beyond","N. D. B. Bruce; C. Catton; S. Janjic","Univ. of Manitoba, Winnipeg, MB, Canada; Univ. of Manitoba, Winnipeg, MB, Canada; Univ. of Manitoba, Winnipeg, MB, Canada","2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","12 Dec 2016","2016","","","516","524","In this paper we consider the problem of visual saliency modeling, including both human gaze prediction and salient object segmentation. The overarching goal of the paper is to identify high level considerations relevant to deriving more sophisticated visual saliency models. A deep learning model based on fully convolutional networks (FCNs) is presented, which shows very favorable performance across a wide variety of benchmarks relative to existing proposals. We also demonstrate that the manner in which training data is selected, and ground truth treated is critical to resulting model behaviour. Recent efforts have explored the relationship between human gaze and salient objects, and we also examine this point further in the context of FCNs. Close examination of the proposed and alternative models serves as a vehicle for identifying problems important to developing more comprehensive models going forward.","1063-6919","978-1-4673-8851-1","10.1109/CVPR.2016.62","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780431","","Visualization;Machine learning;Predictive models;Data models;Benchmark testing;Training;Object segmentation","gaze tracking;image segmentation;learning (artificial intelligence);neural nets","feature contrast;visual saliency modeling;human gaze prediction;salient object segmentation;deep learning model;fully convolutional networks;FCN","","28","","37","","12 Dec 2016","","","IEEE","IEEE Conferences"
"Gaze Estimation Method Based on EOG Signals","Y. Bei; Y. Sichun; L. Mengke; L. Xiangkun","Sch. of Autom. Sci. & Electr. Eng., Beihang Univ., Beijing, China; Sch. of Autom. Sci. & Electr. Eng., Beihang Univ., Beijing, China; Sch. of Autom. Sci. & Electr. Eng., Beihang Univ., Beijing, China; Sch. of Autom. Sci. & Electr. Eng., Beihang Univ., Beijing, China","2016 Sixth International Conference on Instrumentation & Measurement, Computer, Communication and Control (IMCCC)","8 Dec 2016","2016","","","443","446","Electrooculogram (EOG) is an important bio-electricity signal generated by physical activities of eyeballs. Information about gaze position can be obtained by analyzing EOG signals, which can lay a foundation for a novel way of human-computer interaction (HCI) based on EOG signals. In this paper, we propose a gaze estimation method based on EOG signals. After filtering and amplifying the acquired EOG signals, framing and normalization is done to make the signals easier to classify. The wavelet coefficients are extracted as the feature vectors, and BP classifier is trained to classify EOG signals of different directions. The results show that this method works well with different people and the average accuracy achieves 96%.","","978-1-5090-1195-7","10.1109/IMCCC.2016.51","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774817","gaze estimation method;EOG signal;wavelet coefficient;BP neural work","Electrooculography;Feature extraction;Wavelet coefficients;Estimation;Low-pass filters;Electrodes","backpropagation;electro-oculography;feature extraction;filtering theory;gaze tracking;human computer interaction;medical signal detection;medical signal processing;signal classification;wavelet transforms","BP classifier;feature vectors;wavelet coefficient extraction;signal classification;signal normalization;signal framing;signal filtering;acquired EOG signal amplification;HCI;human-computer interaction;eyeball physical activities;bioelectricity signal generation;electrooculogram;gaze position estimation method","","","","12","","8 Dec 2016","","","IEEE","IEEE Conferences"
"Mouse calibration aided real-time gaze estimation based on boost Gaussian Bayesian learning","N. Ye; X. Tao; L. Dong; N. Ge","Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, P. R. China; Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, P. R. China; Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, P. R. China; Tsinghua National Laboratory for Information Science and Technology (TNList), Beijing, P. R. China","2016 IEEE International Conference on Image Processing (ICIP)","8 Dec 2016","2016","","","2797","2801","In this paper, we propose a novel gaze estimation method to evaluate the attention span of users upon on-screen content via a single webcam. Our method is based on supervised descent method for eye region of interest (ROI) extraction. Then, boost Gaussian Bayesian regressors are applied to learn a robust mapping from the input eye ROI to gaze coordinates. To get enough training samples, we implant our scheme as a plug-in into web browsers for data collection from users without bothering. To improve accuracy, we also introduce mouse click to help train the regressors. Experiment results show that our method outperforms the existing method and can provide gaze estimation data for user behaviour analysis in real-time implementation.","2381-8549","978-1-4673-9961-6","10.1109/ICIP.2016.7532869","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532869","Gaze estimation;single webcam;boost Gaussian Bayesian learning","Estimation;Bayes methods;Training;Lighting;Webcams;Calibration;Mice","Bayes methods;behavioural sciences computing;belief networks;Gaussian processes;gaze tracking;image classification;learning (artificial intelligence);mouse controllers (computers);online front-ends","user behaviour analysis;mouse click;data collection;plug-ins;Web browsers;gaze coordinates;input eye ROI;robust mapping learning;boost Gaussian Bayesian regressors;eye ROI extraction;eye region-of-interest extraction;supervised descent method;webcam;on-screen content;user attention span evaluation;boost Gaussian Bayesian learning;mouse calibration aided real-time gaze estimation","","2","","24","","8 Dec 2016","","","IEEE","IEEE Conferences"
"Local Binary Pattern Histogram features for on-screen eye-gaze direction estimation and a comparison of appearance based methods","C. M. Yilmaz; C. Kose","Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey; Department of Computer Engineering, Karadeniz Technical University, Trabzon, Turkey","2016 39th International Conference on Telecommunications and Signal Processing (TSP)","1 Dec 2016","2016","","","693","696","Human Computer Interaction (HCI) has become an important focus of both computer science researches and industrial applications. And, on-screen gaze estimation is one of the hottest topics in this rapidly growing field. Eye-gaze direction estimation is a sub-research area of on-screen gaze estimation and the number of studies that focused on the estimation of on-screen gaze direction is limited. Due to this, various appearance-based video-oculography methods are investigated in this work. Firstly, a new dataset is created via user images taken from daylight censored cameras located at computer screen. Then, Local Binary Pattern Histogram (LBPH), which is used in this work for the first time to obtain on-screen gaze direction information, and Principal Component Analysis (PCA) methods are employed to extract image features. And, parameter optimized Support Vector Machine (SVM), Artificial Neural Networks (ANNs) and k-Nearest Neighbor (k-NN) learning methods are adopted in order to estimate on-screen gaze direction. Finally, these methods' abilities to correctly estimate the on-screen gaze direction are compared using the resulting classification accuracies of applied methods and previous works. The best classification accuracy of 96.67% is obtained when using LBPH and SVM method pair which is better than previous works. The results also show that appearance based methods are pretty applicable for estimating on-screen gaze direction.","","978-1-5090-1288-6","10.1109/TSP.2016.7760973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7760973","Human computer interaction;gaze estimation;gaze direction estimation;video-oculography;appearance-based methods","Feature extraction;Estimation;Principal component analysis;Support vector machines;Histograms;Cameras;Computers","feature extraction;gaze tracking;human computer interaction;neural nets;principal component analysis;support vector machines","k-nearest neighbor learning methods;ANN;artificial neural networks;SVM;support vector machine;image feature extraction;principal component analysis methods;appearance-based video-oculography methods;on-screen gaze estimation;human computer interaction;on-screen eye-gaze direction estimation;local binary pattern histogram features","","1","1","16","","1 Dec 2016","","","IEEE","IEEE Conferences"
"Robot reading human gaze: Why eye tracking is better than head tracking for human-robot collaboration","O. Palinko; F. Rea; G. Sandini; A. Sciutti","Robotics, Brain and Cognitive Sciences Department of the Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Robotics, Brain and Cognitive Sciences Department of the Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Robotics, Brain and Cognitive Sciences Department of the Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Robotics, Brain and Cognitive Sciences Department of the Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy","2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","1 Dec 2016","2016","","","5048","5054","Robots are at the position to become our everyday companions in the near future. Still, many hurdles need to be cleared to achieve this goal. One of them is the fact that robots are still not able to perceive some important communication cues naturally used by humans, e.g. gaze. In the recent past, eye gaze in robot perception was substituted by its proxy, head orientation. Such an approach is still adopted in many applications today. In this paper we introduce performance improvements to an eye tracking system we previously developed and use it to explore if this approximation is appropriate. More precisely, we compare the impact of the use of eye- or head-based gaze estimation in a human robot interaction experiment with the iCub robot and naïve subjects. We find that the possibility to exploit the richer information carried by eye gaze has a significant impact on the interaction. As a result, our eye tracking system allows for a more efficient human-robot collaboration than a comparable head tracking approach, according to both quantitative measures and subjective evaluation by the human participants.","2153-0866","978-1-5090-3762-9","10.1109/IROS.2016.7759741","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7759741","","Gaze tracking;Head;Magnetic heads;Robot kinematics;Visualization;Cameras","gaze tracking;humanoid robots;human-robot interaction;pose estimation;robot vision","quantitative measures;human-robot collaboration;iCub robot;human robot interaction;head-based gaze estimation;eye-based gaze estimation;eye tracking system;performance improvements;head orientation;robot proxy;eye gaze;robot perception;communication cues;head tracking;robot human gaze reading","","31","","15","","1 Dec 2016","","","IEEE","IEEE Conferences"
"Driver's distraction detection based on gaze estimation","S. Maralappanavar; R. Behera; U. Mudenagudi","Department of Electronics and Communication, B.V.B College of Engineeering, Hubli, India; KPIT Technologies Ltd., Pune, India; Department of Electronics and Communication, B.V.B College of Engineeering, Hubli, India","2016 International Conference on Advances in Computing, Communications and Informatics (ICACCI)","3 Nov 2016","2016","","","2489","2494","Drivers easily get distracted by the activities happening around them such as texting, talking on mobile phone or talking to the neighbouring person. All these activities take driver's attention away from the road which may lead to accidents, cause harm to the driver, pedestrians and also other vehicles on the road. In this paper, a method is proposed to estimate the gaze of the driver and determine whether the driver is distracted or not. Driver's gaze direction is estimated as an indicator of his attentiveness. The driver's gaze estimation is done by detecting the gaze with the help of face, eye, pupil, eye corners and then the detected gaze is then categorized as whether the driver is distracted or not. The algorithm is developed in OpenCV and tested on a CPU platform (Intel core with 4 GB RAM). The processing time taken for the execution of a single frame is around one second. The gaze detection accuracy obtained is 75%.","","978-1-5090-2029-4","10.1109/ICACCI.2016.7732431","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7732431","Driver's Distraction;Gaze Estimation;Viola-Jones;Eye detection","Vehicles;Face;Image color analysis;Face detection;Estimation;Feature extraction;Skin","behavioural sciences computing;gaze tracking;traffic engineering computing","driver distraction detection;pedestrians;driver attention;driver gaze direction;driver gaze estimation;OpenCV;CPU platform;Intel core","","4","","19","","3 Nov 2016","","","IEEE","IEEE Conferences"
"Detection of drivers visual attention using smartphone","D. Xiao; C. Feng","College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China","2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)","24 Oct 2016","2016","","","630","635","A lot of traffic accidents are caused by driver distraction, while most existing driver attention assistance solutions need specialized hardware, which limit their wide accessibility. We present a convenient driver attention detecting system based on smartphones with dual cameras. Firstly, a feature detector describing pupil location, yaw and pitch angle of eyes, position and size of face detected with the front camera is proposed to track drivers gaze direction. Secondly, nine safe gaze areas and one unsafe gaze area are defined. Then a SVM classifier is used to estimate gaze area. At the same time, motion objects in the view of road are detected with the rear camera using optical flow methodology combined dynamic background compensation. At last, an inference module is proposed to quantify the level of drivers visual attention by inferring whether the motion objects are located in the drivers gaze areas. Experimental results on an android pad show that the proposed system can detect driver visual attention in real driving scenarios efficiently.","","978-1-5090-4093-3","10.1109/FSKD.2016.7603247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7603247","Visual attention;Driver assistance;Dual camera;Gaze estimate;Optical flow;Dynamic background compensation","Cameras;Feature extraction;Face;Automobiles;Support vector machines;Hardware","cameras;driver information systems;face recognition;feature extraction;image classification;image motion analysis;image sequences;inference mechanisms;road accidents;support vector machines","driver visual attention detection system;smart phone;traffic accidents;driver attention assistance solutions;feature detector;dual cameras;face detection;front camera;SVM classifier;gaze area estimation;inference module;motion objects;Android pad;optical flow methodology;dynamic background compensation","","3","","17","","24 Oct 2016","","","IEEE","IEEE Conferences"
"A study on gaze estimation system using the direction of eyes and face","K. Sakurai; M. Yan; H. Tamura; K. Tanno","Interdisciplinary Graduate School of Agriculture and Engineering, University of Miyazaki, Japan; Faculty of Engineering, University of Miyazaki, Japan; Faculty of Engineering, University of Miyazaki, Japan; Faculty of Engineering, University of Miyazaki, Japan","2016 World Automation Congress (WAC)","6 Oct 2016","2016","","","1","6","The aim of this study is to present electrooculogram signals and Kinect sensor (RGB-D sensor) that can be used as a human interface efficiently. The gaze estimation using eyes and face is also important in study for preventing traffic accidents. In this paper, we introduce eye movement tracking system using cross-channels electrooculogram signals and RGB-D sensor for face tracking. Thus, gaze estimation system can be established by both eye movement and face tracking. In order to confirm the effectiveness of our proposal system, we tried the experiments in an environment in which the experimental subjects can move freely eyes and head. As the results of the experiments, angle of 30°, 60°,-30°,-60° gaze estimation is recognized with high accuracy in our system. In addition, the identification rate of each angle is also high accuracy.","","978-1-8893-3551-3","10.1109/WAC.2016.7582969","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582969","Gaze Estimation;Electrooculogram Signal;RGB-D Sensor","Electrooculography;Face;Cameras;Engines;Tracking;Integrated optics;Stimulated emission","electro-oculography;gaze tracking;interactive systems;medical signal processing;user interfaces","gaze estimation system;eye movement tracking;face tracking;electrooculogram signal;EOG signal;Kinect sensor;RGB-D sensor;human interface","","","","8","","6 Oct 2016","","","IEEE","IEEE Conferences"
"3-D gaze estimation by stereo gaze direction","W. Pichitwong; K. Chamnongthai","Dept. Electronic and Telecommunication Engineering, King Mongkut's University of Technology Thonburi, Bangkok, Thailand; dept. Electronic and Telecommunication Engineering, King Mongkut's University of Technology Thonburi, Bangkok, Thailand","2016 13th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)","8 Sep 2016","2016","","","1","4","The propose of this study aims to use eyeball model approach and algebra-geometric to analyze 3-D gaze estimation. Which stereo gaze directions calculate from 3-D coordinates of the value of Cornea Center and Pupil Center of left and right eyes. Then stereo gaze directions are used to get point of gaze (POG) of 3-D coordination. The experimental is designed in three conditions, (i) no head movement (ii) yaw head movement and (iii) roll head movement. The results from simulation of 3-D gaze estimation show POG accuracy.","","978-1-4673-9749-0","10.1109/ECTICon.2016.7561491","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7561491","3-D gaze estimation;stereo gaze direction","Cornea;Estimation;Solid modeling;Visualization;Testing;Analytical models;Face","eye;gaze tracking;stereo image processing","3D gaze estimation;stereo gaze direction;eyeball model approach;algebraic-geometry;cornea center;pupil center;point of gaze;POG","","2","","11","","8 Sep 2016","","","IEEE","IEEE Conferences"
"Improvement of eye detection performance for inside-out camera","W. Chinsatitf; T. Saitoh","Kyushu Institute of Technology, Iizuka, Japan; Kyushu Institute of Technology, Iizuka, Japan","2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)","25 Aug 2016","2016","","","1","6","This paper presents a fast and precious eye detection technique by using gradient value for improve the performance of inside-out camera. The propose methods using gradient vector to estimate a temporary pupil position before using ellipse fitting to detect actual eye position. We collected 280 eye images that capture from our inside-out camera, and apply the proposed method and previous method to the dataset. The experimental results show that the propose method is improving performance of wearable gaze estimation system.","","978-1-5090-0806-3","10.1109/ICIS.2016.7550794","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7550794","Eye image;gaze estimation;eye detection;gradient value;inside-out camera","Cameras;Image edge detection;Estimation;Mathematical model;Glass;Gradient methods;Calibration","eye;gaze tracking;gradient methods;image sensors;object detection","eye detection performance;inside-out camera;gradient vector;temporary pupil position;ellipse fitting;actual eye position;wearable gaze estimation system","","","","13","","25 Aug 2016","","","IEEE","IEEE Conferences"
"Visual salience and priority estimation for locomotion using a deep convolutional neural network","N. Anantrasirichai; I. D. Gilchrist; D. R. Bull","Bristol Vision Institute, University of Bristol, Bristol BS8 1UB, UK; Bristol Vision Institute, University of Bristol, Bristol BS8 1UB, UK; Bristol Vision Institute, University of Bristol, Bristol BS8 1UB, UK","2016 IEEE International Conference on Image Processing (ICIP)","19 Aug 2016","2016","","","1599","1603","This paper presents a novel method of salience and priority estimation for the human visual system during locomotion. This visual information contains dynamic content derived from a moving viewpoint. The priority map, ranking key areas on the image, is created from probabilities of gaze fixations, merged from bottom-up features and top-down control on the locomotion. Two deep convolutional neural networks (CNNs), inspired by models of the primate visual system, are employed to capture local salience features and compute probabilities. The first network operates through the foveal and peripheral areas around the eye positions. The second network obtains the importance of fixated points that have long durations or multiple visits, of which such areas need more times to process or to recheck to ensure smooth locomotion. The results show that our proposed method outperforms the state-of-the-art by up to 30 %, computed from average of four well known metrics for saliency estimation.","2381-8549","978-1-4673-9961-6","10.1109/ICIP.2016.7532628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532628","Salience;convolutional neural network;deep learning;locomotion","Visualization;Estimation;Training;Hafnium;Visual systems;Image edge detection;Gaze tracking","mobile robots;neural nets;probability;robot vision","deep convolutional neural network;human visual system;ranking key areas;priority map;probabilities;gaze fixations;CNN;locomotion;priority estimation;visual salience","","2","","25","","19 Aug 2016","","","IEEE","IEEE Conferences"
"Two-phase approach — Calibration and iris contour estimation — For gaze tracking of head-mounted eye camera","J. Li; S. Li","Graduate School of Engineering, Tottori University, Tottori, Japan; Graduate School of Information Sciences, Hiroshima City University, Hiroshima, Japan","2016 IEEE International Conference on Image Processing (ICIP)","19 Aug 2016","2016","","","3136","3140","The fitting of an ellipse to the iris contour is usually performed using five unknown parameters. In this study, we divide the continuous gaze estimation of a head-mounted eye camera into two phases. One phase, known as the calibration phase, is used to estimate the eyeball center position in relation to the coordinate system of the head-mounted eye camera. The other phase is used to fit the iris contour in 2D images employing only two parameters for gaze estimation. As seen from the experimental results, the proposed method demonstrates both credible eyeball center estimation and an accurate iris contour estimation in comparison with the conventional five unknown parameter approach.","2381-8549","978-1-4673-9961-6","10.1109/ICIP.2016.7532937","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532937","Iris fitting;head-mounted camera;eyeball calibration;gaze estimation","Decision support systems;Indexes","edge detection;gaze tracking;image sensors;iris recognition","two-phase approach;iris contour estimation;gaze tracking;head-mounted eye camera;calibration phase;gaze estimation","","2","","14","","19 Aug 2016","","","IEEE","IEEE Conferences"
"Combining isophote and cascade classifier information for precise pupil localization","S. Vater; F. P. León","Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), 76187 Karlsruhe, Germany; Institute of Industrial Information Technology (IIIT), Karlsruhe Institute of Technology (KIT), 76187 Karlsruhe, Germany","2016 IEEE International Conference on Image Processing (ICIP)","19 Aug 2016","2016","","","589","593","This paper investigates precise pupil center localization in low-resolution images. Being an essential preprocessing step in many applications such as gaze estimation, face alignment as well as human-computer interaction, robust, precise, and efficient methods are necessary. We present a method for accurate eye center localization operating with images from simple off-the-shelf hardware such as webcams. The proposed method utilizes the isophote representation that allows to find pupil center candidates by introducing a novel voting mechanism for pixel weights. To cope with multiple local maxima resulting from the isophote voting map, we combine this information with quasi-continuous responses of a modified cascade classifier framework utilizing appearance-based features. We conduct experiments on the BioID database and show that the presented method outperforms results of existing methods within an error range of the pupil diameter while running at 10 fps on a standard CPU with 3.3 GHz in a Matlab implementation.","2381-8549","978-1-4673-9961-6","10.1109/ICIP.2016.7532425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532425","Image processing;machine vision;feature extraction;object detection;eye localization","Iris;Estimation;Databases;Histograms;Robustness;Biomedical imaging;Face","database management systems;error analysis;gaze tracking;image classification;image resolution;smart pixels","cascade classifier information;precise pupil localization;low-resolution images;accurate eye center localization;off-the-shelf hardware;isophote representation;pupil center candidates;voting mechanism;pixel weights;isophote voting map;quasi-continuous responses;modified cascade classifier framework;BioID database;Matlab implementation;standard CPU","","2","","22","","19 Aug 2016","","","IEEE","IEEE Conferences"
"Transfer learning with deep networks for saliency prediction in natural video","S. Chaabouni; J. Benois-Pineau; C. Ben Amar","LaBRI UMR 5800, University of Bordeaux, 351 crs de la Liberation, F33405 Talence Cedex, France; LaBRI UMR 5800, University of Bordeaux, 351 crs de la Liberation, F33405 Talence Cedex, France; REGIM-Lab LR11ES48, University of Sfax National Engineering School of Sfax, BP1173, 3038 Sfax, Tunisia","2016 IEEE International Conference on Image Processing (ICIP)","19 Aug 2016","2016","","","1604","1608","The main purpose of transfer learning is to resolve the problem of different data distribution, generally, when the training samples of source domain are different from the training samples of the target domain. Prediction of salient areas in natural video suffers from the lack of large video benchmarks with human gaze fixations. Different databases only provide dozens up to one or two hundred of videos. The only public large database is HOLLYWOOD with 1707 videos available with gaze recordings. The main idea of this paper is to transfer the knowledge learned with the deep network on a large dataset to train the network on a small dataset to predict salient areas. The results show an improvement on two small publicly available video datasets.","2381-8549","978-1-4673-9961-6","10.1109/ICIP.2016.7532629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7532629","Transfer learning;deep learning;saliency map;visual attention;residual motion","Training;Databases;Computer architecture;Computational modeling;Benchmark testing;Visualization;Image color analysis","learning (artificial intelligence);neural nets;video databases;video signal processing","transfer learning;deep networks;saliency prediction;natural video;source domain;human gaze fixations;public large database","","28","","22","","19 Aug 2016","","","IEEE","IEEE Conferences"
"Eye tracking using monocular camera for gaze estimation applications","Guojun Yang; J. Saniie","Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, United States; Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, United States","2016 IEEE International Conference on Electro Information Technology (EIT)","8 Aug 2016","2016","","","0292","0296","Gaze estimation reveals valuable information in terms of human behavior. For example, gaze location can be used as a mean of human computer interaction. In practice, locating the eyes position is a challenging task. Moreover, it is more challenging to estimate the location of the gaze. It requires a system to extract the information of the human eye location then estimate the direction of the gaze. There are existing systems that achieve similar tasks. Many of these systems rely on complex systems, such as wearable sensors or an infrared camera to find the exact focus of eyes gaze. Consequently, these systems are not only complex but also suitable for limited applications. In this paper, we present efficient eye focus methods using a monocular camera, which brings more flexibility and convenience to the system. Three different algorithms for locating the eye's focus have been examined and compared. These algorithms are highly efficient and able to discern the location of the eye's focus in real-time using a regular laptop or desktop computers.","2154-0373","978-1-4673-9985-2","10.1109/EIT.2016.7535254","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7535254","","Cameras;Reflection;Sensors;Transforms;Gaze tracking;Algorithm design and analysis;Retina","cameras;feature extraction;gaze tracking","desktop computers;laptop computers;eye focus methods;complex systems;human eye location;information extraction;eye position localisation;human computer interaction;gaze location;human behavior;gaze estimation;monocular camera;eye tracking","","3","","17","","8 Aug 2016","","","IEEE","IEEE Conferences"
"A robust eye gaze estimation using geometric eye features","K. Jariwala; U. Dalal; A. Vincent","Computer Engineering, SVNIT, Surat, INDIA; Electronics Engineering, SVNIT, Surat, INDIA; Computer Engineering, SVNIT, Surat, INDIA","2016 Third International Conference on Digital Information Processing, Data Mining, and Wireless Communications (DIPDMWC)","4 Aug 2016","2016","","","142","147","Gaze estimation is the process of determining the point of gaze in the space, or the visual axis of an eye. It plays an important role in representing human attention; therefore, it can be most appropriately used in Human Computer Interaction as a means of an advance computer input. Here, the focus is to develop a gaze estimation method for Human Computer Interaction using an ordinary webcam mounted on the top of the computer screen without any additional or specialized hardware. The eye center coordinates are obtained with the geometrical eye model and edge gradients. To improve the reliability, the estimates from two eye centers are combined to reduce the noise and improve the accuracy. Facial land marking is done to identify a precise reference point on the face between the nose. The ellipse fitting and RANSAC method is used to estimate the gaze coordinates and to reject the outliers. This approach can estimate the gaze coordinates with high degree of accuracy even when significant numbers of outliers are present in the data set. Several refinements such as feedback and masking, queuing and averaging are proposed to make the system more stable and useful practically. The results show that the proposed method can be successfully applied to commercial gaze tracking systems using ordinary webcams.","","978-1-4673-9379-9","10.1109/DIPDMWC.2016.7529379","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7529379","eye tracking;computer vision;image processing","Computers;Visualization;Estimation;Webcams;Face","face recognition;gaze tracking;human computer interaction;random processes","robust eye gaze estimation;geometric eye feature;human computer interaction;ordinary webcam;eye center coordinates;edge gradient;facial land marking;ellipse fitting;commercial gaze tracking system;gaze coordinates;RANSAC method","","","","20","","4 Aug 2016","","","IEEE","IEEE Conferences"
"Wearable binocular eye tracking targets in 3-D environment using 2-D regression-based gaze estimation","C. Chang; C. Huang; C. Hu",NA; NA; NA,"2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW)","28 Jul 2016","2016","","","1","2","This paper presents a binocular eye tracking device which used 2-D regression-based method to avoid the complicated vector calculations, often applied in geometry-based 3-D tracking, to increase processing speed. It is probably due to the speed increasing that enables us to do the experiments of dynamic estimation errors, which are actually like an experiment to trace a moving object in real 3-D environment if the predefined points, used for evaluating the error estimation, increased to a large number. So far, the preliminary estimations seemed acceptable. It also found 3-D estimation errors mainly caused by the 2-D calibration errors. Therefore, seeking a better 2-D calibration would become one of the important issues for further experiments.","","978-1-5090-2073-7","10.1109/ICCE-TW.2016.7521001","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7521001","","Calibration;Cameras;Gaze tracking;Estimation error;Visualization;Head","gaze tracking;regression analysis","wearable binocular eye tracking target;3D environment;2D regression-based gaze estimation;geometry-based 3D tracking;error estimation;2D calibration error","","","","5","","28 Jul 2016","","","IEEE","IEEE Conferences"
"An approach of head movement compensation when using a head mounted eye tracker","C. Huang; W. Tan",NA; NA,"2016 IEEE International Conference on Consumer Electronics-Taiwan (ICCE-TW)","28 Jul 2016","2016","","","1","2","This paper presents an approach about head movement compensation for head mounted eye tracker. Eye tracker is a device for measuring eye movements and estimating POG (Point of gaze). When using eye tracker, head movement will cause the error of POG estimated, so the head movement compensation is one of important topics of eye tracker. In this paper, we use scene camera to detect circular patterns pasted on monitor and estimate monitor's depth and 3D position. When calibrating, get the information of mapping function and calibration plane, when using eye tracker, estimate visual axis by them and calculate POG, the experiment result shows this approach can reduce the error caused by head movement effectively.","","978-1-5090-2073-7","10.1109/ICCE-TW.2016.7520987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7520987","","Monitoring;Head;Tracking;Cameras;Calibration;Visualization;Mathematical model","cameras;gaze tracking;image processing equipment;spatial variables measurement","calibration plane;mapping function;3D position;monitor depth estimation;circular pattern detection;scene camera;point of gaze estimation;eye movement measurement;head mounted eye tracker;head movement compensation","","2","","5","","28 Jul 2016","","","IEEE","IEEE Conferences"
"Prediction of visual attention with Deep CNN for studies of neurodegenerative diseases","S. Chaabouni; F. Tison; J. Benois-Pineau; C. Ben Amar","LaBRI UMR 5800, University of Bordeaux, Cours de la Libration, 33405 Talence Cedex, France; CHU de Bordeaux-GH Pellegrin, Place Amelie Raba Leon, 33076 Bordeaux Cedex, France; LaBRI UMR 5800, University of Bordeaux, Cours de la Libration, 33405 Talence Cedex, France; REGIM-Lab LR11ES48, University of Sfax, National Engineering School of Sfax, 3038 Sfax, Tunisia","2016 14th International Workshop on Content-Based Multimedia Indexing (CBMI)","30 Jun 2016","2016","","","1","6","As a part of the automatic study of visual attention of affected populations with neurodegenerative diseases and to predict whether new gaze records a complaint of these diseases, we should design an automatic model that predicts salient areas in video. Past research showed, that people suffering form dementia are not reactive with regard to degradations on still images. In this paper we study the reaction of healthy normal control subjects on degraded area in videos. Furthermore, in the goal to build an automatic prediction model for salient areas in intentionally degraded videos, we design a deep learning architecture and measure its performances when predicting salient regions on completely unseen data. The obtained results are interesting regarding the reaction of normal control subjects against a degraded area in video.","1949-3991","978-1-4673-8695-1","10.1109/CBMI.2016.7500243","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500243","","Visualization;Diseases;Degradation;Measurement;Predictive models;Sociology;Statistics","diseases;medical image processing;video signal processing","visual attention prediction;deep CNN;neurodegenerative diseases;gaze records;dementia;automatic prediction model;deep learning architecture","","2","","21","","30 Jun 2016","","","IEEE","IEEE Conferences"
"Eye detection by using deep learning","Ş. Karahan; Y. S. Akgül","Bilgisayar Mühendisliği Bölümü, Gebze Teknik Üniversitesi, Kocaeli, 41400, Türkiye; Bilgisayar Mühendisliği Bölümü, Gebze Teknik Üniversitesi, Kocaeli, 41400, Türkiye","2016 24th Signal Processing and Communication Application Conference (SIU)","23 Jun 2016","2016","","","2145","2148","In recent years, deep learning algorithm has been one of the most used method in machine learning. Success rate of the most popular machine learning problems has been increased by using it. In this work, we develop an eye detection method by using a deep neural network. The designed network, which is accepted as an input by Caffe, has 3 convolution layers and 3 max pooling layers. This model has been trained with 16K positive and 52K negative image patches. The last layer of the network is the classification layer which operates a softmax algorithm. The trained model has been tested with images, which were provided on FDDB and CACD datasets, and also compared with Haar eye detection algorithm. Recall value of the method is higher than the Haar algorithm on the two datasets. However, according to the precision rate, the Haar algorithm is successful on FDDB dataset.","","978-1-5090-1679-2","10.1109/SIU.2016.7496197","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7496197","Deep Learning;Caffe;Digits;Eye Detection","Hidden Markov models;Neural networks;Face recognition;Machine learning;Algorithm design and analysis;Convolution;Classification algorithms","eye;gaze tracking;image classification;neural nets","deep learning algorithm;machine learning;eye detection method;deep neural network;convolution layers;max pooling layers;image patches;classification layer;softmax algorithm;FDDB dataset;CACD dataset;Haar eye detection algorithm;recall value;Haar algorithm","","1","","","","23 Jun 2016","","","IEEE","IEEE Conferences"
"3D gaze cursor: Continuous calibration and end-point grasp control of robotic actuators","P. M. Tostado; W. W. Abbott; A. A. Faisal","Brain & Behaviour Lab - Dept. of Bioengineering, Imperial College London, South Kensington Campus, SW7 2AZ, UK; Brain & Behaviour Lab - Dept. of Bioengineering, Imperial College London, South Kensington Campus, SW7 2AZ, UK; Brain & Behaviour Lab - Dept. of Bioengineering, Imperial College London, South Kensington Campus, SW7 2AZ, UK","2016 IEEE International Conference on Robotics and Automation (ICRA)","9 Jun 2016","2016","","","3295","3300","Eye movements are closely related to motor actions, and hence can be used to infer motor intentions. Additionally, eye movements are in some cases the only means of communication and interaction with the environment for paralysed and impaired patients with severe motor deficiencies. Despite this, eye-tracking technology still has a very limited use as a human-robot control interface and its applicability is highly restricted to 2D simple tasks that operate on screen based interfaces and do not suffice for natural physical interaction with the environment. We propose that decoding the gaze position in 3D space rather than in 2D results into a much richer “spatial cursor” signal that allows users to perform everyday tasks such as grasping and moving objects via gaze-based robotic teleoperation. Eye tracking in 3D calibration is usually slow - we demonstrate here that by using a full 3D trajectory for system calibration generated by a robotic arm rather than a simple grid of discrete points, gaze calibration in the 3 dimensions can be successfully achieved in short time and with high accuracy. We perform the non-linear regression from eye-image to 3D-end point using Gaussian Process regressors, which allows us to handle uncertainty in end-point estimates gracefully. Our telerobotic system uses a multi-joint robot arm with a gripper and is integrated with our in-house “GT3D” binocular eye tracker. This prototype system has been evaluated and assessed in a test environment with 7 users, yielding gaze-estimation errors of less than 1cm in the horizontal, vertical and depth dimensions, and less than 2cm in the overall 3D Euclidean space. Users reported intuitive, low-cognitive load, control of the system right from their first trial and were straightaway able to simply look at an object and command through a wink to “grasp this” object with the robot gripper.","","978-1-4673-8026-3","10.1109/ICRA.2016.7487502","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7487502","","Three-dimensional displays;Robot kinematics;Calibration;Grippers;Tracking;Trajectory","actuators;Gaussian processes;grippers;human-robot interaction;regression analysis;telerobotics","3D gaze cursor;continuous calibration;end-point grasp control;robotic actuators;motor intention;paralysed patients;impaired patients;human-robot control interface;spatial cursor signal;gaze-based robotic teleoperation;3D calibration;nonlinear regression;Gaussian process regressors;multijoint robot arm;Euclidean space;robot gripper","","13","","20","","9 Jun 2016","","","IEEE","IEEE Conferences"
"Estimating 3D Gaze Directions Using Unlabeled Eye Images via Synthetic Iris Appearance Fitting","F. Lu; Y. Gao; X. Chen","State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China; School of Software, Tsinghua University, Beijing, China; State Key Laboratory of Virtual Reality Technology and Systems, School of Computer Science and Engineering, Beihang University, Beijing, China","IEEE Transactions on Multimedia","12 Aug 2016","2016","18","9","1772","1782","Estimating three-dimensional (3D) human eye gaze by capturing a single eye image without active illumination is challenging. Although the elliptical iris shape provides a useful cue, existing methods face difficulties in ellipse fitting due to unreliable iris contour detection. These methods may fail frequently especially with low resolution eye images. In this paper, we propose a synthetic iris appearance fitting (SIAF) method that is model-driven to compute 3D gaze direction from iris shape. Instead of fitting an ellipse based on exactly detected iris contour, our method first synthesizes a set of physically possible iris appearances and then optimizes inside this synthetic space to find the best solution to explain the captured eye image. In this way, the solution is highly constrained and guaranteed to be physically feasible. In addition, the proposed advanced image analysis techniques also help the SIAF method be robust to the unreliable iris contour detection. Furthermore, with multiple eye images, we propose a SIAF-joint method that can further reduce the gaze error by half, and it also resolves the binary ambiguity which is inevitable in conventional methods based on simple ellipse fitting.","1941-0077","","10.1109/TMM.2016.2576284","National Natural Science Foundation of China-CARFC(grant numbers:U1533129); National Natural Science Foundation of China(grant numbers:61325011,61532003); Fundamental Research Funds for the Central Universities; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484318","Gaze estimation;iris fitting;three-dimensional (3D) human gaze;unlabeled eye images","Iris;Three-dimensional displays;Training;Shape;Solid modeling;Cameras;Face","gaze tracking;image resolution;iris recognition","3D gaze direction estimation;unlabeled eye images;synthetic iris appearance fitting;three-dimensional human eye gaze;elliptical iris shape;iris contour detection;low resolution eye images;iris appearances;advanced image analysis techniques;SIAF-joint method;gaze error;ellipse fitting","","12","","43","IEEE","2 Jun 2016","","","IEEE","IEEE Journals"
"Gaze estimation based on camera relay","H. Qi; Q. Wu; B. Guo; J. Li; J. Sun; H. Yan","School of Information Science and Engineering, Shandong University, Jinan China; School of Information Science and Engineering, Shandong University, Jinan China; School of Information Science and Engineering, Shandong University, Jinan China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan China; School of Information Science and Engineering, Shandong Normal University, Jinan China; School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan China","2016 Sixth International Conference on Information Science and Technology (ICIST)","2 Jun 2016","2016","","","511","514","Gaze-tracking is a hot technology of human-computer interaction, which requires an explicit personal calibration process before every use for the sake of higher accuracy. When a number of different gaze interactive systems are used continuously, the calibration process would take the trouble. Due to the accuracy of interaction relying on the calibration of the camera which is used, in this paper, we proposed a method to transmit the personal calibration information based on camera calibration. The system devotes to deliver the personal calibration information between gaze interactive systems so that only one calibration is needed for the multiple gaze interactive systems. Given the calibrated cameras in the interactive systems, personalized eye parameters calibrated on one camera can be transmitted to any of the other cameras one by one. When one of the other cameras is used, the personal calibration process can be omitted without lowering the accuracy of the premise of the calculation much, where implements multiple interactive systems to realize the seamless transfer to use. Experiment between the two cameras is explored to validate the method we proposed. The experimental results show that the method is feasible and reliable.","","978-1-5090-1224-4","10.1109/ICIST.2016.7483467","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7483467","gaze tracking;camera calibration;gaze calibration;camera relay","Calibration;Cameras;Gaze tracking;Relays;Computers;Interactive systems;Estimation error","calibration;cameras;computer vision;gaze tracking;gesture recognition;human computer interaction;interactive systems","gaze estimation;camera relay;human-computer interaction;calibration process;gaze interactive system;gesture recognition;computer vision","","","","9","","2 Jun 2016","","","IEEE","IEEE Conferences"
"OpenFace: An open source facial behavior analysis toolkit","T. Baltrušaitis; P. Robinson; L. Morency",NA; NA; NA,"2016 IEEE Winter Conference on Applications of Computer Vision (WACV)","26 May 2016","2016","","","1","10","Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.","","978-1-5090-0641-0","10.1109/WACV.2016.7477553","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7477553","","Face;Training;Real-time systems;Magnetic heads;Estimation;Videos","computer vision;face recognition;learning (artificial intelligence);pose estimation","eye-gaze estimation;facial action unit recognition;head pose estimation;facial landmark detection;computer vision;machine learning;open source facial behavior analysis toolkit;OpenFace","","262","3","72","","26 May 2016","","","IEEE","IEEE Conferences"
"Driver Gaze Region Estimation without Use of Eye Movement","L. Fridman; P. Langhans; J. Lee; B. Reimer",Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology; Massachusetts Institute of Technology,"IEEE Intelligent Systems","25 May 2016","2016","31","3","49","56","Automated estimation of the allocation of a driver's visual attention could be a critical component of future advanced driver assistance systems. In theory, vision-based tracking of the eye can provide a good estimate of gaze location. But in practice, eye tracking from video is challenging because of sunglasses, eyeglass reflections, lighting conditions, occlusions, motion blur, and other factors. Estimation of head pose, on the other hand, is robust to many of these effects but can't provide as fine-grained of a resolution in localizing the gaze. For the purpose of keeping the driver safe, it's sufficient to partition gaze into regions. In this effort, a proposed system extracts facial features and classifies their spatial configuration into six regions in real time. The proposed method achieves an average accuracy of 91.4 percent at an average decision rate of 11 Hz on a dataset of 50 drivers from an on-road study.","1941-1294","","10.1109/MIS.2016.47","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478592","gaze classification;head pose estimation;driver distraction;driver assistance systems;on-road study;intelligent systems","Gaze estimation;Magnetic heads;Feature extraction;Pattern recognition","computer vision;driver information systems;feature extraction;gaze tracking;image classification;pose estimation","driver gaze region estimation;driver visual attention;driver assistance system;vision-based tracking;head pose estimation;facial feature extraction;spatial configuration classification","","50","2","13","","25 May 2016","","","IEEE","IEEE Magazines"
"Estimation of Driver Head Yaw Angle Using a Generic Geometric Model","A. Narayanan; R. M. Kaimal; K. Bijlani","Amrita E-Learning Research Laboratory and the Department of Computer Science, Amrita Vishwa Vidyapeetham, Amritapuri Campus, Kollam, India; Department of Computer Science, Amrita Vishwa Vidyapeetham, Amritapuri Campus, Kollam, India; Amrita E-Learning Research Laboratory, Amrita Vishwa Vidyapeetham, Amritapuri, Kollam, India","IEEE Transactions on Intelligent Transportation Systems","23 Nov 2016","2016","17","12","3446","3460","Head yaw angle is an important indicator of a driver's distraction. Forward collision warning systems incorporate head yaw angle estimators for detecting driver's head-off-road glances. This paper presents a generic geometric model for head yaw estimation. The generic model is customized into 12 different models. The performance of the cylindrical and ellipsoidal models is improved by introducing nose projection and truncation. The yaw estimation problem is formulated using a single variable. This formulation provides better visualization of the cylindrical and ellipsoidal models. It is shown that the ellipsoidal model in literature is indeed a cylindrical model with nose projection. We have also proved that, under the ellipsoidal/cylindrical framework, the estimated yaw angle is independent of shifting the center of rotation along depth. Four previous works are shown as the customization of the proposed generic model. The performance of the proposed models are evaluated using four standard head pose datasets and an ADAS dataset. The proposed nose-projected truncated ellipsoidal model outperforms the state-of-the-art geometric models. Further, we have studied the impact of model parameters and inaccurate facial feature localization through simulation experiments.","1558-0016","","10.1109/TITS.2016.2551298","National Mission on Education through Information and Communication Technology; Ministry of Human Resource Development of India; Amrita Vishwa Vidyapeetham; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7464887","Driver inattention detection;gaze estimation;geometric models;yaw angle estimation","Vehicles;Estimation;Face;Manifolds;Solid modeling;Magnetic heads","driver information systems;estimation theory;gaze tracking;pose estimation;road safety","driver head yaw angle estimation;generic geometric model;driver distraction;forward collision warning systems;driver head-off-road glances detection;cylindrical models;nose projection;nose truncation;head pose datasets;ADAS dataset;nose-projected truncated ellipsoidal model;model parameters;facial feature localization","","9","","57","IEEE","4 May 2016","","","IEEE","IEEE Journals"
"A 0.5° Error 10 mW CMOS Image Sensor-Based Gaze Estimation Processor","K. Bong; I. Hong; G. Kim; H. Yoo","School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, South Korea","IEEE Journal of Solid-State Circuits","4 Apr 2016","2016","51","4","1032","1040","A CMOS image sensor (CIS)-based gaze estimation processor (GEP) is proposed for the next-generation user interface (UI) of the smart glasses. It integrates a functional CIS, a RISC, and an ellipse fitting engine (EFE) on a single chip to reduce the power consumption. The functional CIS includes mixed-signal preprocessing circuits, pupil edge detection circuit (PEDC), and glint corner detection circuit (GCDC). Logarithmic processing element (LPE) is adopted in EFE to reduce the power consumption further. As a result, PEDC/GCDC and LPE can reduce the overall power consumption by 55.2% and 14.1%, respectively. Implemented in 65 nm CMOS technology, the 11.29 mm<sup>2</sup> chip consumes 10 mW power at 30 fps real-time operation, and 0.5° error is achieved in experiments.","1558-173X","","10.1109/JSSC.2015.2504466","Basic Science Research Program through the National Research Foundation of Korea (NRF); Ministry of Science, ICT, and Future Planning(grant numbers:NRF-2015R1A2A1A05001889); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7438806","Eye tracking;functional CMOS image sensor (CIS);gaze estimation;gaze user interface (UI);smart glasses;Eye tracking;functional CMOS image sensor (CIS);gaze estimation;gaze user interface (UI);smart glasses","Estimation;Image edge detection;Detection algorithms;Glass;Reduced instruction set computing;Power demand;Calibration","CMOS image sensors;edge detection;glass;mixed analogue-digital integrated circuits;reduced instruction set computing;user interfaces","CMOS image sensor;CIS-based gaze estimation processor;GEP;user interface;UI;smart glasses;RISC;ellipse fitting engine;EFE;power consumption reduction;mixed-signal preprocessing circuit;pupil edge detection circuit;PEDC;glint corner detection circuit;GCDC;logarithmic processing element;LPE;power 10 mW;size 65 nm","","4","","11","IEEE","22 Mar 2016","","","IEEE","IEEE Journals"
"Eye-gaze systems - An analysis of error sources and potential accuracy in consumer electronics use cases","A. Kar; S. Bazrafkan; C. C. Ostache; P. Corcoran","Center for Cognitive, Connected & Computational Imaging, College of Engineering & Informatics, NUI Galway, Galway, Ireland; Center for Cognitive, Connected & Computational Imaging, College of Engineering & Informatics, NUI Galway, Galway, Ireland; Center for Cognitive, Connected & Computational Imaging, College of Engineering & Informatics, NUI Galway, Galway, Ireland; Center for Cognitive, Connected & Computational Imaging, College of Engineering & Informatics, NUI Galway, Galway, Ireland","2016 IEEE International Conference on Consumer Electronics (ICCE)","14 Mar 2016","2016","","","319","320","Several generic CE use cases and corresponding techniques for eye gaze estimation (EGE) are reviewed. The optimal approaches for each use case are determined from a review of recent literature. In addition, the most probable error sources for EGE are determined and the impact of these error sources is quantified. A discussion and analysis of the research outcome is given and future work outlined.","2158-4001","978-1-4673-8364-6","10.1109/ICCE.2016.7430628","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7430628","eye gaze estimation methods;gaze estimation error sources;eye-trackers;smartphones","Head;Cameras;Estimation;Gaze tracking;Consumer electronics;Smart phones;Image resolution","cameras;consumer electronics;eye;gaze tracking;image resolution","eye-gaze system;consumer electronics;eye gaze estimation;EGE;probable error source;eye image resolution;camera","","4","","12","","14 Mar 2016","","","IEEE","IEEE Conferences"
"Real-time categorization of driver's gaze zone using the deep learning techniques","In-Ho Choi; Sung Kyung Hong; Yong-Guk Kim","Dept. of Computer Eng., Sejong University, Kwangjin-Gu, Seoul, Korea; Dept. of Aerospace, Sejong University, Kwangjin-Gu, Seoul, Korea; Dept. of Computer Eng., Sejong University, Kwangjin-Gu, Seoul, Korea","2016 International Conference on Big Data and Smart Computing (BigComp)","7 Mar 2016","2016","","","143","148","This paper presents a study in which driver's gaze zone is categorized using new deep learning techniques. Since the sequence of gaze zones of a driver reflects precisely what and how he behaves, it allows us infer his drowsiness, focusing or distraction by analyzing the images coming from a camera. A Haar feature based face detector is combined with a correlation filter based MOSS tracker for the face detection task to handle a tough visual environment in the car. Driving database is a big-data which was constructed using a recording setup within a compact sedan by driving around the urban area. The gaze zones consist of 9 categories depending on where a driver is looking at during driving. A convolutional neural network is trained to categorize driver's gaze zone from a given face detected image using a multi-GPU platform, and then its network parameters are transferred to a GPU within a PC running on Windows to operate in the real-time basis. Result suggests that the correct rate of gaze zone categorization reaches to 95% in average, indicating that our system outperforms the state-of-art gaze zone categorization methods based on conventional computer vision techniques.","2375-9356","978-1-4673-8796-5","10.1109/BIGCOMP.2016.7425813","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7425813","Real-time system;Driver's gaze zone;Deep learning;Convolutional neural netwok;Driver distraction and fatigue","Face;Vehicles;Databases;Sensors;Cameras;Face detection","Big Data;computer vision;correlation methods;driver information systems;face recognition;feature extraction;gaze tracking;graphics processing units;Haar transforms;learning (artificial intelligence);neural nets","gaze zone categorization;Windows;multiGPU platform;convolutional neural network;big data;driving database;face detection task;correlation filter based MOSS tracker;Haar feature based face detector;camera;drowsiness;deep learning techniques;real-time driver gaze zone categorization methods","","20","","10","","7 Mar 2016","","","IEEE","IEEE Conferences"
"A 2.71 nJ/Pixel Gaze-Activated Object Recognition System for Low-Power Mobile Smart Glasses","I. Hong; K. Bong; D. Shin; S. Park; K. Jason Lee; Y. Kim; H. Yoo","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Republic of Korea","IEEE Journal of Solid-State Circuits","30 Dec 2015","2016","51","1","45","55","A low-power object recognition (OR) system with intuitive gaze user interface (UI) is proposed for battery-powered smart glasses. For low-power gaze UI, we propose a low-power single-chip gaze estimation sensor, called gaze image sensor (GIS). In GIS, a novel column-parallel pupil edge detection circuit (PEDC) with new pupil edge detection algorithm XY pupil detection (XY-PD) is proposed which results in 2.9× power reduction with 16× larger resolution compared to previous work. Also, a logarithmic SIMD processor is proposed for robust pupil center estimation, (1 pixel error, with low-power floating-point implementation. For OR, low-power multicore OR processor (ORP) is implemented. In ORP, task-level pipeline with keypoint-level scoring is proposed to reduce the number of cores as well as the operating frequency of keypoint-matching processor (KMP) for low-power consumption. Also, dual-mode convolutional neural network processor (CNNP) is designed for fast tile selection without external memory accesses. In addition, a pipelined descriptor generation processor (DGP) with LUT-based nonlinear operation is newly proposed for low-power OR. Lastly, dynamic voltage and frequency scaling (DVFS) for dynamic power reduction in ORP is applied. Combining both of the GIS and ORP fabricated in 65 nm CMOS logic process, only 75 mW average power consumption is achieved with real-time OR performance, which is 1.2× and 4.4× lower power than the previously published work.","1558-173X","","10.1109/JSSC.2015.2476786","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7286768","Convolutional neural network (CNN);dynamic voltage and frequency scaling (DVFS);eye tracking;focal-plane processing;gaze estimation;logarithmic approximation;object recognition (OR);smart glasses;vision chip;Convolutional neural network (CNN);dynamic voltage and frequency scaling (DVFS);eye tracking;focal-plane processing;gaze estimation;logarithmic approximation;object recognition (OR);smart glasses;vision chip","Image edge detection;Glass;Estimation;Robustness;Acceleration;Pipelines;Calibration","CMOS logic circuits;eye;gaze tracking;glass;image sensors;low-power electronics;multiprocessing systems;neural nets;object recognition;parallel processing;user interfaces","gaze-activated object recognition system;low-power mobile smart glasses;low-power object recognition system;intuitive gaze user interface;battery-powered smart glasses;low-power gaze UI;low-power single-chip gaze estimation sensor;gaze image sensor;column-parallel pupil edge detection circuit;PEDC;XY pupil detection;logarithmic SIMD processor;robust pupil center estimation;pixel error;low-power floating-point implementation;low-power multicore OR processor;task-level pipeline;keypoint-level scoring;keypoint-matching processor;KMP;dual-mode convolutional neural network processor;CNNP;pipelined descriptor generation processor;DGP;LUT-based nonlinear operation;dynamic voltage and frequency scaling;DVFS;dynamic power reduction;CMOS logic process;size 65 nm","","11","","27","IEEE","1 Oct 2015","","","IEEE","IEEE Journals"
"Gaze Estimation From Color Image Based on the Eye Model With Known Head Pose","J. Li; S. Li","Graduate School of Engineering, Tottori University, Tottori, Japan; Graduate School of Information Sciences, Hiroshima City University, Hiroshima, Japan","IEEE Transactions on Human-Machine Systems","13 May 2016","2016","46","3","414","423","This paper proposes a novel method of gaze estimation based on an eye model with known head pose. The most crucial factors in the eye-model-based approach to gaze estimation are the 3-D positions of the eyeball and iris centers. In the proposed method, an RGB-D camera, Kinect sensor, is used to obtain the head pose as well as the eye region of the color image. The 3-D position of the eyeball center is determined in the calibration phase by gazing at the center of the color image camera. Then, to estimate the 3-D position of the iris center, the 3-D contour of the iris is projected onto the color image with the known head pose obtained from color and depth cues of an RGB-D camera. Thus, the ellipse of the iris in the image can be described using only two parameters: the yaw and pitch angles of the eyeball in the iris coordinate system, rather than the conventional five parameters of an ellipse. The proposed method can fit an iris that is not complete due to eyelid occlusion. The average errors of vertical and horizontal angles of the gaze estimation for seven subjects are 5.9° and 4.4°, respectively. The processing speed is as high as 330 ms per frame. However, for lower resolution and poor illumination images, as tested on the public database EYEDIAP, the performance of the proposed eye-model-based method is inferior to that of the-state-of-the-art appearance-based method.","2168-2305","","10.1109/THMS.2015.2477507","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7272080","Color and depth cues;eyeball calibration;gaze estimation;iris fitting;3-D eye model;Color and depth cues;eyeball calibration;gaze estimation;iris fitting;3-D eye model","Iris;Head;Cameras;Estimation;Calibration;Optical imaging;Visualization","calibration;estimation theory;gaze tracking;image colour analysis;image resolution;image sensors;interactive devices;pose estimation","gaze estimation;color image;eye model;head pose;RGB-D camera;Kinect sensor;calibration phase;yaw angle;pitch angle;iris coordinate system;image resolution","","20","","24","IEEE","18 Sep 2015","","","IEEE","IEEE Journals"
"Human computer interaction for disabled using eye motion tracking","U. Sambrekar; D. Ramdasi","Department of Instrumentation and Control, Cummins College of Engineering for Women, Pune, 411052, India; Department of Instrumentation and Control, Cummins College of Engineering for Women, Pune, 411052, India","2015 International Conference on Information Processing (ICIP)","13 Jun 2016","2015","","","745","750","Human Computer Interaction is a trend-in technology. Working in this field, we have developed a system to provide a solution to the limb disabled people to interact with computer. Different algorithms are implemented in this paper, to estimate the gaze of user to recognize the reference key. The first step performed on each input frame is the face detection, which is achieved using Viola-Jones algorithm, whereas Circular Hough Transform is used to locate the pupil in eye image. Similarly glint (a small and intense dot inside the pupil image) is detected by the blob analysis method. Using these pupil location and glint location, gaze direction of the user is estimated. With these functions, the user is able to handle input devices of a computer like keyboard and helps to recognize the key to be pressed. Further the type-in process is continued with the blinking phenomenon which is detected using template matching method. Currently the system is limited to 4-key keypad. The system is also designed with opening some window application softwares like Skype, media player etc. which are useful to the limb disable people. The camera used in this application, to access video is Logitech HD 720p webcam (C310) and the software implementation part is done in MATLAB. The system is tested under various environmental conditions, giving the appreciable results and speed.","","978-1-4673-7758-4","10.1109/INFOP.2015.7489481","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7489481","Human Computer Interaction;Circular Hough Transform;Pupil Detection;Glint Detection;Gaze Estimation;Key Recognition;Blink Detection","Human computer interaction;Computers;Transforms;Image edge detection;Feature extraction;Cameras;Face","face recognition;gaze tracking;Hough transforms;human computer interaction;image matching;image motion analysis","eye motion tracking;human computer interaction;limb disabled people;user gaze estimation;reference key recognition;face detection;Viola-Jones algorithm;circular Hough transform;pupil location;eye image;pupil image;blob analysis method;glint location;user gaze direction;blinking phenomenon;template matching method;Logitech HD 720p webcam","","4","","12","","13 Jun 2016","","","IEEE","IEEE Conferences"
"Gaze classification on a mobile device by using deep belief networks","H. Park; D. Kim","POSTECH Pohang, Republic of Korea; POSTECH Pohang, Republic of Korea","2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","9 Jun 2016","2015","","","685","689","In this paper, we introduce a gaze classification method which classifies the locations of human gaze on a display of a mobile device. For example, when the user see the upper part of a display, our method classifies the gaze as the upper part among the available choices: upper, middle, and lower parts of the display. Our method uses appearance-based gaze estimation and gray-scale images captured from a camera of a mobile device. This method does not require any personal calibration. We train gaze classifiers by using Deep Belief Networks with considering head poses in general environments for a mobile device. The gaze classification method is applied to human-computer interaction and various application programs.","2327-0985","978-1-4799-6100-9","10.1109/ACPR.2015.7486590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486590","","Estimation;Face;Smart phones;Detectors;Cameras","belief networks;cameras;gaze tracking;image classification;man-machine systems;mobile handsets","mobile device;deep belief networks;gaze classification method;human gaze locations;appearance-based gaze estimation;gray-scale images;camera;personal calibration;human-computer interaction","","3","1","12","","9 Jun 2016","","","IEEE","IEEE Conferences"
"Gaze estimation driven solution for interacting children with ASD","H. Cai; X. Zhou; H. Yu; H. Liu","School of Computing, University of Portsmouth, UK; School of Computing, University of Portsmouth, UK; School of Computing, University of Portsmouth, UK; School of Computing, University of Portsmouth, UK","2015 International Symposium on Micro-NanoMechatronics and Human Science (MHS)","24 Mar 2016","2015","","","1","6","This paper investigates gaze estimation solutions for interacting children with Autism Spectrum Disorders (ASD). Previous research shows that satisfactory accuracy of gaze estimation can be achieved in constrained settings. However, most of the existing methods can not deal with large head movement (LHM) that frequently happens when interacting with children with ASD scenarios. We propose a gaze estimation method aiming at dealing with large head movement and achieving real time performance. An intervention table equipped with multiple sensors is designed to capture images with LHM. Firstly, reliable facial features and head poses are tracked using supervised decent method. Secondly, a convolution based integer-differential eye localization approach is used to locate the eye center efficiently and accurately. Thirdly, a rotation invariant gaze estimation model is built based on the located facial features, eye center, head pose and the depth data captured from Kinect. Finally, a multi-sensor fusion strategy is proposed to adaptively select the optimal camera to estimate the gaze as well as to fuse the depth information of the Kinect with the web camera. Experimental results showed that the gaze estimation method can achieve acceptable accuracy even in LHM situation and could potentially be applied in therapy for children with ASD.","","978-1-4673-8217-5","10.1109/MHS.2015.7438336","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7438336","Gaze estimation;eye tracking;eye location;child;ASD","Cameras;Estimation;Face;Three-dimensional displays;Facial features;Convolution","gaze tracking;image fusion;medical image processing;patient treatment;pose estimation","gaze estimation driven solution;ASD;children interaction;autism spectrum disorders;large head movement;LHM;facial feature tracking;head pose tracking;supervised decent method;convolution based integer-differential eye localization approach;Kinect;multisensor fusion strategy;Web camera;therapy","","6","","22","","24 Mar 2016","","","IEEE","IEEE Conferences"
"Novel application of a range image sensor to eye gaze estimation by using the relationship between face and eye directions","H. Tamaki; R. Yoshida; T. Ogitsu; H. Takemura; H. Mizoguchi; M. Namatame; F. Kusunoki; E. Yamaguchi; S. Inagaki; Y. Takeda; M. Sugimoto; R. Egusa","Department of Mechanical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba, Japan; Department of Mechanical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba, Japan; Department of Mechanical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba, Japan; Department of Mechanical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba, Japan; Department of Mechanical Engineering, Tokyo University of Science, 2641 Yamazaki, Noda-shi, Chiba, Japan; Faculty of Industrial Technology, Tsukuba University of Technology, 4-3-15, Amakubo, Ibaraki, Japan; Department of Information Design, Tama Art University, 2-1723, Yarimizu, Hachioji, Tokyo, Japan; Graduate School of Human Development and Environment, Kobe University, Yayoigaoka 6, Sanda-shi, Hyogo, Japan; Graduate School of Human Development and Environment, Kobe University, Yayoigaoka 6, Sanda-shi, Hyogo, Japan; Graduate School of Human Development and Environment, Kobe University, Yayoigaoka 6, Sanda-shi, Hyogo, Japan; Graduate School of Information Science and Technology, Hokkaido University, Kita 15, Nishi 8, Kita-ku, Sapporo, Japan; JSPS research fellow, 5-3-1, Kojimachi, Chiyoda-ku, Tokyo, 102-0083, Japan","2015 9th International Conference on Sensing Technology (ICST)","24 Mar 2016","2015","","","443","446","In this study, we estimate gaze direction using the relationship between the face and eyes. The face and eyes are divided laterally and in lengthwise direction. We set up an object laterally and in lengthwise direction and measured the face angle using a Kinect sensor, which is range image sensor, when gazing at an object. The angle of the eyes is fixed at the angle of the object and grasp of the relationship between face and eyes. We calculate the gaze direction using the relation between the face and eyes. Then, the results of present experiment, if based on relationship between face and eyes, has been shown to be effective though the results of the experiment.","2156-8073","978-1-4799-6314-0","10.1109/ICSensT.2015.7438439","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7438439","face and eyes directions;range image sensor","Face;Estimation;Sensors;Image sensors;Conferences;Cameras;Fitting","angular measurement;image sensors","eye gaze direction estimation;face direction estimation;face angle measurement;Kinect image sensor","","1","","5","","24 Mar 2016","","","IEEE","IEEE Conferences"
"A hybrid approach for eye-centre localization for estimation of eye-gazes using low-cost web cam","A. R. Sarkar; G. Sanyal; S. Majumder","Dept. of Computer Sc. & Engg., National Institute of Technology, Durgapur, WB 713209, India; Dept. of Computer Sc. & Engg., National Institute of Technology, Durgapur, WB 713209, India; Surface Robotics Laboratory, CSIR - Central Mechanical Engg. Resrach Institute, Durgapur, WB 713209, India","2015 IEEE International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)","17 Mar 2016","2015","","","273","278","Gaze estimation is one of the recent popular research topics in the domain of computer science/ engineering for vision-based human-computer-interaction. It has huge potential for application in assessment of physical condition of drivers while driving, controlling robots and prosthetics, surveillance etc. For successful gaze estimation the eyes have to be detected perfectly and this will lead to efficient eye centre localization. Several approaches have been used by researchers for eye and eye centre detection. The present work uses Haar-like cascade classifier for effective face and eye detection. This method has the advantage of lower computational load, faster processing due to the associated AdaBoost algorithm. High success rate can be easily achieved in several environments using proper training sets. However, for eye centre localization an improved version of Hough transform in two dimensional parametric space has been used as it is very simple to use and implement practically. This hybrid approach has been successfully tested using low-cost webcams in different lighting conditions with and without spectacles.","","978-1-4673-6735-6","10.1109/ICRCICN.2015.7434249","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7434249","Face detection;Eye tracking;Gaze estimation;Haar-like features;Hough circle;Stroke;Rehabilitation","Estimation;Cameras;Face;Feature extraction;Iris;Calibration","cameras;Hough transforms;human computer interaction;learning (artificial intelligence);object detection","eye-centre localization;eye-gaze estimation;low-cost Web cam;vision-based human-computer-interaction;eye centre detection;Haar-like cascade classifier;face detection;eye detection;AdaBoost algorithm;Hough transform;two-dimensional parametric space","","1","","31","","17 Mar 2016","","","IEEE","IEEE Conferences"
"Eye corner detection with texture image fusion","Z. Zhang; Y. Shen; W. Lin; B. Zhou","Department of Electronic Engineering, Shanghai Jiao Tong University, China; Department of Electronic Engineering, Shanghai Jiao Tong University, China; Department of Electronic Engineering, Shanghai Jiao Tong University, China; School of Information Engineering, Zhengzhou University, China","2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)","25 Feb 2016","2015","","","992","995","The localization of eye corner is of great importance since it offers crucial information in various face-related applications including face tracking, gaze estimation, and facial expression recognition. In this paper, a new approach is proposed which localizes eye corners in a precise and robust way. In our approach, we first estimate a rough location about an eye corner. Then, a set of texture images are derived from regions around the estimated eye corner location. These regions are then fused together to decide the reliability of the estimated eye corner location, and an eye corner's location will be re-estimated if the current estimated eye corner is evaluated as unreliable. Finally, a local refinement process is also applied which refines the location of the estimated eye corner to create a more satisfactory result. Experiments show that our approach can achieve better detection results than the existing methods.","","978-9-8814-7680-7","10.1109/APSIPA.2015.7415420","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7415420","","Reliability;Image edge detection;Layout;Face;Iris;Image color analysis;Kernel","estimation theory;gaze tracking;image fusion;image texture;iris recognition","eye corner detection;texture image fusion;eye corner localization;face tracking;gaze estimation;facial expression recognition;eye corner location estimation;local refinement process","","2","","14","","25 Feb 2016","","","IEEE","IEEE Conferences"
"Eye-gaze estimation accuracy and key in human vision","M. Shimizu; K. Fukui","Graduate School of Science and Technology, Nihon University, 7-24-1, Narashinodai, Funabashi, 274-8501, Japan; Graduate School of Science and Technology, Nihon University, 7-24-1, Narashinodai, Funabashi, 274-8501, Japan","2015 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)","25 Feb 2016","2015","","","48","53","Eye movement and eye-gaze direction have significant roles in human communications. Eye-gaze direction often indicates what a person is thinking about or trying to do. Empirically, it is possible to recognize which eye (right or left) is being fixated by a person who is standing in front of us at 1 m distance; this indicates that our eye-gaze estimation accuracy is less than 3.7 deg. This paper describes our specific examination of eye-gaze direction estimation accuracy and method in the human vision system. Extensive measurements show that the eye-gaze estimation accuracy is 2-4 deg from 1 m distance. However, they also indicate that individual eye-gaze estimation errors differ in magnitude and tendency. A person who has good eyesight estimates well, in general. Furthermore, measurements reveal that human vision might use an eyeball model-based method that employs the center of the iris or pupil, rather than an iris shape-based method with an elliptical iris shape.","","978-1-4799-8996-6","10.1109/ICSIPA.2015.7412162","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7412162","","Iris;Three-dimensional displays;Estimation error;Iris recognition;Shape;Eyelids","eye;gaze tracking;motion estimation","eye-gaze estimation;human vision;eye movement;eye-gaze direction;human communications;eyeball model-based method;pupil;iris shape-based method;elliptical iris shape;distance 1 m","","","","13","","25 Feb 2016","","","IEEE","IEEE Conferences"
"Rendering of Eyes for Eye-Shape Registration and Gaze Estimation","E. Wood; T. Baltruaitis; X. Zhang; Y. Sugano; P. Robinson; A. Bulling","Univ. of Cambridge, Cambridge, UK; Univ. of Cambridge, Cambridge, UK; Max Planck Inst. for Inf., Saarbrϋcken, Germany; Max Planck Inst. for Inf., Saarbrϋcken, Germany; Univ. of Cambridge, Cambridge, UK; Max Planck Inst. for Inf., Saarbrϋcken, Germany","2015 IEEE International Conference on Computer Vision (ICCV)","18 Feb 2016","2015","","","3756","3764","Images of the eye are key in several computer vision problems, such as shape registration and gaze estimation. Recent large-scale supervised methods for these problems require time-consuming data collection and manual annotation, which can be unreliable. We propose synthesizing perfectly labelled photo-realistic training data in a fraction of the time. We used computer graphics techniquesto build a collection of dynamic eye-region models from head scan geometry. These were randomly posed to synthesize close-up eye images for a wide range of head poses, gaze directions, and illumination conditions. We used our model's controllability to verify the importance of realistic illumination and shape variations in eye-region training data. Finally, we demonstrate the benefits of our synthesized training data (SynthesEyes) by out-performing state-of-the-art methods for eye-shape registration as well as cross-dataset appearance-based gaze estimation in the wild.","2380-7504","978-1-4673-8391-2","10.1109/ICCV.2015.428","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7410785","","Shape;Estimation;Training data;Head;Geometry;Three-dimensional displays;Computational modeling","computational geometry;computer vision;eye;gaze tracking;image registration;pose estimation;rendering (computer graphics)","eyes rendering;eye-shape registration;cross-dataset appearance-based gaze estimation;computer vision problems;photorealistic training data synthesis;computer graphics techniques;dynamic eye-region models;head scan geometry;head poses;gaze directions;illumination conditions;shape variations","","87","2","40","","18 Feb 2016","","","IEEE","IEEE Conferences"
"Depth Compensation Model for Gaze Estimation in Sport Analysis","F. B. Narcizo; D. W. Hansen","IT Univ. of Copenhagen, Copenhagen, Denmark; IT Univ. of Copenhagen, Copenhagen, Denmark","2015 IEEE International Conference on Computer Vision Workshop (ICCVW)","15 Feb 2016","2015","","","788","795","A depth compensation model is presented as a novel approach to reduce the effects of parallax error for head-mounted eye trackers. The method can reduce the parallax error when the distance between the user and the target is prior known. The model is geometrically presented and its performance is tested in a totally controlled environment with aim to check the influences of eye tracker parameters and ocular biometric parameters on its behavior. We also present a gaze estimation method based on epipolar geometry for binocular eye tracking setups. The depth compensation model has shown very promising to the field of eye tracking. It can reduce 10 times less the influence of parallax error in multiple depth planes.","","978-1-4673-9711-7","10.1109/ICCVW.2015.107","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7406456","","Cameras;Gaze tracking;Calibration;Training;Estimation;Geometry;Transmission line matrix methods","gaze tracking;image processing;sport","depth compensation model;sport analysis;parallax error;head-mounted eye trackers;eye tracker parameters;ocular biometric parameters;gaze estimation method;epipolar geometry;binocular eye tracking setups","","11","","23","","15 Feb 2016","","","IEEE","IEEE Conferences"
"Model-based approach for gaze estimation from corneal imaging using a single camera","L. El Hafi; K. Takemura; J. Takamatsu; T. Ogasawara","Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara 630-0192, Japan; Department of Applied Computer Engineering, School of Information Science and Technology, Tokai University, 4-1-1 Kitakaname, Hiratsuka, Kanagawa 259-1292, Japan; Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara 630-0192, Japan; Robotics Laboratory, Graduate School of Information Science, Nara Institute of Science and Technology (NAIST), 8916-5 Takayama, Ikoma, Nara 630-0192, Japan","2015 IEEE/SICE International Symposium on System Integration (SII)","11 Feb 2016","2015","","","88","93","This paper describes a method to estimate the gaze direction using cornea images captured by a single camera. The purpose is to develop wearable devices capable of obtaining natural user responses, such as interests and behaviors, from eye movements and scene images reflected on the cornea. From an image of the eye, an ellipse is fitted on the colored iris area. A 3D eye model is reconstructed from the ellipse and rotated to simulate projections of the iris area for different eye poses. The gaze direction is then evaluated by matching the iris area of the current image with the corresponding projection obtained from the model. We finally conducted an experiment using a head-mounted prototype to demonstrate the potential of such an eye-tracking method solely based on cornea images captured from a single camera.","","978-1-4673-7242-8","10.1109/SII.2015.7404959","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404959","","Cameras;Cornea;Solid modeling;Iris;Optical imaging;Visualization;Three-dimensional displays","cameras;eye;gaze tracking;image capture;image colour analysis;wearable computers","model-based approach;gaze estimation;corneal imaging;camera;wearable devices;natural user responses;eye movements;scene images;colored iris area matching;3D eye model;head-mounted prototype;eye-tracking method","","3","","12","","11 Feb 2016","","","IEEE","IEEE Conferences"
"Gaze Estimation Using Human Joint Rotation Angel","T. Otani; H. Matsuda; H. Touyama; T. Nakata","Dept. of Inf. Syst. Eng., Toyama Prefectural Univ., Toyama, Japan; Dept. of Inf. Syst. Eng., Toyama Prefectural Univ., Toyama, Japan; Dept. of Inf. Syst. Eng., Toyama Prefectural Univ., Toyama, Japan; Dept. of Inf. Syst. Eng., Toyama Prefectural Univ., Toyama, Japan","2015 International Conference on Cyberworlds (CW)","4 Feb 2016","2015","","","351","354","Currently, eye trackers can estimate with high precision using eye images. This method requires high-quality eye images, and thus cannot estimate using low-quality eye images, which have low resolution, illumination changes, and occlusions. Other methods use pose information, including head direction, body direction, and head motion. This method obtains pose information using the Eigenspace method, and thus requires a calibration phase. In addition, the eye direction using this method is only horizontal. We propose a gaze estimation algorithm that does not require high quality eye images. The purpose is estimation without using eye images, with an error of less than ten degrees. Experimental results show that the method can estimate gaze direction without using eye images.","","978-1-4673-9403-1","10.1109/CW.2015.59","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7398439","gaze estimation;pose estimation;user interface","Estimation;Head;Image resolution;Systems engineering and theory;Tracking;Calibration;Glass","calibration;eigenvalues and eigenfunctions;estimation theory;gaze tracking;pose estimation","gaze estimation algorithm;human joint rotation angle;pose information;eigenspace method;calibration phase;eye direction","","","","4","","4 Feb 2016","","","IEEE","IEEE Conferences"
"A Survey on Gaze Estimation","X. Wang; K. Liu; X. Qian","Sch. of Mech. Electron. & Inf. Eng., China Univ. of Min. & Technol., Beijing, China; Sch. of Mech. Electron. & Inf. Eng., China Univ. of Min. & Technol., Beijing, China; Sch. of Mech. Electron. & Inf. Eng., China Univ. of Min. & Technol., Beijing, China","2015 10th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)","14 Jan 2016","2015","","","260","267","Eye orientation contains abundance of significant information for estimating user's intention and purpose. Large number of research on gaze direction has been given and improved in statistics, biometrics, organizational behavior, etc. Details of gaze methods are essential conditions for human-computer interactive control, human intention prediction, expression analysis, cognitive state analysis, etc. Although active and significant progress in last decades, eye detection still remains challenging to deviation of location, variability in scale, individuality of eyes, complexity of cameral, calibration of person and related instruments, fixation of head pose. In this paper, gaze estimation has been discussed and various models of gaze detection and tracking have been presented, which show current theoretical need to be further modified in order to improve general gaze tracking estimation solutions in computer vision and beyond.","","978-1-4673-9323-2","10.1109/ISKE.2015.12","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383057","gaze estimation;eye detection;gaze direction;head pose estimation;detection and tracking","Head;Cameras;Calibration;Magnetic heads;Feature extraction","computer vision;gaze tracking;object detection","gaze detection;general gaze tracking estimation;computer vision","","4","","50","","14 Jan 2016","","","IEEE","IEEE Conferences"
"A low cost webcam based eye tracker for communicating through the eyes of young children with ASD","J. R. Khonglah; A. Khosla","Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab 144011, India; Dr. B. R. Ambedkar National Institute of Technology, Jalandhar, Punjab 144011, India","2015 1st International Conference on Next Generation Computing Technologies (NGCT)","11 Jan 2016","2015","","","925","928","Evolutions in eye tracking systems and their distinct applications have led to their increased use in almost every field of Human-Computer Interaction (HCI). However, cost, setup and availability hinder the integration of these systems with daily use. This paper presents an infrared based detection technique to resist the ambient light variations, and combines scanning of a neighbourhood of 10×10 pixels and blob analysis to generate a gaze vector to calculate the gaze position. The main goal of this paper is to create a low cost and easy-to-setup webcam-based eye tracker that requires no calibration and using it as an assistive technology for young children with autism as a digital communication medium for a session of Applied Behavior Analysis (ABA).","","978-1-4673-6809-4","10.1109/NGCT.2015.7375255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7375255","eye tracking;cost;human computer interaction;gaze estimation;autism spectrum disorder","Autism;Head;Calibration;Digital communication;Human computer interaction;Estimation;Boring","assisted living;gaze tracking;handicapped aids;medical disorders;object tracking","webcam based eye tracker;ASD;eye tracking systems;infrared based detection technique;gaze vector;assistive technology;digital communication medium;applied behavior analysis;ABA;autism spectrum disorder;blob analysis","","3","","13","","11 Jan 2016","","","IEEE","IEEE Conferences"
"Eye gaze tracking for a humanoid robot","O. Palinko; F. Rea; G. Sandini; A. Sciutti","Robotics, Brain and Cognitive Sciences Department of the Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Robotics, Brain and Cognitive Sciences Department of the Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Robotics, Brain and Cognitive Sciences Department of the Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy; Robotics, Brain and Cognitive Sciences Department of the Fondazione Istituto Italiano di Tecnologia, Genova, 16163, Italy","2015 IEEE-RAS 15th International Conference on Humanoid Robots (Humanoids)","28 Dec 2015","2015","","","318","324","Humans use eye gaze in their daily interaction with other humans. Humanoid robots, on the other hand, have not yet taken full advantage of this form of implicit communication. In this paper we present a passive monocular gaze tracking system implemented on the iCub humanoid robot. The validation of the system proved that it is a viable low-cost, calibration-free gaze tracking solution for humanoid platforms, with a mean absolute error of about 5 degrees on horizontal angle estimates. We also demonstrated the applicability of our system to human-robot collaborative tasks, showing that the eye gaze reading ability can enable successful implicit communication between humans and the robot. Finally, in the conclusion we give generic guidelines on how to improve our system and discuss some potential applications of gaze estimation for humanoid robots.","","978-1-4799-6885-5","10.1109/HUMANOIDS.2015.7363561","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7363561","","Gaze tracking;Face;Humanoid robots;Feature extraction;Cameras","face recognition;gaze tracking;humanoid robots;human-robot interaction;legged locomotion;robot vision","eye gaze tracking;human-robot interaction;passive monocular gaze tracking system;iCub humanoid robot;low-cost calibration-free gaze tracking;mean absolute error;horizontal angle estimation;human-robot collaborative tasks;eye gaze reading ability;implicit communication","","13","","31","","28 Dec 2015","","","IEEE","IEEE Conferences"
"Multimodal joint visual attention model for natural human-robot interaction in domestic environments","J. Domhof; A. Chandarr; M. Rudinac; P. Jonker","Delft Robotics Institute, TU, The Netherlands; Delft Robotics Institute, TU, The Netherlands; Delft Robotics Institute, TU, The Netherlands; Delft Robotics Institute, TU, The Netherlands","2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","17 Dec 2015","2015","","","2406","2412","In this paper, we introduce a non-verbal multimodal joint visual attention model for human-robot interaction in household scenarios. Our model combines the bottom-up saliency and depth-based segmentation with the top-down cues such as pointing and gaze to detect the objects of interest according to the user. For generation of the top-down saliency maps, we have introduced novel methods for object saliency, based on the pointing direction as well as the gaze direction. For gaze estimation, a hybrid model has been introduced which automatically selects keypoint-based matching or back-projection based on the textureness of the object model. The combination of different cues ensures reliable object detection and interaction independent of the relative position between the user, robot and objects. Extensive experiments show good detection results in different interaction scenarios as well as in challenging environmental conditions.","","978-1-4799-9994-1","10.1109/IROS.2015.7353703","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7353703","","Robots;Visualization;Image color analysis;Three-dimensional displays;Computational modeling;Cameras;Estimation","human-robot interaction;image segmentation;image texture;object detection","natural human-robot interaction;domestic environment;nonverbal multimodal joint visual attention model;gaze estimation;hybrid model;keypoint-based matching;object detection","","5","","18","","17 Dec 2015","","","IEEE","IEEE Conferences"
"Gaze tracking system using structure sensor & zoom camera","H. Lee; J. Seo; H. Jo","Realistic Broadcasting Media Research Department, ETRI, Daejeon, South Korea; Realistic Broadcasting Media Research Department, ETRI, Daejeon, South Korea; Image Engineering Lab., Hanyang University, Seoul, South Korea","2015 International Conference on Information and Communication Technology Convergence (ICTC)","17 Dec 2015","2015","","","830","832","We propose a gaze tracking system that can be applicable to a large display over 60 inch with 2~3m viewing distance. The proposed gaze tracking device is designed to obtain high resolution near-infrared (NIR) eye images even though a user is far away from the system. And the proposed gaze tracking algorithm extracts pupil center and glints accurately and shows high gaze estimation performance.","","978-1-4673-7116-2","10.1109/ICTC.2015.7354677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7354677","gaze;tracking;zoom camera;depth sensor;structure sensor","Gaze tracking;Face;Image resolution;Interpolation;Webcams;Calibration","cameras;face recognition;gaze tracking;iris recognition","gaze tracking system;high resolution near-infrared eye images;high resolution NIR eye images;pupil center;glints;gaze estimation performance;zoom camera;structure sensor","","1","","1","","17 Dec 2015","","","IEEE","IEEE Conferences"
"Estimation of eye gaze direction angles based on active appearance models","P. Koutras; P. Maragos","School of E.C.E., National Technical University of Athens, 15773 Athens, Greece; School of E.C.E., National Technical University of Athens, 15773 Athens, Greece","2015 IEEE International Conference on Image Processing (ICIP)","10 Dec 2015","2015","","","2424","2428","In this paper we demonstrate efficient methods for continuous estimation of eye gaze angles with application to sign language videos. The difficulty of the task lies on the fact that those videos contain images with low face resolution since they are recorded from distance. First, we proceed to the modeling of face and eyes region by training and fitting Global and Local Active Appearance Models (LAAM). Next, we propose a system for eye gaze estimation based on a machine learning approach. In the first stage of our method, we classify gaze into discrete classes using GMMs that are based either on the parameters of the LAAM, or on HOG descriptors for the eyes region. We also propose a method for computing gaze direction angles from GMM log-likelihoods. We qualitatively and quantitatively evaluate our methods on two sign language databases and compare with a state of the art geometric model of the eye based on LAAM landmarks, which provides an estimate in direction angles. Finally, we further evaluate our framework by getting ground truth data from an eye tracking system Our proposed methods, and especially the GMMs using LAAM parameters, demonstrate high accuracy and robustness even in challenging tasks.","","978-1-4799-8339-1","10.1109/ICIP.2015.7351237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351237","Eye gaze estimation;gaze direction angles;face and eyes modeling;active appearance models;histograms of oriented gradients;Gaussian mixture models;eye-tracker estimation","Active appearance model;Estimation;Face;Shape;Assistive technology;Gesture recognition;Databases","computer vision;eye;Gaussian processes;gaze tracking;geometry;image classification;image resolution;learning (artificial intelligence);mixture models;sign language recognition;video signal processing;visual databases","eye gaze direction angle estimation;active appearance models;sign language videos;low face resolution;face modelling;eyes region modelling;global active appearance model;local active appearance model;LAAM;machine learning approach;gaze classification;HOG descriptors;GMM log-likelihood;sign language databases;geometric model;eye tracking system","","7","","35","","10 Dec 2015","","","IEEE","IEEE Conferences"
"Instantaneous real-time head pose at a distance","S. S. Mukherjee; R. H. Baxter; N. M. Robertson","Visionlab, Heriot-Watt University, Edinburgh, UK; Visionlab, Heriot-Watt University, Edinburgh, UK; Visionlab, Heriot-Watt University, Edinburgh, UK","2015 IEEE International Conference on Image Processing (ICIP)","10 Dec 2015","2015","","","3471","3475","In this paper we focus on robust, real-time human head pose estimation in low resolution RGB data without any smoothing motion priors e.g. direction of motion. Our main contributions lie in three major areas. First, we show that a generative Deep Belief Network model can be learned on human head data from multiple types of data sources. These sources have similar underlying data that are not necessarily labelled or have the same kind of ground truth. Second, we perform discriminative training using multiple disparate supervisory labels to fine tune the model for head pose estimation. Third, we present state-of-the-art results on two publicly available datasets using this new approach. Our implementation computes head pose for a head image in 0.8 milliseconds, making it real-time and highly scalable.","","978-1-4799-8339-1","10.1109/ICIP.2015.7351449","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7351449","Head Pose;Gaze;Surveillance;Deep Belief Network;Deep Learning;Unsupervised Learning","Head;Magnetic heads;Training;Surveillance;Image reconstruction;Cities and towns;Hair","belief networks;image colour analysis;image resolution;pose estimation","instantaneous real-time head pose;real-time human head pose estimation;low resolution RGB data;generative deep belief network model;discriminative training;disparate supervisory labels","","","","22","","10 Dec 2015","","","IEEE","IEEE Conferences"
"Real-time eye localization, blink detection, and gaze estimation system without infrared illumination","B. Chen; P. Wu; S. Chien","Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Media IC and System Lab, Graduate Institute of Electronics Engineering and Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan","2015 IEEE International Conference on Image Processing (ICIP)","10 Dec 2015","2015","","","715","719","Gaze tracking systems have high potential to be used as natural user interface devices; however, the mainstream systems are designed with infrared illumination, which may be harmful for human eyes. In this paper, a real-time eye localization, blink detection, and gaze estimation system is proposed without infrared illumination. To deal with various lighting conditions and reflections on the iris, the proposed system is based on a continuously updated color model for robust iris detection. Moreover, the proposed algorithm employs both the simplified and the original eye images to achieve the balance between robustness and accuracy. Experimental results show that the proposed system can achieve the accuracy of 96.8% for blink detection and the accuracy of 1.973 degree for gaze estimation with the processing speed of 10-11fps. The performance is comparable to previous works with infrared illumination.","","978-1-4799-8339-1","10.1109/ICIP.2015.7350892","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7350892","Eye tracking;eye localization;blink detection;gaze estimation","Iris;Lighting;Image color analysis;Yttrium;Feature extraction;Estimation;Histograms","gaze tracking;image colour analysis;object detection","infrared illumination;eye images;robust iris detection;color model;iris reflections;lighting conditions;gaze estimation system;blink detection;real-time eye localization;natural user interface devices;gaze tracking systems","","7","","15","","10 Dec 2015","","","IEEE","IEEE Conferences"
"A calibration simplified method for gaze interaction based on using experience","Cong Niu; J. Sun; Jing Li; Hua Yan","School of Information Science and Engineering Shandong University, Jinan 250100, China; School of Information Science and Engineering Shandong University, Jinan 250100, China; School of Mechanical and Electrical Engineering, Shandong Management University, Jinan 250100, China; School of Computer Science and Technology, Shandong University of Finance and Economics, Jinan 250014, China","2015 IEEE 17th International Workshop on Multimedia Signal Processing (MMSP)","3 Dec 2015","2015","","","1","5","The step of calibration prevents the gaze interaction from interacting naturally, which usually needs five or more calibration points to obtain the user's calibration information. In this paper, a calibration simplified method is proposed, which is based on user experience and consists of the user experience accumulation stage and the calibration simplified stage. In the user experience accumulation stage, the calibration is still needed as usual before each gaze interaction and the calibration parameters and the position of the user are stored as the calibration experience, where the calibration parameters include the actual coordinates of the calibration points and their estimation errors. In the calibration simplified stage, the user needs to fixate on only one calibration point for calibration before the calibration information can be estimated according to the user experience stored in the first stage. Even the calibration information can be estimated according to only the position of the user and the stored calibration parameters, which means the user can interact with computer via gaze directly without calibration. The simulations show that the gaze estimation accuracy of the proposed calibration simplified method can be 1.4047° with one calibration point and 1.7489° without calibration point, which are much higher than that without the user experience.","","978-1-4673-7478-1","10.1109/MMSP.2015.7340846","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340846","Eye Tracking;Gaze Tracking;Calibration;User Experience","Calibration;Yttrium;Real-time systems;Human computer interaction;Cameras;Adaptive optics;Monitoring","calibration;gaze tracking;image processing;user interfaces","calibration simplified method;gaze interaction;user calibration information;user experience accumulation stage;estimation errors;calibration points;eye tracking;gaze tracking","","","","9","","3 Dec 2015","","","IEEE","IEEE Conferences"
"Convolution-based means of gradient for fast eye center localization","H. Cai; H. Yu; C. Yao; S. Chen; H. Liu","School of Computing, University of Portsmouth, UK; School of Computing, University of Portsmouth, UK; College of Mechanical Engineering, Zhejiang University of Technology, China; College of Computer Science, Zhejiang University of Technology, China; School of Computing, University of Portsmouth, UK","2015 International Conference on Machine Learning and Cybernetics (ICMLC)","3 Dec 2015","2015","2","","759","764","Localizing eye center is primary challenge for application-s involving gaze estimation, face recognition and human machine interaction. The challenge is caused by significant variability of eye appearance in illumination, shape, color, viewing angle and dynamics, and computation related issues. In this paper, we propose a convolution-based means of gradient method to efficiently and accurately locate the eye center in low resolution images. Priority of enhancing its computation is achieved by the use of FFT transform and fewer identified pixels of circular boundary of potential eye centres. The proposed algorithm is validated in the research database platform of BioID face database. The experimental results confirm that the proposed outperforms the-state-of-art methods and its potential in real-time eye gaze tracking related applications.","","978-1-4673-7221-3","10.1109/ICMLC.2015.7340650","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7340650","Eye center location;Iris center location;Convolution;Gradients","","convolution;fast Fourier transforms;gaze tracking;gradient methods;image resolution;iris recognition","convolution;fast eye center localization;gaze estimation;face recognition;human machine interaction;gradient method;low resolution images;FFT transform;BioID face database;real-time eye gaze tracking","","4","","18","","3 Dec 2015","","","IEEE","IEEE Conferences"
"Usability of Pilot's Gaze in Aeronautic Cockpit for Safer Aircraft","A. Pavelková; A. Herout; K. Behún","Graph@FIT Brno Univ. of Technol., Brno, Czech Republic; Graph@FIT Brno Univ. of Technol., Brno, Czech Republic; Graph@FIT Brno Univ. of Technol., Brno, Czech Republic","2015 IEEE 18th International Conference on Intelligent Transportation Systems","2 Nov 2015","2015","","","1545","1550","With increasing complexity of aircraft equipment, the demands on the aircraft crew are unprecedented. The modern cockpit must adjust its pace of communication for optimal workload of the crew and ensure that the information presented to the pilot is actually received. We contribute to the solution of these problems by designing a passive (purely video-based and unobtrusive) gaze tracking solution. It allows for detection of the pilot missing a piece of information communicated by the cockpit equipment, and for estimation of pilot's workload and immediate focus. We collected a dataset at an aeronautic simulator by using the OptiTrack tracking equipment for development and evaluation of similar gaze tracking systems and we make it publicly available. The experiments show that proposed algorithms for passive and unobtrusive gaze estimation work satisfactorily for the targeted purposes.","2153-0017","978-1-4673-6596-3","10.1109/ITSC.2015.252","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7313344","","Face;Detectors;Magnetic heads;Training;Vegetation","aerospace safety;aerospace simulation;aircraft instrumentation;gaze tracking","pilot gaze;aeronautic cockpit;safer aircraft;aircraft equipment;aircraft crew;passive gaze tracking solution;cockpit equipment;aeronautic simulator;OptiTrack tracking equipment;gaze tracking systems;unobtrusive gaze estimation","","3","","32","","2 Nov 2015","","","IEEE","IEEE Conferences"
"MuseumVisitors: A dataset for pedestrian and group detection, gaze estimation and behavior understanding","F. Bartoli; G. Lisanti; L. Seidenari; S. Karaman; A. Del Bimbo","University of Florence, 50121 Firenze, Italy; University of Florence, 50121 Firenze, Italy; University of Florence, 50121 Firenze, Italy; University of Florence, 50121 Firenze, Italy; University of Florence, 50121 Firenze, Italy","2015 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","26 Oct 2015","2015","","","19","27","In this paper we describe a new dataset, under construction, acquired inside the National Museum of Bargello in Florence. It was recorded with three IP cameras at a resolution of 1280 × 800 pixels and an average framerate of five frames per second. Sequences were recorded following two scenarios. The first scenario consists of visitors watching different artworks (individuals), while the second one consists of groups of visitors watching the same artworks (groups). This dataset is specifically designed to support research on group detection, occlusion handling, tracking, re-identification and behavior analysis. In order to ease the annotation process we designed a user friendly web interface that allows to annotate: bounding boxes, occlusion area, body orientation and head gaze, group belonging, and artwork under observation. We provide a comparison with other existing datasets that have group and occlusion annotations. In order to assess the difficulties of this dataset we have also performed some tests exploiting seven representative state-of-the-art pedestrian detectors.","2160-7516","978-1-4673-6759-2","10.1109/CVPRW.2015.7301279","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7301279","","Cameras;Detectors;Videos;Calibration;Feature extraction;Head;Computer vision","computer vision;image sensors;IP networks;museums;pedestrians","MuseumVisitors;gaze estimation;pedestrian dataset;group detection;behavior understanding;annotation process;IP cameras;occlusion handling;tracking;re-identification;behavior analysis;bounding boxes;occlusion area;body orientation;head gaze;group belonging;artwork under observation","","7","","34","","26 Oct 2015","","","IEEE","IEEE Conferences"
"Appearance-based gaze estimation in the wild","X. Zhang; Y. Sugano; M. Fritz; A. Bulling","Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Scalable Learning and Perception Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","15 Oct 2015","2015","","","4511","4520","Appearance-based gaze estimation is believed to work well in real-world settings, but existing datasets have been collected under controlled laboratory conditions and methods have been not evaluated across multiple datasets. In this work we study appearance-based gaze estimation in the wild. We present the MPIIGaze dataset that contains 213,659 images we collected from 15 participants during natural everyday laptop use over more than three months. Our dataset is significantly more variable than existing ones with respect to appearance and illumination. We also present a method for in-the-wild appearance-based gaze estimation using multimodal convolutional neural networks that significantly outperforms state-of-the art methods in the most challenging cross-dataset evaluation. We present an extensive evaluation of several state-of-the-art image-based gaze estimation algorithms on three current datasets, including our own. This evaluation provides clear insights and allows us to identify key research challenges of gaze estimation in the wild.","1063-6919","978-1-4673-6964-0","10.1109/CVPR.2015.7299081","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299081","","Estimation;Videos;Art","computer vision;convolution;gaze tracking;motion estimation;neural nets","in-the-wild appearance-based gaze estimation;convolutional neural network;computer vision","","227","6","53","","15 Oct 2015","","","IEEE","IEEE Conferences"
"Encoding based saliency detection for videos and images","T. Mauthner; H. Possegger; G. Waltner; H. Bischof","Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Austria; Institute for Computer Graphics and Vision, Graz University of Technology, Austria","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","15 Oct 2015","2015","","","2494","2502","We present a novel video saliency detection method to support human activity recognition and weakly supervised training of activity detection algorithms. Recent research has emphasized the need for analyzing salient information in videos to minimize dataset bias or to supervise weakly labeled training of activity detectors. In contrast to previous methods we do not rely on training information given by either eye-gaze or annotation data, but propose a fully unsupervised algorithm to find salient regions within videos. In general, we enforce the Gestalt principle of figure-ground segregation for both appearance and motion cues. We introduce an encoding approach that allows for efficient computation of saliency by approximating joint feature distributions. We evaluate our approach on several datasets, including challenging scenarios with cluttered background and camera motion, as well as salient object detection in images. Overall, we demonstrate favorable performance compared to state-of-the-art methods in estimating both ground-truth eye-gaze and activity annotations.","1063-6919","978-1-4673-6964-0","10.1109/CVPR.2015.7298864","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7298864","","Videos;Image color analysis;Encoding;Image coding;Histograms;Training;Detectors","image motion analysis;object detection;video coding","encoding based saliency detection;video saliency detection method;human activity recognition;activity detection algorithms;Gestalt principle;figure-ground segregation;joint feature distribution approximation;salient object detection;ground-truth eye-gaze estimation;activity annotation estimation;appearance cues;motion cues","","24","","33","","15 Oct 2015","","","IEEE","IEEE Conferences"
"Automatic gaze analysis in multiparty conversations based on Collective First-Person Vision","S. Kumano; K. Otsuka; R. Ishii; J. Yamato","NTT Communication Science Laboratories, Japan; NTT Communication Science Laboratories, Japan; NTT Communication Science Laboratories, Japan; NTT Communication Science Laboratories, Japan","2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","1 Oct 2015","2015","05","","1","8","This paper extends the affective computing research field by introducing first-person vision to automatic conversation analysis. We target medium-sized-party face-to-face conversations where each person wears inward-looking and outward-looking cameras. We demonstrate that the fundamental techniques required for group gaze analysis, i.e. speaker detection, face tracking, and gaze estimation, can be accurately and effectively performed via self-training in a unified framework by gathering captured audio-visual signals to a centralized system and using a general conversation rule, i.e. listeners look mainly at the speaker. We visualize the characteristics of participants' gaze behavior as a gazee-centered heat map, which quantitatively reveals what parts of the gazee's body and for how long the participant looked at it while the gazer speaks or listens. An experiment involving two groups of six-person conversations demonstrates the potential of the proposed framework.","","978-1-4799-6026-2","10.1109/FG.2015.7284861","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7284861","","Face;Cameras;Iris;Calibration;Target tracking;Estimation","cameras;computer vision;data visualisation;gaze tracking","automatic gaze analysis;multiparty conversations;collective first-person vision;affective computing;automatic conversation analysis;medium-sized-party face-to-face conversations;outward-looking cameras;inward-looking cameras;group gaze analysis;speaker detection;face tracking;gaze estimation;audio-visual signals;centralized system;general conversation rule;participant gaze behavior characteristics;gazee-centered heat map","","1","","40","","1 Oct 2015","","","IEEE","IEEE Conferences"
"Deep Head Pose: Gaze-Direction Estimation in Multimodal Video","S. S. Mukherjee; N. M. Robertson","Visionlab at the Edinburgh Research Partnership in Engineering and Mathematics, Heriot-Watt University and the University of Edinburgh, UK; Visionlab at the Edinburgh Research Partnership in Engineering and Mathematics, Heriot-Watt University and the University of Edinburgh, UK","IEEE Transactions on Multimedia","26 Oct 2015","2015","17","11","2094","2107","In this paper we present a convolutional neural network (CNN)-based model for human head pose estimation in low-resolution multi-modal RGB-D data. We pose the problem as one of classification of human gazing direction. We further fine-tune a regressor based on the learned deep classifier. Next we combine the two models (classification and regression) to estimate approximate regression confidence. We present state-of-the-art results in datasets that span the range of high-resolution human robot interaction (close up faces plus depth information) data to challenging low resolution outdoor surveillance data. We build upon our robust head-pose estimation and further introduce a new visual attention model to recover interaction with the environment . Using this probabilistic model, we show that many higher level scene understanding like human-human/scene interaction detection can be achieved. Our solution runs in real-time on commercial hardware.","1941-0077","","10.1109/TMM.2015.2482819","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7279167","Convolutional neural networks (CNNs);deep learning;gaze direction;head-pose;RGB-D","Head;Estimation;Human computer interaction;Surveillance;Magnetic heads;Visualization;Image resolution","approximation theory;estimation theory;feedforward neural nets;human-robot interaction;image classification;image colour analysis;learning (artificial intelligence);pose estimation;probability;regression analysis;robot vision;video surveillance","deep head pose;gaze-direction estimation;multimodal video;convolutional neural network-based model;CNN-based model;human head pose estimation;low-resolution multimodal RGB-D data;human gazing direction classification;learned deep classifier;approximate regression confidence estimation;high-resolution human robot interaction;low resolution outdoor surveillance data;visual attention model;probabilistic model;human-human interaction detection;scene interaction detection","","74","","46","OAPA","28 Sep 2015","","","IEEE","IEEE Journals"
"A 0.5-degree error 10mW CMOS image sensor-based gaze estimation processor with logarithmic processing","K. Bong; I. Hong; G. Kim; H. Yoo","Department of EE, KAIST, Yuseong, Daejeon, Republic of Korea; Department of EE, KAIST, Yuseong, Daejeon, Republic of Korea; Department of EE, KAIST, Yuseong, Daejeon, Republic of Korea; Department of EE, KAIST, Yuseong, Daejeon, Republic of Korea","2015 Symposium on VLSI Circuits (VLSI Circuits)","3 Sep 2015","2015","","","C46","C47","A CMOS image sensor (CIS)-based gaze estimation processor (GEP) is proposed for the next generation user interface (UI) of the smart glasses. It integrates a functional CMOS image sensor (CIS), a RISC, and an ellipse fitting engine (EFE) on a single chip to reduce the power consumption. The functional CIS includes mixed-signal pre-processing circuits, pupil edge detection circuit (PEDC) and glint corner detection circuit (GCDC). Logarithmic processing elements (LPE) is used for EFE to reduce the power consumption further. As a result, PEDC/GCDC and LPE can reduce overall power by 55.2% and 14.1%, respectively. Implemented in 65nm CMOS technology, the 11.29mm<sup>2</sup> chip consumes 10mW power at 30fps real-time operation, and 0.5° error is achieved in a glass-type wearable system.","2158-5636","978-4-86348-502-0","10.1109/VLSIC.2015.7231321","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7231321","","Image edge detection;Fitting;Estimation;Glass;Reduced instruction set computing;Power demand;CMOS image sensors","CMOS image sensors;edge detection;glass;intelligent materials;intelligent sensors;reduced instruction set computing;user interfaces","CMOS image sensor;gaze estimation processor;CIS;GEP;user interface;UI;smart glass-type wearable system;RISC;ellipse fitting engine;EFE;power consumption;mixed-signal pre-processing circuit;pupil edge detection circuit;PEDC;glint corner detection circuit;GCDC;logarithmic processing element;LPE;power 10 mW;size 65 nm","","","","5","","3 Sep 2015","","","IEEE","IEEE Conferences"
"AR-SSVEP for brain-machine interface: Estimating user's gaze in head-mounted display with USB camera","S. Horii; S. Nakauchi; M. Kitazaki","Graduate School of Engineering, Toyohashi University of Technology; Department of Computer Science and Engineering, Toyohashi University of Technology; Department of Computer Science and Engineering, Toyohashi University of Technology","2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","193","194","We aim to develop a brain-machine interface (BMI) system that estimates user's gaze or attention on an object to pick it up in the real world. In Experiment 1 and 2 we measured steady-state visual evoked potential (SSVEP) using luminance and/or contrast modulated flickers of photographic scenes presented on a head-mounted display (HMD). We applied multiclass SVM to estimate gaze locations for every 2s time-window data, and obtained significantly good classifications of gaze locations with the leave-one-session-out cross validation. In Experiment 3 we measured SSVEP using luminance and contrast modulated flickers of real scenes that were online captured by a USB camera and presented on the HMD. We put AR markers on real objects and made their locations flickering on HMD. We obtained the best performance of gaze classification with highest luminance and contrast modulation (73-91% accuracy at chance level 33%), and significantly good classification with low (25% of the highest) luminance and contrast modulation (42-50% accuracy). These results suggest that the luminance-modulated flickers of real scenes through USB camera can be applied to BMI by using augmented reality technology.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223361","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223361","SSVEP;EEG;Brain-machine Interface;Augmented-reality;SVM","Modulation;Universal Serial Bus;Cameras;Electroencephalography;Brain-computer interfaces;Visualization;Accuracy","augmented reality;brain-computer interfaces;cameras;helmet mounted displays;support vector machines;visual evoked potentials","AR-SSVEP;brain-machine interface;user gaze estimation;head-mounted display;USB camera;BMI system;steady-state visual evoked potential;SSVEP;contrast modulated flickers;photographic scenes;HMD;multiclass SVM;time-window data;leave-one-session-out cross validation;luminance;AR markers;gaze classification performance;contrast modulation;augmented reality technology","","3","","5","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Non-obscuring binocular eye tracking for wide field-of-view head-mounted-displays","M. Stengel; S. Grogorick; M. Eisemann; E. Eisemann; M. Magnor",TU Braunschweig; TU Braunschweig; TU Braunschweig; TU Delft; TU Braunschweig,"2015 IEEE Virtual Reality (VR)","27 Aug 2015","2015","","","357","358","We present a complete hardware and software solution for integrating binocular eye tracking into current state-of-the-art lens-based Head-mounted Displays (HMDs) without affecting the user's wide field-of-view off the display. The system uses robust and efficient new algorithms for calibration and pupil tracking and allows realtime eye tracking and gaze estimation. Estimating the relative gaze direction of the user opens the door to a much wider spectrum of virtual reality applications and games when using HMDs. We show a 3d-printed prototype of a low-cost HMD with eye tracking that is simple to fabricate and discuss a variety of VR applications utilizing gaze estimation.","2375-5334","978-1-4799-1727-3","10.1109/VR.2015.7223443","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7223443","Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism — Virtual reality Computer Graphics [C.3];Special-Purpose and Application-based Systems — Real-time and embedded systems","Lenses;Calibration;Rendering (computer graphics);Estimation;Prototypes","calibration;gaze tracking;helmet mounted displays;lenses;virtual reality","nonobscuring binocular eye tracking;wide field-of-view head-mounted-display;lens-based head-mounted display;calibration;pupil tracking;realtime eye tracking;gaze estimation;relative gaze direction;virtual reality applications;3D-printed prototype;low-cost HMD;VR application","","","","6","","27 Aug 2015","","","IEEE","IEEE Conferences"
"Indicating eye contacts in one-to-many video teleconference with one web camera","N. Ye; X. Tao; L. Dong; Y. Li; N. Ge","Tsinghua National Laboratory for Information Science and Technology (TNList), China; Tsinghua National Laboratory for Information Science and Technology (TNList), China; Tsinghua National Laboratory for Information Science and Technology (TNList), China; Tsinghua National Laboratory for Information Science and Technology (TNList), China; Tsinghua National Laboratory for Information Science and Technology (TNList), China","2015 Asia Pacific Conference on Multimedia and Broadcasting","20 Aug 2015","2015","","","1","5","In video teleconference systems, one drawback that affecting quality of experience (QoE) is the lack of eye contacts. Current methods on how to improve eye contacts in one-to-many video teleconference systems are far from commercial deployment since related hardware are very sophisticated and expensive. In this paper, we propose a novel appearance-based scheme to estimate gaze direction with accessible equipments. Differing from current geometrical fitting methods, machine learning algorithms are used in our scheme, making it more robust against the size and resolution of the eye segments appearing in images. To be specific, an adaptive filter is designed to smooth the fluctuated training data; meanwhile, support vector machine (SVM) is applied to get robust results under various conditions. The experiment results show that, under different illuminations and distances, our method can achieve better performance on gaze direction estimation than current methods in terms of accuracy.","","978-1-4799-7967-7","10.1109/APMediaCast.2015.7210284","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7210284","One-to-many video teleconference;QoE;Gaze estimation;Adaptive filter;Support vector machine","Estimation;Head;Cameras;Support vector machines;Accuracy;Feature extraction;Lighting","adaptive filters;image sensors;learning (artificial intelligence);quality of experience;support vector machines;teleconferencing;video communication","eye contacts;one-to-many video teleconference;one web camera;video teleconference systems;quality of experience;QoE;geometrical fitting methods;machine learning algorithms;adaptive filter;fluctuated training data;support vector machine;SVM;gaze direction estimation","","","","22","","20 Aug 2015","","","IEEE","IEEE Conferences"
"Interpolation based polynomial regression for eye gazing estimation: A comparative study","S. Rattarom; N. Aunsri; S. Uttama","School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand; School of Information Technology, Mae Fah Luang University, Chiang Rai, Thailand","2015 12th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)","20 Aug 2015","2015","","","1","4","This paper presents a comparative study on the use of interpolation based gaze estimations with low-cost web camera with built in infrared LEDs. Six polynomial models were carefully compared based on at 9 points calibration; the best average accuracy from this study among all participants is found to be 1.10 degree from the center of gaze line. The best model provides better quality when compared to results from number of groups of research with web camera and standard open source gaze tracking software. In particular, the accuracy was acceptable under the reasonable price and very easy to establish. The results also emphasize the performance of the polynomial models that may vary with the environment and must be chosen carefully.","","978-1-4799-7961-5","10.1109/ECTICon.2015.7207124","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7207124","gaze tracking;interpolation based gaze estimation;mapping function","Accuracy;Cameras;Estimation;Mathematical model;Polynomials;Calibration","gaze tracking;image processing;image sensors;light emitting diodes;polynomial approximation;regression analysis","polynomial regression;interpolation;eye gazing estimation;gaze estimations;Web camera;infrared LED;polynomial models;standard open source gaze tracking software;reasonable price","","2","","14","","20 Aug 2015","","","IEEE","IEEE Conferences"
"Robust gaze estimation based on adaptive fusion of multiple cameras","N. M. Arar; H. Gao; J. Thiran","Signal Processing Laboratory (LTS5) École Polytechnique Fédérale de Lausanne, Switzerland; Signal Processing Laboratory (LTS5) École Polytechnique Fédérale de Lausanne, Switzerland; Signal Processing Laboratory (LTS5) École Polytechnique Fédérale de Lausanne, Switzerland","2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","23 Jul 2015","2015","1","","1","7","Gaze movements play a crucial role in human-computer interaction (HCI) applications. Recently, gaze tracking systems with a wide variety of applications have attracted much interest by the industry as well as the scientific community. The state-of-the-art gaze trackers are mostly non-intrusive and report high estimation accuracies. However, they require complex setups such as camera and geometric calibration in addition to subject-specific calibration. In this paper, we introduce a multi-camera gaze estimation system which requires less effort for the users in terms of the system setup and calibration. The system is based on an adaptive fusion of multiple independent camera systems in which the gaze estimation relies on simple cross-ratio (CR) geometry. Experimental results conducted on real data show that the proposed system achieves a significant accuracy improvement, by around 25%, over the traditional CR-based single camera systems through the novel adaptive multi-camera fusion scheme. The real-time system achieves <;0.9° accuracy error with very few calibration data (5 points) under natural head movements, which is competitive with more complex systems. Hence, the proposed system enables fast and user-friendly gaze tracking with minimum user effort without sacrificing too much accuracy.","","978-1-4799-6026-2","10.1109/FG.2015.7163121","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7163121","","Cameras;Estimation;Calibration;Monitoring;Accuracy;Feature extraction;Robustness","cameras;gaze tracking;human computer interaction;image fusion;motion estimation","multiple camera adaptive fusion;gaze movements;human-computer interaction;HCI;gaze tracking systems;geometric calibration;multicamera gaze estimation system;multiple independent camera systems;cross-ratio geometry;CR geometry;CR-based single camera systems;adaptive multicamera fusion scheme;natural head movements;subject-specific calibration","","5","","18","","23 Jul 2015","","","IEEE","IEEE Conferences"
"Iris Tracking Using a Single Web-Cam without IR Illumination","S. S. Mohapatra; K. Kinage","MIT Coll. of Eng., Pune, India; MIT Coll. of Eng., Pune, India","2015 International Conference on Computing Communication Control and Automation","16 Jul 2015","2015","","","706","711","Iris Tracking is the process of determining the point of gaze or the motion of an eye. In today's era, the combination of Iris Tracking and gaze estimation shows a person's interest. In this paper, we have focused on a single-camera-based gaze estimation algorithm. The paper also describes the implementation of both iris and movement of cursor according to iris position which can be used and detected for gaze estimation in order to improve accuracy without using IR Illumination or any sensor camera. The system works at different distraction conditions, as the normalized error rate is minimal.","","978-1-4799-6892-3","10.1109/ICCUBEA.2015.144","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7155939","Iris Tracking;behavioral biometrics;gaze estimation;human-device interaction","Iris;Lighting;Accuracy;Tracking;Estimation;Cameras;Error analysis","cameras;gaze tracking;image motion analysis;iris recognition;lighting","webcam;IR illumination;iris tracking process;eye motion;single-camera-based gaze estimation algorithm;iris position;sensor camera","","2","","15","","16 Jul 2015","","","IEEE","IEEE Conferences"
"A keypoint-level parallel pipelined object recognition processor with gaze activation image sensor for mobile smart glasses system","I. Hong; D. Shin; Y. Kim; K. Bong; S. Park; K. Lee; H. Yoo","Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea; Department of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1, Guseong-dong, Yuseong-gu, Daejeon, 305-701, Republic of Korea","2015 IEEE Symposium in Low-Power and High-Speed Chips (COOL CHIPS XVIII)","16 Jul 2015","2015","","","1","3","In this paper, a low-power real-time gaze-activated object recognition processor is proposed for a battery-powered smart glasses system. For high energy efficiency, we propose keypoint-level pipelined architecture to increase the hardware utilziation which results in significant power reduction of the real-time recognition processor. In addition, low-power gaze-activation image sensor with mixed-mode architecture is proposed for the glass user's gaze estimation. Therefore, only the small image region where the glasses user is seeing needs to be processed by the recognition processor leading to further power reduction. As a result, the proposed object recognition processor shows 30fps real-time performance only with 75mW power consumption, which is 3.5x and 4.4x smaller power than the state-of-the-art works.","","978-1-4673-7325-8","10.1109/CoolChips.2015.7158531","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7158531","keypoint-level pipeline;heterogeneous multi-core;gaze estimation;smart glasses","Glass;Computer architecture;Pipeline processing;Object recognition;Pipelines;Image sensors;Real-time systems","gaze tracking;image sensors;microprocessor chips;object recognition","keypoint-level parallel pipelined object recognition processor;gaze activation image sensor;mobile smart glasses system;low-power real-time gaze-activated object recognition processor;battery-powered smart glasses system;hardware utilziation;gaze estimation","","1","","5","","16 Jul 2015","","","IEEE","IEEE Conferences"
"Estimation of gaze for human computer interaction","U. Sambrekar; D. Ramdasi","Department of Instrumentation and Control, Cummins College of Engineering for Women, Pune, India; Department of Instrumentation and Control, Cummins College of Engineering for Women, Pune, India","2015 International Conference on Industrial Instrumentation and Control (ICIC)","9 Jul 2015","2015","","","1236","1239","Gaze of the human eye may act as a communication medium for limb disabled people to interact with computers. In order to determine gaze direction, the first necessary step in image processing is segmentation of iris from entire eye image. In this paper Daugman's algorithm is implemented that uses Integro Differential Operator (IDO) to differentiate iris boundary from sclera part of eye and to find the centroid, to determine where the person is looking. Also the computation time required for IDO to scan the image is minimized by applying the operator only to the local minima pixels in the image. The algorithm is implemented in MATLAB and tested on static eye images selected from the CASIA database as well as on the eye images captured by an iPhone 4s, 8 MP camera with a resolution of 3264×2448 pixels. This method can be useful in eye tracking to handle the input devices of computer like keyboard and mouse.","","978-1-4799-7165-7","10.1109/IIC.2015.7150936","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150936","Gaze Estimation;Daugman's Algorithm;Integro-Differential operator;Iris and Pupil Detection","Iris;Computers;Iris recognition;Tracking;Human computer interaction;Image segmentation;Cameras","human computer interaction;image segmentation;integro-differential equations;iris recognition;object detection","gaze estimation;human computer interaction;limb disabled people;image processing;iris segmentation;Daugman algorithm;integro differential operator;IDO;iris boundary detection;Matlab;CASIA database;iPhone 4s;keyboard;mouse","","2","","8","","9 Jul 2015","","","IEEE","IEEE Conferences"
"A novel single IR light based gaze estimation method using virtual glints","Y. Shin; K. Choi; S. Kim; S. Ko","School of Electrical Engineering Department, Korea University, Anam-dong, Sungbuk-gu, Seoul, 136-713, Rep. of Korea; School of Electrical Engineering Department, Korea University, Anam-dong, Sungbuk-gu, Seoul, 136-713, Rep. of Korea; School of Electrical Engineering Department, Korea University, Anam-dong, Sungbuk-gu, Seoul, 136-713, Rep. of Korea; School of Electrical Engineering Department, Korea University, Anam-dong, Sungbuk-gu, Seoul, 136-713, Rep. of Korea","IEEE Transactions on Consumer Electronics","8 Jul 2015","2015","61","2","254","260","Among various infrared (IR) image based remote eye gaze tracking (REGT) methods, 2-D interpolation based methods, e.g., the cross ratio-based and homography normalization (HN)-based methods, have been widely researched owing to their highly accurate performance and simplicity with respect to hardware setup. These methods, however, can be employed only when user utilizes more than four IR light sources to obtain multiple corneal reflections projected on the image plane, i.e. glints. In this paper, a novel 2-D interpolation-based REGT method with a single IR light source is proposed, which attains both accuracy and headpose robustness. In the proposed method, the virtual glint (VG), which can substitute for the actual glint, is first estimated by utilizing the mathematical and geometrical principles established between the 3-D location of an IR light source and the corresponding glint in the image plane, and then the point of gaze (POG) is calculated by employing the HN method using the estimated VGs. The experimental results indicate that the proposed REGT method is highly competitive with conventional ones requiring multiple IR light sources, in terms of accuracy and robustness against head movements.","1558-4127","","10.1109/TCE.2015.7150601","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7150601","Remote eye gaze tracking;binocular;scaledorthographic camera model;virtual glints;single IR light source","Light sources;Cameras;Estimation;Calibration;Reflection;Accuracy;Solid modeling","gaze tracking;infrared imaging;interpolation;object tracking;virtual reality","single IR light based gaze estimation method;virtual glints;infrared image based remote eye gaze tracking method;2D interpolation-based REGT method;point of gaze;scaled-orthographic camera model","","6","","18","","8 Jul 2015","","","IEEE","IEEE Journals"
"Eye gaze direction detection using Principal Component Analysis and appearance based methods","Ç. M. Yılmaz; C. Köse","Bilgisayar Mühendisliği Bölümü, Karadeniz Teknik Üniversitesi, Trabzon, Türkiye; Bilgisayar Mühendisliği Bölümü, Karadeniz Teknik Üniversitesi, Trabzon, Türkiye","2015 23nd Signal Processing and Communications Applications Conference (SIU)","22 Jun 2015","2015","","","1050","1053","User's gaze on computer screen is widely being used in human computer interaction, virtual reality, computer game industry, usability tests, improving the quality of physically disabled people and drowsiness detection. In this work, gaze image data and computer screen are spatially mapped using appearance based video oculography methods. Furthermore, gazing directions of left, right, lower, upper, center and closed eye state are detected. In order to do this, a new gaze database is created and color space channels which best describes gaze images' appearance are selected. Users' gaze direction is detected using Principal Component Analysis (PCA) feature extraction method and various machine learning approaches. Evaluation of best approaches is performed by using the resulting classification accuracy of applied methods. Eventually, PCA and Artificial Neural Network (ANN) approach shows 95.36% estimation accuracy of gazes in five different direction and closed eye state, %98.0 average accuracy of left/right and up/down directions, which is comparable to the results in literature.","2165-0608","978-1-4673-7386-9","10.1109/SIU.2015.7130013","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7130013","eye gaze;gaze direction detection;appearance based video-oculography;human computer interaction;principal component analysis","Principal component analysis;Computers;Accuracy;Estimation;Human computer interaction;Feature extraction;Artificial neural networks","feature extraction;learning (artificial intelligence);neural nets;object detection;principal component analysis","eye gaze direction detection;principal component analysis;appearance based method;human computer interaction;computer screen;virtual reality;computer game industry;usability tests;physically disabled people quality;drowsiness detection;appearance based video oculography method;feature extraction method;machine learning;artificial neural network;ANN","","","","11","","22 Jun 2015","","","IEEE","IEEE Conferences"
"Gaze Estimation From Eye Appearance: A Head Pose-Free Method via Eye Image Synthesis","F. Lu; Y. Sugano; T. Okabe; Y. Sato","School of Computer Science and Engineering, and the International Research Institute for Multidisciplinary Science, Beihang University, Beijing, China; Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Kyushu Institute of Technology, Fukuoka, Japan; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan","IEEE Transactions on Image Processing","23 Jul 2015","2015","24","11","3680","3693","In this paper, we address the problem of free head motion in appearance-based gaze estimation. This problem remains challenging because head motion changes eye appearance significantly, and thus, training images captured for an original head pose cannot handle test images captured for other head poses. To overcome this difficulty, we propose a novel gaze estimation method that handles free head motion via eye image synthesis based on a single camera. Compared with conventional fixed head pose methods with original training images, our method only captures four additional eye images under four reference head poses, and then, precisely synthesizes new training images for other unseen head poses in estimation. To this end, we propose a single-directional (SD) flow model to efficiently handle eye image variations due to head motion. We show how to estimate SD flows for reference head poses first, and then use them to produce new SD flows for training image synthesis. Finally, with synthetic training images, joint optimization is applied that simultaneously solves an eye image alignment and a gaze estimation. Evaluation of the method was conducted through experiments to assess its performance and demonstrate its effectiveness.","1941-0042","","10.1109/TIP.2015.2445295","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7122896","Eye;Gaze estimation;Face and gesture recognition;Head pose-free;Image synthesis;Eye;gaze estimation;face and gesture recognition;head pose-free;image synthesis","Head;Cameras;Training;Estimation;Image generation;Three-dimensional displays;Transmission line matrix methods","cameras;eye;image capture;optimisation;pose estimation","head pose-free method;appearance-based gaze estimation;image capture;free head motion;eye image synthesis;single camera;single-directional flow model;SD flow model;optimization;eye image alignment","Algorithms;Eye;Face;Fixation, Ocular;Humans;Image Processing, Computer-Assisted;Pattern Recognition, Automated;Video Recording","33","","44","IEEE","12 Jun 2015","","","IEEE","IEEE Journals"
"Driver Gaze Tracking and Eyes Off the Road Detection System","F. Vicente; Z. Huang; X. Xiong; F. De la Torre; W. Zhang; D. Levi","Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Gen. Motors Co., Warren, MI, USA; Gen. Motors Co., Herzliya, Israel","IEEE Transactions on Intelligent Transportation Systems","31 Jul 2015","2015","16","4","2014","2027","Distracted driving is one of the main causes of vehicle collisions in the United States. Passively monitoring a driver's activities constitutes the basis of an automobile safety system that can potentially reduce the number of accidents by estimating the driver's focus of attention. This paper proposes an inexpensive vision-based system to accurately detect Eyes Off the Road (EOR). The system has three main components: 1) robust facial feature tracking; 2) head pose and gaze estimation; and 3) 3-D geometric reasoning to detect EOR. From the video stream of a camera installed on the steering wheel column, our system tracks facial features from the driver's face. Using the tracked landmarks and a 3-D face model, the system computes head pose and gaze direction. The head pose estimation algorithm is robust to nonrigid face deformations due to changes in expressions. Finally, using a 3-D geometric analysis, the system reliably detects EOR.","1558-0016","","10.1109/TITS.2015.2396031","General Motors; Grant CPS-0931999; NSF(grant numbers:IIS-1116583); Robotics Institute; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7053946","Driver monitoring system;eyes off the road detection;gaze estimation;head pose estimation;Driver monitoring system;eyes off the road detection;gaze estimation;head pose estimation","Vehicles;Estimation;Cameras;Face;Three-dimensional displays;Magnetic heads","driver information systems;face recognition;feature extraction;gaze tracking;geometry;object detection;pose estimation;road accidents;road safety;road traffic","facial expressions;driver-dependent calibration;3D geometric analysis;nonrigid face deformations;head pose estimation algorithm;gaze direction;steering wheel column;video stream;3D geometric reasoning;gaze estimation;robust facial feature tracking;EOR detection;eyes off the road detection system;inexpensive vision-based system;driver attention focus;automobile safety system;passively driver activity monitoring;United States;vehicle collisions;distracted driving;driver gaze tracking","","152","6","45","IEEE","3 Mar 2015","","","IEEE","IEEE Journals"
"Appearance-Based Gaze Estimation With Online Calibration From Mouse Operations","Y. Sugano; Y. Matsushita; Y. Sato; H. Koike","Perceptual User Interfaces Group, Max Planck Institute for Informatics, Saarbrücken, Germany; Microsoft Research Asia, Beijing, China; Institute of Industrial Science, The University of Tokyo, Tokyo, Japan; Graduate School of Information Science and Engineering, Tokyo Institute of Technology, Tokyo, Japan","IEEE Transactions on Human-Machine Systems","20 May 2017","2015","45","6","750","760","This paper presents an unconstrained gaze estimation method using an online learning algorithm. We focus on a desktop scenario, where a user operates a personal computer, and use the mouse-clicked positions to infer, where on the screen the user is looking at. Our method continuously captures the user's head pose and eye images with a monocular camera, and each mouse click triggers learning sample acquisition. In order to handle head pose variations, the samples are adaptively clustered according to the estimated head pose. Then, local reconstruction-based gaze estimation models are incrementally updated in each cluster. We conducted a prototype evaluation in real-world environments, and our method achieved an estimation accuracy of 2.9°.","2168-2305","","10.1109/THMS.2015.2400434","Microsoft Research IJARC Core Project and JST CREST; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7050250","Computer vision;eye movement;human–computer interface;tracking;Computer vision;eye movement;human–computer interface;tracking","Estimation;Calibration;Gaze tracking;Human computer interaction;Computer vision","adaptive signal processing;calibration;cameras;gaze tracking;human computer interaction;image reconstruction;learning (artificial intelligence);pattern clustering;pose estimation","appearance-based gaze estimation;online calibration;mouse operations;unconstrained gaze estimation method;online learning algorithm;desktop scenario;personal computer;mouse-clicked positions;user head pose images;eye images;monocular camera;mouse click;head pose variations;adaptive clustering;head pose estimation;local reconstruction-based gaze estimation models;human computer interface","","23","","41","IEEE","26 Feb 2015","","","IEEE","IEEE Journals"
"Towards Convenient Calibration for Cross-Ratio Based Gaze Estimation","N. M. Arar; H. Gao; J. Thiran","Signal Process. Lab. (LTS5), Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland; Signal Process. Lab. (LTS5), Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland; Signal Process. Lab. (LTS5), Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland","2015 IEEE Winter Conference on Applications of Computer Vision","23 Feb 2015","2015","","","642","648","Eye gaze movements are considered as a salient modality for human computer interaction applications. Recently, cross-ratio (CR) based eye tracking methods have attracted increasing interest because they provide remote gaze estimation using a single uncalibrated camera. However, due to the simplification assumptions in CR-based methods, their performance is lower than the model-based approaches [8]. Several efforts have been made to improve the accuracy by compensating for the assumptions with subject specific calibration. This paper presents a CR-based automatic gaze estimation system that accurately works under natural head movements. A subject-specific calibration method based on regularized least-squares regression (LSR) is introduced for achieving higher accuracy compared to other state-of-the-art calibration methods. Experimental results also show that the proposed calibration method generalizes better when fewer calibration points are used. This enables user friendly applications with minimum calibration effort without sacrificing too much accuracy. In addition, we adaptively fuse the estimation of the point of regard (PoR) from both eyes based on the visibility of eye features. The adaptive fusion scheme reduces accuracy error by around 20% and also increases the estimation coverage under natural head movements.","1550-5790","978-1-4799-6683-7","10.1109/WACV.2015.91","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045945","","Calibration;Estimation;Accuracy;Cameras;Feature extraction;Light emitting diodes;Monitoring","calibration;estimation theory;gaze tracking;human computer interaction;image fusion;least squares approximations;regression analysis","cross-ratio based automatic gaze estimation;eye gaze movements;human computer interaction applications;CR based eye tracking methods;uncalibrated camera;subject specific calibration method;regularized least-square regression;regularized LSR;point of regard estimation;PoR estimation;adaptive fusion scheme","","5","","21","","23 Feb 2015","","","IEEE","IEEE Conferences"
"Direct Gaze Estimation Based on Nonlinearity of EOG","H. Manabe; M. Fukumoto; T. Yagi","Research Labs, NTT DOCOMO, Yokosuka, Japan; Research Labs, NTT DOCOMO; Tokyo Institute of Technology","IEEE Transactions on Biomedical Engineering","19 May 2015","2015","62","6","1553","1562","Electrooculography (EOG) is one of the measures used to estimate the direction of a person's gaze; however, conventional EOG techniques suffer from a drift issue which makes it difficult to extract an accurate absolute eye angle. The technique proposed here is based on the nonlinearity of the EOG and offers a practical solution to this problem. It estimates the absolute eye angles before and after a saccade, which cancels the offset due to the drift. Additionally, it does not require any effort from the user or any target, but instead uses only the difference of the EOGs. Experiments with five subjects confirm that the proposed technique can estimate the absolute eye angle with an error of less than $4^\circ$. They also show improvements are achieved with several options such as weighting and multiple saccades. The technique will contribute to practical EOG-based interaction systems.","1558-2531","","10.1109/TBME.2015.2394409","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7017542","Gaze estimation;EOG;non-linearity;saccade;Electrooculography (EOG);Gaze estimation;nonlinearity;saccade","Electrooculography;Estimation;Electrodes;Mathematical model;Calibration;Equations","biomechanics;electro-oculography;gaze tracking","direct gaze estimation;EOG nonlinearity;electrooculography;person gaze direction;EOG techniques;absolute eye angle;multiple saccade;practical EOG-based interaction system","Electrooculography;Fixation, Ocular;Humans;Male;Saccades;Signal Processing, Computer-Assisted","31","","38","IEEE","21 Jan 2015","","","IEEE","IEEE Journals"
"A Probabilistic Approach to Online Eye Gaze Tracking Without Explicit Personal Calibration","J. Chen; Q. Ji","Computer Vision Laboratory, GE Global Research Center, Schenectady, NY, USA; Department of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY, USA","IEEE Transactions on Image Processing","10 Feb 2015","2015","24","3","1076","1086","Existing eye gaze tracking systems typically require an explicit personal calibration process in order to estimate certain person-specific eye parameters. For natural human computer interaction, such a personal calibration is often inconvenient and unnatural. In this paper, we propose a new probabilistic eye gaze tracking system without explicit personal calibration. Unlike the conventional eye gaze tracking methods, which estimate the eye parameter deterministically using known gaze points, our approach estimates the probability distributions of the eye parameter and eye gaze. Using an incremental learning framework, the subject does not need personal calibration before using the system. His/her eye parameter estimation and gaze estimation can be improved gradually when he/she is naturally interacting with the system. The experimental result shows that the proposed system can achieve <;3° accuracy for different people without explicit personal calibration.","1941-0042","","10.1109/TIP.2014.2383326","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6990593","Gaze estimation;gaze calibration;dynamic Bayesian network;Gaze estimation;gaze calibration;dynamic Bayesian network","Estimation;Calibration;Optical imaging;Probabilistic logic;Three-dimensional displays;Visualization;Probability distribution","calibration;gaze tracking;human computer interaction;image processing;parameter estimation;statistical distributions","online eye gaze tracking;personal calibration;person-specific eye parameter estimation;natural human computer interaction;probabilistic eye gaze tracking system;probability distributions;gaze estimation","Animals;Bayes Theorem;Calibration;Fixation, Ocular;Humans;Image Processing, Computer-Assisted;Video Recording","29","","34","IEEE","18 Dec 2014","","","IEEE","IEEE Journals"
"Hybrid Method for 3-D Gaze Tracking Using Glint and Contour Features","C. Lai; S. Shih; Y. Hung","Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Chi Nan University, Nantou, Taiwan; Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan","IEEE Transactions on Circuits and Systems for Video Technology","6 Jan 2015","2015","25","1","24","37","Glint features have important roles in gaze-tracking systems. However, when the operation range of a gaze-tracking system is enlarged, the performance of glint-feature-based (GFB) approaches will be degraded mainly due to the curvature variation problem at around the edge of the cornea. Although the pupil contour feature may provide complementary information to help estimating the eye gaze, existing methods do not properly handle the cornea refraction problem, leading to inaccurate results. This paper describes a contour-feature-based (CFB) 3-D gaze-tracking method that is compatible to cornea refraction. We also show that both the GFB and CFB approaches can be formulated in a unified framework and, thus, they can be easily integrated. Furthermore, it is shown that the proposed CFB method and the GFB method should be integrated because the two methods provide complementary information that helps to leverage the strength of both features, providing robustness and flexibility to the system. Computer simulations and real experiments show the effectiveness of the proposed approach for gaze tracking.","1558-2205","","10.1109/TCSVT.2014.2329362","National Science Council(grant numbers:NSC-102-2221-E-002-171); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6826500","Cornea refraction;eye gaze tracking;glint;pupil contour","Cornea;Cameras;Vectors;Light sources;Estimation;Tracking;Optical refraction","estimation theory;face recognition;feature extraction;gaze tracking;refraction","hybrid method;3D gaze tracking;glint features;contour features;glint-feature-based approach;curvature variation problem;pupil contour feature;complementary information;gaze estimation;cornea refraction problem","","24","","25","IEEE","5 Jun 2014","","","IEEE","IEEE Journals"
"Appearance-based eye control system by manifold learning","K. Liang; Y. Chahir; M. Molina; C. Tijus; F. Jouen","CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus 75014, France; CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus 75014, France; PALM Laboratory EA 4649, University of Caen, France; CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus 75014, France; CHArt Laboratory, EPHE Paris, 4-14 rue Ferrus 75014, France","2014 International Conference on Computer Vision Theory and Applications (VISAPP)","12 Oct 2015","2014","3","","148","155","Eye-movements are increasingly employed to study usability issues in HCI (Human-Computer Interacetion) contexts. In this paper we introduce our appearance-based eye control system which utilizes 5 specific eye movements, such as closed-eye movement and eye movements with gaze fixation at the positions (up, down, right, left) for HCI applications. In order to measure these eye movements, we employ a fast appeance-based gaze tracking method with manifold learning technique. First we propose to concatenate local eye appearance Center-Symmetric Local Binary Pattern(CS-LBP) descriptor for each subregion of eye image to form an eye appearance feature vector. The calibration phase is then introduced to construct a trainning samples by spectral clustering. After that, Laplacian Eigenmaps will be applied to the trainning set and unseen input together to get the structure of eye manifolds. Finally we can infer the eye movement of the new input by its distances with the clusters in the trainning set. Experimental results demonstrate that our system with quick 4-points calibration not only can reduce the run-time cost, but also provide another way to mesure eye movements without mesuring gaze coordinates to a HCI application such as our eye control system.","","978-9-8975-8133-5","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7295074","Appearance Eye Descriptor;Appearance-based Method;Manifold Learning;Spectral Clustering;Human-computer Interaction","Manifolds;Calibration;Laplace equations;Feature extraction;Control systems;Histograms;Human computer interaction","","","","","","24","","12 Oct 2015","","","IEEE","IEEE Conferences"
"Efficient PERCLOS and Gaze Measurement Methodologies to Estimate Driver Attention in Real Time","S. Darshana; D. Fernando; S. Jayawardena; S. Wickramanayake; C. DeSilva","Dept. of Comput. Sci. & Eng., Univ. of Moratuwa, Moratuwa, Sri Lanka; Dept. of Comput. Sci. & Eng., Univ. of Moratuwa, Moratuwa, Sri Lanka; Dept. of Comput. Sci. & Eng., Univ. of Moratuwa, Moratuwa, Sri Lanka; Dept. of Comput. Sci. & Eng., Univ. of Moratuwa, Moratuwa, Sri Lanka; Dept. of Comput. Sci. & Eng., Univ. of Moratuwa, Moratuwa, Sri Lanka","2014 5th International Conference on Intelligent Systems, Modelling and Simulation","1 Oct 2015","2014","","","289","294","Drivers not being cautious enough is one of the major reasons for many of today's fatal road accidents. Drivers being fatigued or distracted have been identified as the main two reasons behind drivers losing their attention. PERCLOS and gaze estimation are two visual cue based parameters which can be used to estimate driver drowsiness and distraction respectively. This paper describes advanced and efficient methodologies for obtaining these two parameters. We use an infrared sensitive camera equipped with infrared LEDs in obtaining visual features of the driver. In PERCLOS estimation, for each frame, edge detected eye images are classified using a linear support vector machine. Exponentially smoothed vertical and horizontal movements of the pupils are taken into consideration in gaze estimation. The proposed methodology for eye state detection achieves real time recognition accuracy 83.64% whereas gaze estimation methodology achieves 80.5% accuracy.","2166-0670","978-1-4799-3858-2","10.1109/ISMS.2014.56","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7280923","Drowsiness;Distraction;Fatigue;PERCLOS;Gaze;SVM;Binary PCA","Accuracy;Image edge detection;Support vector machines;Principal component analysis;Face detection;Face;Real-time systems","cameras;driver information systems;edge detection;eye;feature extraction;gaze tracking;image classification;infrared imaging;principal component analysis;road accidents;road safety;support vector machines","gaze measurement methodologies;driver attention estimation;fatal road accidents;driver fatigue;gaze estimation;visual cue based parameters;driver drowsiness;driver distraction;infrared sensitive camera;infrared LEDs;visual features;PERCLOS estimation;edge detected eye images classification;linear support vector machine;pupils exponentially smoothed vertical movements;pupils horizontal movements;eye state detection;real time recognition accuracy;PCA;principal component analysis","","6","","17","","1 Oct 2015","","","IEEE","IEEE Conferences"
"GMM-based saliency aggregation for calibration-free gaze estimation","J. Choi; B. Ahn; J. Park; I. S. Kweon","Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea; Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea","2014 IEEE International Conference on Image Processing (ICIP)","29 Jan 2015","2014","","","1096","1099","A typical gaze estimator needs an explicit personal calibration stage with many discrete fixation points. This limitation can be resolved by mapping multiple eye images and corresponding saliency maps of a video clip during an implicit calibration stage. Compared to previous calibration-free methods, our approach clusters eye images by using Gaussian Mixture Model (GMM) in order to increase calibration accuracy and reduce training redundancy. Eye feature vectors representing eye images undergo soft clustering with GMM as well as the corresponding saliency maps for aggregation. The GMM based soft-clustering boosts the accuracy of Gaussian process regression which maps between eye feature vectors and gaze directions given this constructed data. The experimental results show an increase in gaze estimation accuracy compared to previous works on calibration-free method.","2381-8549","978-1-4799-5751-4","10.1109/ICIP.2014.7025218","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025218","Gaze estimation;Saliency;Gaussian mixture model;Gaussian process regression","Estimation;Vectors;Calibration;Accuracy;Computational modeling;Cameras;Clustering methods","calibration;estimation theory;Gaussian processes;gaze tracking;mixture models;video signal processing","GMM-based saliency aggregation;calibration-free gaze estimation;personal implicit calibration stage accuracy;discrete fixation points;multiple eye image mapping;saliency maps;video clip;soft clusters eye images;Gaussian mixture model;training redundancy reduction;eye feature vectors;Gaussian process regression;gaze directions;data construction;real-time free head pose gaze tracking system","","2","","11","","29 Jan 2015","","","IEEE","IEEE Conferences"
"Hierarchical gaze estimation based on adaptive feature learning","X. Wang; K. Xue; D. Nam; J. Han; H. Wang","Samsung R&D Institute China, Beijing, China; Samsung R&D Institute China, Beijing, China; Samsung Advanced Institute of Technology, Yongin, Korea; Samsung Advanced Institute of Technology, Yongin, Korea; Samsung R&D Institute China, Beijing, China","2014 IEEE International Conference on Image Processing (ICIP)","29 Jan 2015","2014","","","3347","3351","Existing appearance-based gaze estimation methods suffer from tedious calibration and appearance variation caused by head movement. In this paper, to handle this problem, we propose a novel appearance-based gaze estimation method by introducing supervised adaptive feature extraction and hierarchical mapping model. Firstly, an adaptive feature learning method is proposed to extract topology-preserving (TOP) feature individually. Then hierarchical mapping method is proposed to localize gaze position based on coarse-to-fine strategy. Appearance synthesis approach is used to increase the refer sample density. Experiments show that under the condition of sparse calibration, proposed method has better performance in accuracy than existing methods under fixed head pose without chinrest. Moreover, our method can be easily extended for head pose-varying gaze estimation.","2381-8549","978-1-4799-5751-4","10.1109/ICIP.2014.7025677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025677","gaze estimation;feature learning;appearance synthesis;hierarchical mapping","Calibration;Estimation;Feature extraction;Training;Interpolation;Head;Optical imaging","feature extraction;gaze tracking;learning (artificial intelligence);pose estimation;topology","hierarchical gaze estimation;gaze estimation methods;head movement;supervised adaptive feature extraction;hierarchical mapping model;adaptive feature learning method;topology preserving extraction;TOP feature;appearance synthesis approach;coarse-to-fine strategy;sparse calibration","","2","","17","","29 Jan 2015","","","IEEE","IEEE Conferences"
"Joint estimation of head pose and visual focus of attention","Y. Huang; D. Duan; J. Cui; F. Davoine; L. Wang; H. Zha","Key Lab. of Machine Perception (MoE), School of EECS; Key Lab. of Machine Perception (MoE), School of EECS; Key Lab. of Machine Perception (MoE), School of EECS; CNRS, LIAMA Sino-European Lab., Beijing; Psychology Department, Peking University, Beijing, China; Key Lab. of Machine Perception (MoE), School of EECS","2014 IEEE International Conference on Image Processing (ICIP)","29 Jan 2015","2014","","","3332","3336","Head pose is an important indicator of a person's visual focus of attention (VFoA). A traditional way to recognize VFoA is to consider accurate head pose or gaze estimations. However, these estimations usually degrade drastically in middle or low resolution video data. In this paper, a joint estimation of head pose and VFoA is proposed to address this issue; both head pose and VFoA are iteratively refined until convergence. This approach is evaluated in a specific scenario involving children around a table playing together with toys. Datasets are acquired and annotated by psychologists in Peking university. The experimental results demonstrate the usefulness of the join estimation process to recognize visual focus of attention in middle resolution video sequences.","2381-8549","978-1-4799-5751-4","10.1109/ICIP.2014.7025674","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7025674","Head pose estimation;visual focus of attention;joint estimation;low resolution","Magnetic heads;Head;Estimation;Visualization;Joints;Accuracy;Pattern recognition","image resolution;image sequences;iterative methods;pose estimation","head pose estimation;visual attention focus;visual focus of attention;VFoA recognition;gaze estimations;iterative refinement;psychologists;Peking University;middle resolution;video sequences","","1","","20","","29 Jan 2015","","","IEEE","IEEE Conferences"
"Iris center localization using integral projection and gradients","D. Li; F. Hu; L. Wang; M. Zhang","Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China; Department of Information Science and Electronic Engineering, Zhejiang University, Hangzhou 310027, China","2014 International Conference on Audio, Language and Image Processing","15 Jan 2015","2014","","","211","215","Iris center localization becomes more and more popular in the applications of face alignment, human-computer interaction and gaze estimation. However, techniques for low-resolution and low-contrast eye images which are taken in uncontrolled circumstance still need improvement. This paper presents an accurate and fast algorithm to realize iris center localization, by combining integral projection and image gradients. First, the vertical integral projection function is adopted to get precise boundaries of the eyes and to generate a weight map. Second, a novel method combining dot products of gradients with the weight map is proposed. The iris center is detected by searching the maximum of multiplication between the dot products and the weight from the map. A commonly used evaluation is applied to test our result on the very challenging BioID database for eye center and iris center localization. Experiments show that this method can accurately locate irises under natural light conditions and has characteristic of low computation complexity.","","978-1-4799-3903-9","10.1109/ICALIP.2014.7009788","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7009788","Eye center localization;pupil and iris localization;image gradients;integral projection","Iris;Face;Databases;Eyebrows;Iris recognition;Estimation;Lighting","computational complexity;gaze tracking;human computer interaction;iris recognition","iris center localization;integral projection;face alignment;human-computer interaction;gaze estimation;low-resolution eye images;low-contrast eye images;image gradients;gradient dot products;weight map;BioID database;eye center;natural light conditions;low computation complexity","","1","","22","","15 Jan 2015","","","IEEE","IEEE Conferences"
"Segmenting the periocular region using a hierarchical graphical model fed by texture / shape information and geometrical constraints","H. Proença; J. C. Neves; G. Santos","IT - Instituto de Telecomunicações, University of Beira Interior, Portugal; IT - Instituto de Telecomunicações, University of Beira Interior, Portugal; IT - Instituto de Telecomunicações, University of Beira Interior, Portugal","IEEE International Joint Conference on Biometrics","29 Dec 2014","2014","","","1","7","Using the periocular region for biometric recognition is an interesting possibility: this area of the human body is highly discriminative among subjects and relatively stable in appearance. In this paper, the main idea is that improved solutions for defining the periocular region-of-interest and better pose / gaze estimates can be obtained by segmenting (labelling) all the components in the periocular vicinity. Accordingly, we describe an integrated algorithm for labelling the periocular region, that uses a unique model to discriminate between seven components in a single-shot: iris, sclera, eyelashes, eyebrows, hair, skin and glasses. Our solution fuses texture / shape descriptors and geometrical constraints to feed a two-layered graphical model (Markov Random Field), which energy minimization provides a robust solution against uncontrolled lighting conditions and variations in subjects pose and gaze.","","978-1-4799-3584-0","10.1109/BTAS.2014.6996228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996228","","Eyebrows;Iris;Cornea;Feature extraction;Labeling;Eyelashes","computational geometry;feature extraction;gaze tracking;image fusion;image segmentation;image texture;lighting;minimisation;pose estimation","periocular region segmentation;hierarchical graphical model;texture information;shape information;biometric recognition;human body;periocular region-of-interest;pose estimation;gaze estimation;periocular component segmentation;periocular component labelling;iris;sclera;eyelashes;eyebrows;hair;skin;glasses;texture-shape descriptor fusion;geometrical constraints;two-layered graphical model;Markov random field;energy minimization;uncontrolled lighting conditions;pose variations;gaze variations","","6","","21","","29 Dec 2014","","","IEEE","IEEE Conferences"
"Gaze estimation as a framework for iris liveness detection","I. Rigas; O. V. Komogortsev","Texas State University, 601 University Drive, San Marcos, 78666, USA; Texas State University, 601 University Drive, San Marcos, 78666, USA","IEEE International Joint Conference on Biometrics","29 Dec 2014","2014","","","1","8","This work investigates the possibility of detecting iris print-attacks via the analysis of a number of gaze-related features acquired in a process of eye tracking. Gaze estimation algorithms employ models based on the physical structure and function of the eye, providing thus a number of salient features that can be potentially employed for the detection of spoofing print-attacks. In our study, a combined dataset was assembled for the investigation of these features, consisting of eye movement recordings and the corresponding iris images collected from 100 subjects. The collected iris images were utilized in direct implementation of iris print-attacks against an eye tracking device. We developed a methodology for the detection of spoof indicative artifacts in the recorded signals, and fed the extracted features from the live and spoof eye signals into a two-class SVM classifier. The obtained results indicate a best correct classification rate (CCR) of 95.7%. Furthermore, we demonstrate the moderate decrease in liveness detection rates during subsampling of the eye movement signal to frequencies as low as 15 Hz. This result indicates the usefulness of running gaze estimation algorithms on existing iris recognition devices where such sampling frequency rate is common.","","978-1-4799-3584-0","10.1109/BTAS.2014.6996282","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6996282","","Iris recognition;Estimation;Reflection;Visualization;Feature extraction;Optical imaging","feature extraction;image classification;iris recognition;security of data;support vector machines","iris liveness detection;iris print-attacks;gaze-related features;gaze estimation algorithms;spoofing print-attacks;eye movement recordings;iris images;eye tracking device;spoof indicative artifacts;feature extraction;spoof eye signals;two-class SVM classifier;iris recognition devices","","8","4","22","","29 Dec 2014","","","IEEE","IEEE Conferences"
"Appearance-Based Gaze Tracking with Free Head Movement","C. Lai; Y. Chen; K. Chen; S. Chen; S. Shih; Y. Hung",NA; NA; NA; NA; NA; NA,"2014 22nd International Conference on Pattern Recognition","6 Dec 2014","2014","","","1869","1873","In this work, we develop an appearance-based gaze tracking system allowing user to move their head freely. The main difficulty of the appearance-based gaze tracking method is that the eye appearance is sensitive to head orientation. To overcome the difficulty, we propose a 3-D gaze tracking method combining head pose tracking and appearance-based gaze estimation. We use a random forest approach to model the neighbor structure of the joint head pose and eye appearance space, and efficiently select neighbors from the collected high dimensional data set. L<sub>1</sub>-optimization is then used to seek for the best solution for regression from the selected neighboring samples. Experiment results shows that it can provide robust binocular gaze tracking results with less constraints but still provides moderate estimation accuracy of gaze estimation.","1051-4651","978-1-4799-5209-0","10.1109/ICPR.2014.327","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977039","","Head;Estimation;Tracking;Training;Magnetic heads;Accuracy;Cameras","gaze tracking;optimisation;pose estimation;regression analysis;visual databases","free head movement;appearance-based gaze tracking method;head orientation;3D gaze tracking method;head pose tracking;random forest approach;neighbor structure;eye appearance space;high dimensional data set;L1-optimization;regression method;robust binocular gaze tracking","","7","","18","","6 Dec 2014","","","IEEE","IEEE Conferences"
"Manifold Alignment for Person Independent Appearance-Based Gaze Estimation","T. Schneider; B. Schauerte; R. Stiefelhagen","Comput. Vision for Human Comput. Interaction Lab., Karlsruhe Inst. of Technol., Karlsruhe, Germany; Comput. Vision for Human Comput. Interaction Lab., Karlsruhe Inst. of Technol., Karlsruhe, Germany; Comput. Vision for Human Comput. Interaction Lab., Karlsruhe Inst. of Technol., Karlsruhe, Germany","2014 22nd International Conference on Pattern Recognition","6 Dec 2014","2014","","","1167","1172","We show that dually supervised manifold embedding can improve the performance of machine learning based person-independent and thus calibration-free gaze estimation. For this purpose, we perform a manifold embedding for each person in the training dataset and then learn a linear transformation that aligns the individual, person-dependent manifolds. We evaluate the effect of manifold alignment on the recently presented Columbia dataset, where we analyze the influence on 6 regression methods and 8 feature variants. Using manifold alignment, we are able to improve the person-independent gaze estimation performance by up to 31.2 % compared to the best approach without manifold alignment.","1051-4651","978-1-4799-5209-0","10.1109/ICPR.2014.210","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976920","","Manifolds;Estimation;Training;Principal component analysis;Vectors;Head;Discrete cosine transforms","gaze tracking;learning (artificial intelligence);object detection;regression analysis","eye corner detection;regression methods;Columbia dataset;person-dependent manifolds;linear transformation;calibration-free gaze estimation;machine learning;dually supervised manifold embedding;person independent appearance-based gaze estimation;manifold alignment","","36","1","34","","6 Dec 2014","","","IEEE","IEEE Conferences"
"Gaze Estimation Based on 3D Face Structure and Pupil Centers","C. Xiong; L. Huang; C. Liu","Inst. of Autom., Beijing, China; Inst. of Autom., Beijing, China; Inst. of Autom., Beijing, China","2014 22nd International Conference on Pattern Recognition","6 Dec 2014","2014","","","1156","1161","It is a challenging problem to realize a robust and low cost gaze estimation system. Most existing feature-based gaze estimation methods strongly rely on cornea reflections, which are unstable to glasses, head movements and natural light. In this paper, we propose a novel gaze estimation method without use of cornea reflections based on a stereo camera system. Firstly, 3D Active Shape Models (ASM) is reconstructed using stereo vision to represent 3D face structure. Then, without use of cornea reflections, a 3D Pupil-Eye-Contours based feature is proposed to represent human gaze information. What's more, precise estimation of head poses based on 3D face structure is employed to rectify the 3D pupil centers and eye contours for improving the ability of tolerance to head movements. Experiments on fifteen subjects show that the system is accurate and allows natural head movements.","1051-4651","978-1-4799-5209-0","10.1109/ICPR.2014.208","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976918","gaze estimation;3D face structure;head pose","Three-dimensional displays;Estimation;Face;Calibration;Feature extraction;Solid modeling","cameras;gaze tracking;image motion analysis;image reconstruction;image representation;pose estimation;stereo image processing","robust gaze estimation system;3D face structure;pupil centers;low cost gaze estimation system;feature-based gaze estimation methods;natural light;cornea reflections;stereo camera system;3D active shape models;ASM;stereo vision;3D pupil-eye-contours based feature;human gaze information;natural head movements;head pose estimation","","4","","18","","6 Dec 2014","","","IEEE","IEEE Conferences"
"3-D Gaze Tracking Using Pupil Contour Features","C. Lai; S. Shih; H. Tsai; Y. Hung","Grad. Inst. of Networking & Multimedia, Nat. Taiwan Univ., Taipei, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Chi Nan Univ., Nantou, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ., Taipei, Taiwan; Grad. Inst. of Networking & Multimedia, Nat. Taiwan Univ., Taipei, Taiwan","2014 22nd International Conference on Pattern Recognition","6 Dec 2014","2014","","","1162","1166","Glint features have important roles in gaze tracking systems. But when the operation range of a gaze tracking system is enlarged, the performance of glint-feature-based (GFB) approaches will be degraded mainly due to the curvature variation problem at around the edge of the cornea. Although the pupil contour feature may provide complementary information to help estimating the eye gaze, existing methods do not properly handle the cornea refraction problem, leading to inaccurate results. This paper describes a contour-feature-based (CFB) 3-D gaze tracking method that is compatible to cornea refraction. Experiments show the effectiveness of the proposed approach for gaze tracking.","1051-4651","978-1-4799-5209-0","10.1109/ICPR.2014.209","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976919","","Cornea;Cameras;Tracking;Vectors;Estimation;Light sources;Accuracy","edge detection;feature extraction;gaze tracking","3D gaze tracking;pupil contour features;gaze tracking system;glint feature based;curvature variation problem;pupil contour feature;eye gaze estimation;cornea refraction problem;gaze tracking method","","1","","11","","6 Dec 2014","","","IEEE","IEEE Conferences"
"Attention estimation by simultaneous analysis of viewer and view","A. Tawari; A. Møgelmose; S. Martin; T. B. Moeslund; M. M. Trivedi","Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States; Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States; Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States; Visual Analysis of People Lab, Aalborg University, Denmark; Laboratory for Intelligent and Safe Automobiles, UC San Diego, United States","17th International IEEE Conference on Intelligent Transportation Systems (ITSC)","20 Nov 2014","2014","","","1381","1387","This paper introduces a system for estimating the attention of a driver wearing a first person view camera using salient objects to improve gaze estimation. A challenging data set of pedestrians crossing intersections has been captured using Google Glass worn by a driver. A challenge unique to first person view from cars is that the interior of the car can take up a large part of the image. The proposed system automatically filters out the dashboard of the car, along with other parts of the instrumentation. The remaining area is used as a region of interest for a pedestrian detector. Two cameras looking at the driver are used to determine the direction of the driver's gaze, by examining the eye corners and the center of the iris. This coarse gaze estimation is then linked to the detected pedestrians to determine which pedestrian the driver is focused on at any given time.","2153-0017","978-1-4799-6078-1","10.1109/ITSC.2014.6957880","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6957880","","Vehicles;Cameras;Iris;Head;Estimation;Videos;Synchronization","automobiles;driver information systems;gaze tracking;image sensors;object detection;pedestrians;traffic engineering computing","attention estimation;simultaneous viewer-view analysis;driver attention;first person view camera;salient objects;Google Glass;first person view;car interior;car dashboard;pedestrian detector;eye corners;coarse gaze estimation","","19","","22","","20 Nov 2014","","","IEEE","IEEE Conferences"
"Real-Time Gaze Estimation with Online Calibration","L. Sun; M. Song; Z. Liu; M. Sun","Zhejiang University, China; Zhejiang University, China; Microsoft Research; University of Washington, Seattle","IEEE MultiMedia","3 Nov 2014","2014","21","4","28","37","Gaze-tracking technology is highly valuable in many interactive and diagnostic applications. For many gaze estimation systems, calibration is an unavoidable procedure necessary to determine certain person-specific parameters, either explicitly or implicitly. Recently, several offline implicit calibration methods have been proposed to ease the calibration burden. However, the calibration procedure is still cumbersome, and gaze estimation accuracy needs further improvement. In this article, the authors present a novel 3D gaze estimation system with online calibration. The proposed system is based on a new 3D model-based gaze estimation method using a single consumer depth camera sensor (via Kinect). Unlike previous gaze estimation methods using explicit offline calibration with fixed number of calibration points or implicit calibration, their approach constantly improves person-specific eye parameters through online calibration, which enables the system to adapt gradually to a new user. The experimental results and the human-computer interaction (HCI) application show that the proposed system can work in real time with superior gaze estimation accuracy and minimal calibration burden.","1941-0166","","10.1109/MMUL.2014.54","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6916495","multimedia;eye tracking;3D model-based gaze estimation;gaze direction;online calibration;HCI","Calibration;Three-dimensional displays;Research and development;Target tracking;Medical diagnostic imaging;Cameras;Real-time systems","calibration;gaze tracking;human computer interaction;image sensors;solid modelling","online calibration;gaze-tracking technology;interactive application;diagnostic application;person-specific parameters;offline implicit calibration methods;3D model-based gaze estimation method;depth camera sensor;Kinect;human-computer interaction application;HCI application","","14","3","10","IEEE","7 Oct 2014","","","IEEE","IEEE Magazines"
"Design and implementation of gaze tracking system with iPad","Jiajin Zhang; Liu Di; Lichang Chen","School of Science and Information Engineering, Yunnan Agricultural University, Kunming, China; School of Mechanic and Electronic Engineering, Yunnan Agricultural University, Kunming, China; School of Mechanic and Electronic Engineering, Yunnan Agricultural University, Kunming, China","2014 IEEE 20th International Conference on Embedded and Real-Time Computing Systems and Applications","29 Sep 2014","2014","","","1","5","Gaze tracking is the process of measuring gaze point or the motion of an eye relative to the head. Gaze tracking technique provides us a brand new way of human computer interaction. In addition, eye gaze tracking can be also applied to support seriously disabled people in using computer. In this paper, we explored the use of eye gaze tracking technology on a tablet device, designed and implemented an eye tracking system on an iPad device. In our system, gaze estimation is based on analyzing the appearances of eyes which are retrieved by the built-in camera on the iPad. Artificial neural networks are employed to estimate the location of the user gaze from the eye region image. The results indicate that it is possible to obtain an accuracy of 82.5% with the proposed system.","2325-1301","978-1-4799-3953-4","10.1109/RTCSA.2014.6910542","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6910542","gaze tracking;tablet device;artificial neural networks;region detection","Neural networks;Head;Iris;Tracking;Cameras;Accuracy;Iris recognition","estimation theory;gaze tracking;handicapped aids;human computer interaction;neural nets;notebook computers","iPad;human computer interaction;seriously disabled people;eye gaze tracking technology;tablet device;gaze estimation;artificial neural network","","","","11","","29 Sep 2014","","","IEEE","IEEE Conferences"
"Eye-Model-Based Gaze Estimation by RGB-D Camera","L. Jianfeng; L. Shigang","Grad. Sch. of Eng., Tottori Univ., Tottori, Japan; Grad. Sch. of Eng., Tottori Univ., Tottori, Japan","2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops","25 Sep 2014","2014","","","606","610","This paper proposes a method of eye-model-based gaze estimation by RGB-D camera, Kinect sensor. Different from other methods, our method sets up a model to calibrate the eyeball center by gazing at a target in 3D space, not predefined. And then by detecting the pupil center, we can estimate the gaze direction. To achieve this algorithm, we first build a head model relying on Kinect sensor, then obtaining the 3D information of pupil center. As we need to know the eyeball center position in head model, we do a calibration by designing a target to gaze. Because the ray from eyeball center to target and the ray from eyeball center to pupil center should meet a relationship, we can have an equation to solve the real eyeball center position. After calibration, we can have a gaze estimation automatically at any time. Our method allows free head motion and it only needs a simple device, finally it also can run automatically in real-time. Experiments show that our method performs well and still has a room for improvement.","2160-7516","978-1-4799-4308-1","10.1109/CVPRW.2014.93","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6910042","gaze estimation;kinect sensor;eyeball calibration;free head motion;pupil center detection","Estimation;Head;Three-dimensional displays;Calibration;Cameras;Feature extraction;Solid modeling","gaze tracking;motion estimation;object detection;sensors","eye-model-based gaze estimation;RGB-D camera;Kinect sensor;eyeball center calibration;pupil center detection;gaze direction estimation","","20","2","17","","25 Sep 2014","","","IEEE","IEEE Conferences"
"Learning-by-Synthesis for Appearance-Based 3D Gaze Estimation","Y. Sugano; Y. Matsushita; Y. Sato","Univ. of Tokyo, Tokyo, Japan; Microsoft Res. Asia, Beijing, China; Univ. of Tokyo, Tokyo, Japan","2014 IEEE Conference on Computer Vision and Pattern Recognition","25 Sep 2014","2014","","","1821","1828","Inferring human gaze from low-resolution eye images is still a challenging task despite its practical importance in many application scenarios. This paper presents a learning-by-synthesis approach to accurate image-based gaze estimation that is person- and head pose-independent. Unlike existing appearance-based methods that assume person-specific training data, we use a large amount of cross-subject training data to train a 3D gaze estimator. We collect the largest and fully calibrated multi-view gaze dataset and perform a 3D reconstruction in order to generate dense training data of eye images. By using the synthesized dataset to learn a random regression forest, we show that our method outperforms existing methods that use low-resolution eye images.","1063-6919","978-1-4799-5118-5","10.1109/CVPR.2014.235","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909631","","Head;Three-dimensional displays;Estimation;Cameras;Training;Training data;Vegetation","estimation theory;gaze tracking;image reconstruction;image resolution;pose estimation;random processes;regression analysis","learning-by-synthesis;appearance-based 3D gaze estimation;human gaze;low-resolution eye images;image-based gaze estimation;person-independent;head pose-independent;person-specific training data;cross-subject training data;multiview gaze dataset;3D reconstruction;dense training data;synthesized dataset;random regression forest","","122","1","36","","25 Sep 2014","","","IEEE","IEEE Conferences"
"Geometric Generative Gaze Estimation (G<sup>3</sup>E) for Remote RGB-D Cameras","K. A. Funes Mora; J. Odobez","Res. Inst., Martigny, Switzerland; Ecole Polytech. Fed. de Lausanne, Lausanne, Switzerland","2014 IEEE Conference on Computer Vision and Pattern Recognition","25 Sep 2014","2014","","","1773","1780","We propose a head pose invariant gaze estimation model for distant RGB-D cameras. It relies on a geometric understanding of the 3D gaze action and generation of eye images. By introducing a semantic segmentation of the eye region within a generative process, the model (i) avoids the critical feature tracking of geometrical approaches requiring high resolution images, (ii) decouples the person dependent geometry from the ambient conditions, allowing adaptation to different conditions without retraining. Priors in the generative framework are adequate for training from few samples. In addition, the model is capable of gaze extrapolation allowing for less restrictive training schemes. Comparisons with state of the art methods validate these properties which make our method highly valuable for addressing many diverse tasks in sociology, HRI and HCI.","1063-6919","978-1-4799-5118-5","10.1109/CVPR.2014.229","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909625","gaze estimation;generative model;segmentation;RGB-D;HRI;HCI;HHI","Eyelids;Head;Image color analysis;Three-dimensional displays;Image segmentation;Training;Visualization","cameras;gaze tracking;image resolution;image segmentation","remote RGB-D cameras;geometric generative gaze estimation;G3E;head pose invariant gaze estimation;distant RGB-D cameras;3D gaze action;eye image generation;semantic segmentation;eye region;high resolution images;person dependent geometry;ambient conditions;gaze extrapolation;diverse tasks;sociology","","21","2","18","","25 Sep 2014","","","IEEE","IEEE Conferences"
"An improved gaze tracking technique based on eye model","X. Li; Z. Li; J. Qin","School of Electronic Information & Control Engineering, Beijing University of Technology, 100124, China; School of Electronic Information & Control Engineering, Beijing University of Technology, 100124, China; School of Electronic Information & Control Engineering, Beijing University of Technology, 100124, China","Proceedings of the 33rd Chinese Control Conference","15 Sep 2014","2014","","","7286","7291","Aiming at several common drawbacks of existing gaze tracking algorithm which is based on image processing technology: low accuracy, low suitability and high requirement of device, this paper presents an improved gaze tracking method based on the human eye model. The vision scatter model is established by studying the relationship among the center of iris, inner corner of eye and the gaze direction. The model is used for gaze rough estimation. An improved geometric eye model with compensation coefficient is used for gaze precise estimation, and the coefficient depends on the result of rough estimation. The result of experiment shows that the proposed approach has a low average error which is reached 2 ° and good robustness. This approach provides an effective solution for gaze tracking of human-computer interaction under the low pixel condition.","1934-1768","978-9-8815-6387-3","10.1109/ChiCC.2014.6896207","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6896207","gaze tracking;iris tracking;scatter model;gaze estimation","Iris;Estimation;Feature extraction;Accuracy;Cameras;Target tracking","computer vision;eye;gaze tracking;human computer interaction","gaze tracking technique;image processing technology;human eye model;vision scatter model;iris center;inner eye corner;gaze direction;gaze rough estimation;geometric eye model;compensation coefficient;gaze precise estimation;human-computer interaction;low pixel condition","","2","","11","","15 Sep 2014","","","IEEE","IEEE Conferences"
"A system for monitoring the engagement of remote online students using eye gaze estimation","P. Khorrami; Vuong Le; J. C. Hart; T. S. Huang","Beckman Institute, University of Illinois at Urbana Champaign, USA; Beckman Institute, University of Illinois at Urbana Champaign, USA; Department of Computer Science, University of Illinois at Urbana Champaign, USA; Beckman Institute, University of Illinois at Urbana Champaign, USA","2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","8 Sep 2014","2014","","","1","6","We present a system for monitoring the engagement of remote students in an online educational environment. We introduce a novel gaze estimation system that uses a client-side video-based 3D face tracking algorithm, and expands it to estimate the head pose, and intersects this pose-based gaze line of sight with the screen (coplanar with the camera) to estimate the screen position currently viewed. Our preliminary experiments indicate this approach can accurately determine the coarse location of a student's gaze and use it as a cue for engagement estimation. Our system operates in real-time and achieves comparable performance to state-of-the-art gaze estimation techniques that account for free head motion.","1945-7871","978-1-4799-4717-1","10.1109/ICMEW.2014.6890573","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890573","Student Monitoring System;Face Tracking;Head Pose Estimation;Gaze Estimation","Monitoring;Estimation;Three-dimensional displays;Face;Servers;Tracking","computer aided instruction;face recognition;gaze tracking;image motion analysis;pose estimation;video signal processing","eye gaze estimation;remote online student engagement;online educational environment;client-side video-based 3D face tracking algorithm;head pose;pose-based gaze line of sight;coarse location;free head motion","","2","","13","","8 Sep 2014","","","IEEE","IEEE Conferences"
"Realtime gaze estimation with online calibration","L. Sun; M. Song; Z. Liu; M. Sun","Zhejiang University, Hangzhou, 310027, P.R. China; Zhejiang University, Hangzhou, 310027, P.R. China; Microsoft Research, Redmond, WA 98052; University of Washingtong, Seattle, WA 98195","2014 IEEE International Conference on Multimedia and Expo (ICME)","8 Sep 2014","2014","","","1","6","For an eye gaze estimation system, calibration is an unavoidable procedure to determine certain person-specific parameters, either explicitly or implicitly. Although several offline implicit calibration methods have been proposed to ease the calibration burden, the calibration procedure is still cumbersome and the gaze estimation accuracy needs further improvement. In this paper, we propose a novel 3D gaze estimation system with online calibration. The proposed system uses a new 3D model-based gaze estimation method with a single consumer camera (Kinect). Unlike previous gaze estimation methods using explicit offline calibration with fixed number of calibration points or implicit calibration, our approach constantly improves person-specific eye parameters through online calibration, which enables the system to adapt gradually to a new user. The experimental results and the human-computer interaction (HCI) application show that the proposed system can work in realtime with superior gaze estimation accuracy (<; 2°) and minimal calibration burden.","1945-788X","978-1-4799-4761-4","10.1109/ICME.2014.6890322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6890322","gaze estimation;3D model-based;gaze direction;online calibration;HCI","Calibration;Three-dimensional displays;Estimation;Iris;Head;Solid modeling;Cameras","calibration;cameras;feature extraction;gaze tracking;human computer interaction;solid modelling","person-specific parameter;3D eye gaze estimation system;online calibration;consumer camera;Kinect;human-computer interaction application;HCI application;minimal calibration burden;gaze feature extraction","","4","","8","","8 Sep 2014","","","IEEE","IEEE Conferences"
"Eye gaze estimation based on ellipse fitting and three-dimensional model of eye for “Intelligent Poster”","R. Urano; R. Suzuki; T. Sasaki","Shibaura Institute of Technology, Minato-ku, Tokyo, Japan; Shibaura Institute of Technology, Minato-ku, Tokyo, Japan; Shibaura Institute of Technology, Minato-ku, Tokyo, Japan","2014 IEEE/ASME International Conference on Advanced Intelligent Mechatronics","14 Aug 2014","2014","","","1157","1162","The designers judge the advertisement design based on subjective assessment only. But we think that they need an objective assessment to evaluate it. In this research, we propose a novel system which estimates our interest in posters by our eye gaze and it is called “Intelligent Poster”. We hereby get an objective rating of the poster design and knowledge for making good posters. This paper focuses on eye gaze estimation which meets some requirements and has enough accuracy and robustness for “Intelligent Poster”. Many researchers propose various eye gaze estimation, for example, using corneal reflection, wearing measurement hardware, using only cameras. In these days, we can get high resolution cameras in cheap with developing the technology. So, we can simply construct an eye gaze system by using a camera. We combine two methods based on ellipse fitting and three-dimensional model to realize the robust estimation. The achieved accuracy of our proposed method is 6 deg.","2159-6255","978-1-4799-5736-1","10.1109/AIM.2014.6878237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6878237","","Estimation;Cameras;Iris;Fitting;Solid modeling;Accuracy;Shape","cameras;gaze tracking;solid modelling","robust eye gaze estimation;ellipse fitting;intelligent poster;three-dimensional eye model;poster design;corneal reflection;wearing measurement hardware;high resolution cameras","","4","","12","","14 Aug 2014","","","IEEE","IEEE Conferences"
"Head movement compensation algorithm in multi-display communication by gaze","T. Kocejko; A. Bujnowski; J. Ruminski; E. Bylinska; J. Wtorek","Biomedical Engineering Department, Gdansk University of Technology, GUT, Poland; Biomedical Engineering Department, Gdansk University of Technology, GUT, Poland; Biomedical Engineering Department, Gdansk University of Technology, GUT, Poland; Biomedical Engineering Department, Gdansk University of Technology, GUT, Poland; Biomedical Engineering Department, Gdansk University of Technology, GUT, Poland","2014 7th International Conference on Human System Interactions (HSI)","21 Jul 2014","2014","","","88","94","An influence of head movements on the gaze estimation accuracy when using a head mounted eye tracking system is discussed in the paper. This issue has been examined for a multi-display environment. It was found that head movement (rotation) to some extent does not influence on the gaze estimation accuracy seriously. Acceptable results were obtained when using eye-tracker to communicate with a computer via in two displays simultaneously.","2158-2254","978-1-4799-4714-0","10.1109/HSI.2014.6860454","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6860454","eye tracking;gaze tracking;head movement compensation;multi-display environment;multi display;head rotation","Rotation measurement;Head;Accuracy;Tracking;Calibration;Cameras;Position measurement","gaze tracking;human computer interaction;user interfaces","head movement compensation algorithm;multidisplay communication;gaze estimation accuracy;head mounted eye tracking system;eye-tracker","","6","","13","","21 Jul 2014","","","IEEE","IEEE Conferences"
"Gaze tracking system for teleoperation","M. Yu; X. Wang; Y. Lin; X. Bai","School of Automation, Beijing Institute of Technology, 100086, China; School of Automation, Beijing Institute of Technology, 100086, China; Department of Mechanical and Industrial Engineering, Northeastern University, Boston 02115, USA; School of Automation, Beijing Institute of Technology, 100086, China","The 26th Chinese Control and Decision Conference (2014 CCDC)","14 Jul 2014","2014","","","4617","4622","Conventional control interfaces such as joystick, keyboard, and mouse are widely used in teleoperation. However, they are not operated by some users such as who have disabled hands. This paper proposed an eye gaze tracking system for teleoperation. The eye gaze tracking system is based on non-intrusive, visible light, iris center corneal reflection (ICCR) and 2-D mapping-based gaze estimation. In addition, this paper also presented a simple and effective 2-D mapping-based calibration method in order to users with head movement. The calibration verification of five users shows the average errors of our proposed gaze tracker on the x- and y directions are 13.3 and 16.4 pixels, respectively. Through the teleoperation test of robotic car within fixed experimental area, the gaze tracking system achieved a good control result.","1948-9447","978-1-4799-3708-0","10.1109/CCDC.2014.6852997","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6852997","gaze tracking;ICCR;teleoperation","Iris;Calibration;Robots;Cameras;Image edge detection;Computers;Tracking","automobiles;gaze tracking;iris recognition;mobile robots;robot vision;telerobotics","gaze tracking system;teleoperation test;robotic car;iris center corneal reflection;ICCR;2D mapping-based gaze estimation;2D mapping-based calibration method;head movement","","5","","7","","14 Jul 2014","","","IEEE","IEEE Conferences"
"A gaze estimation approach for intelligent viewing in tele-operation","A. Mukherjee; J. K. Mukherjee","Electronics, Vivekanand Institute of Technology VESIT, Chembur, Mumbai 400074, India; Machine Intelligence Laboratory. EISD Bhabha Atomic Research Centre, Mumbai 400085, India","Proceedings of the 2014 IEEE Students' Technology Symposium","1 May 2014","2014","","","214","217","Human like viewing is a major challenge in intelligent remote scenario viewing in tele-robotic applications of mobile as well as fixed frame slaves that are interactively controlled in `man in loop' type control. The control being mainly based on console-views available to the operator, the HMI system must understand what the operator wants to look at. The presented work, attempts at developing a method for guessing operator interest zone in remote workspace area by understanding operator's reaction to `console-view'. The approach establishes a linkage for the remote camera control to subtle operator head movement in reaction to console view. The natural operator reaction is interpreted by our gaze estimator through rapidly extracted image clues.","","978-1-4799-2608-4","10.1109/TechSym.2014.6808049","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6808049","Intelligent HMI;head motion interpretation;image-processing;tele-control;robotics;remote viewer","Head;Image color analysis;Light emitting diodes;Robot kinematics;Estimation;Servomotors","gaze tracking;human-robot interaction;robot vision;telerobotics","gaze estimation;intelligent viewing;tele-operation;human like viewing;intelligent remote scenario viewing;tele-robotic applications;fixed frame slaves;man-in-loop type control;HMI system;remote workspace area;remote camera control;operator head movement;console view;natural operator reaction;image clues","","","","11","","1 May 2014","","","IEEE","IEEE Conferences"
"Yaw Estimation Using Cylindrical and Ellipsoidal Face Models","A. Narayanan; R. M. Kaimal; K. Bijlani","Dept. of Comput. Sci., Amrita Vishwa Vidyapeetham, Kollam, India; Dept. of Comput. Sci., Amrita Vishwa Vidyapeetham, Kollam, India; Amrita E-Learning Res. Lab., Amrita Vishwa Vidyapeetham, Kollam, India","IEEE Transactions on Intelligent Transportation Systems","25 Sep 2014","2014","15","5","2308","2320","Accurate head yaw estimation is necessary for detecting driver inattention in forward collision warning systems. In this paper, we propose three geometric models under the ellipsoidal framework for accurate head yaw estimation. We present theoretical analysis of the cylindrical and ellipsoidal face models used for yaw angle estimation of head rotation. The relationship between cylindrical, ellipsoidal, and proposed models is derived. We provide error functions for all models. Furthermore, for each model, over/under estimation of angle, zero crossings of error, bounds on yaw angle estimate, and bounds on error are presented. Experimental results of the proposed models on four standard head pose data sets yielded a mean absolute error between 4° and 8° demonstrating the efficacy of the proposed models over the state-of-the-art methods.","1558-0016","","10.1109/TITS.2014.2313371","National Mission on Education through Information and Communication Technology; Ministry of Human Resource Development, and by the Amrita Vishwa Vidyapeetham; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6807708","Driver monitoring system;gaze estimation;head-orientation estimation;Driver monitoring system;gaze estimation;head-orientation estimation","Face;Estimation;Computational modeling;Solid modeling;Mathematical model;Vehicles","alarm systems;driver information systems;image motion analysis","cylindrical face models;ellipsoidal face models;head yaw estimation;driver inattention detection;forward collision warning systems;geometric models;ellipsoidal framework;yaw angle estimation;head rotation","","13","","55","IEEE","29 Apr 2014","","","IEEE","IEEE Journals"
"Adaptive Linear Regression for Appearance-Based Gaze Estimation","F. Lu; Y. Sugano; T. Okabe; Y. Sato","Institute of Industrial Science , the University of Tokyo, Tokyo, Japan; Institute of Industrial Science , the University of Tokyo, Tokyo, Japan; Institute of Industrial Science , the University of Tokyo, Tokyo, Japan; Institute of Industrial Science , the University of Tokyo, Tokyo, Japan","IEEE Transactions on Pattern Analysis and Machine Intelligence","1 Sep 2014","2014","36","10","2033","2046","We investigate the appearance-based gaze estimation problem, with respect to its essential difficulty in reducing the number of required training samples, and other practical issues such as slight head motion, image resolution variation, and eye blinking. We cast the problem as mapping high-dimensional eye image features to low-dimensional gaze positions, and propose an adaptive linear regression (ALR) method as the key to our solution. The ALR method adaptively selects an optimal set of sparsest training samples for the gaze estimation via ℓ<sup>1</sup>-optimization. In this sense, the number of required training samples is significantly reduced for high accuracy estimation. In addition, by adopting the basic ALR objective function, we integrate the gaze estimation, subpixel alignment and blink detection into a unified optimization framework. By solving these problems simultaneously, we successfully handle slight head motion, image resolution variation and eye blinking in appearance-based gaze estimation. We evaluated the proposed method by conducting experiments with multiple users and variant conditions to verify its effectiveness.","1939-3539","","10.1109/TPAMI.2014.2313123","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6777326","Eye;gaze estimation;face and gesture recognition;sub-pixel alignment;blink detection","Head;Feature extraction;Estimation;Training;Magnetic heads;Accuracy;Image resolution","feature extraction;gaze tracking;image motion analysis;image resolution;optimisation;regression analysis","adaptive linear regression;ALR method;appearance-based gaze estimation;eye image features;ℓ1-optimization;subpixel alignment;blink detection;slight head motion;image resolution variation;eye blinking","","107","","44","IEEE","21 Mar 2014","","","IEEE","IEEE Journals"
"Toward Privacy-Protecting Safety Systems for Naturalistic Driving Videos","S. Martin; A. Tawari; M. M. Trivedi","Lab. for Intell. & Safe Automobiles, Univ. of California San Diego, La Jolla, CA, USA; Lab. for Intell. & Safe Automobiles, Univ. of California San Diego, La Jolla, CA, USA; Lab. for Intell. & Safe Automobiles, Univ. of California San Diego, La Jolla, CA, USA","IEEE Transactions on Intelligent Transportation Systems","31 Jul 2014","2014","15","4","1811","1822","A common pool of naturalistic driving data is necessary to develop and compare algorithms that infer driver behavior, in order to improve driving safety. Naturalistic driving data, such as video sequences of looking at a driver, however, cause concern for the privacy of individual drivers. In an ideal situation, a deidentification filter applied to a raw image of looking at a driver would, semantically, protect the identity and preserve the behavior (e.g, eye gaze, head pose, and hand activity) of the driver. Driver gaze estimation is of particular interest because it is a good indicator of a driver's visual attention and a good predictor of a driver's intent. Interestingly, the same facial features that are explicitly or implicitly used for gaze estimation play a key role in recognizing a person's identity. In this paper, we implement a specific deidentification filter on video sequences of looking at a driver from naturalistic driving and present novel findings on its effect on face recognition and driver gaze-zone estimation.","1558-0016","","10.1109/TITS.2014.2308543","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776568","Active safety;deidentification;driver assistance systems;driver behavior;gaze estimation;head pose;human factors;Active safety;deidentification;driver assistance systems;driver behavior;gaze estimation;head pose;human factors","Vehicles;Face;Estimation;Face recognition;Cameras;Safety","behavioural sciences computing;face recognition;gaze tracking;image sequences;road safety;safety systems;traffic engineering computing;video signal processing","privacy-protecting safety systems;naturalistic driving videos;naturalistic driving data;driver behavior;driving safety improvement;visual attention;facial features;specific deidentification filter;video sequences;face recognition;driver gaze-zone estimation","","11","","40","IEEE","20 Mar 2014","","","IEEE","IEEE Journals"
"Towards Improving Social Communication Skills With Multimodal Sensory Information","J. Chen; D. Chen; X. Li; K. Zhang","National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, China; School of Computer Science, China University of Geosciences, Wuhan, China; National Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, China; National Engineering Research Center for E-Learning, Central China Normal University, Wuhan, China","IEEE Transactions on Industrial Informatics","12 Dec 2013","2014","10","1","323","330","How to improve social communication skills for children, especially those with social communication difficulties such as attention deficit/hyperactivity disorder, has long been a challenge faced by researchers and therapists. Recent research indicates that computer-assisted approaches may be effective in addressing this issue. This study aimed to understand children's behaviors and then provide appropriate support to improve their social communication skills. We have established an intelligent system, inside which a child can freely play interactive social skills games with virtual characters. The virtual characters can adjust their own behaviors by adapting to the child's cognitive state (e.g., focus of attention) and affective state (e.g., happiness or surprise). The child's behavior is identified in real-time by recognition of multimodal sensory information, which includes head pose and eye gaze estimation, gesture detection, and affective state detection supported by a series of algorithms proposed in this study. Furthermore, this intelligent system has been enabled in a nonintrusive manner using a novel approach of multicamera surveillance to provide the child with natural interaction with the system. Experimental results show the system can estimate a user's attention and affective states with correctness rates of 93% and 91.3%, respectively. The results obtained suggest that the methods have strong potential as alternative methods for sensing human behavior and providing appropriate support.","1941-0050","","10.1109/TII.2013.2271914","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553174","Behavior detection;intelligent systems;multimodal sensory information;social communication skills","","behavioural sciences computing;cameras;cognition;computer games;interactive systems","social communication skills;multimodal sensory information;attention deficit;hyperactivity disorder;computer-assisted approaches;children behaviors;intelligent system;interactive social skill games;virtual characters;child cognitive state;eye gaze estimation;gesture detection;head pose;affective state detection;multicamera surveillance;human behavior sensing","","23","","21","","3 Jul 2013","","","IEEE","IEEE Journals"
"A Low-Computational Approach on Gaze Estimation With Eye Touch System","C. Topal; S. Gunal; O. Koçdeviren; A. Doğan; Ö. N. Gerek","Anadolu University, Eskisehir, Turkey; Anadolu University, Eskisehir, Turkey; Anadolu University, Eskisehir, Turkey; Anadolu University, Eskisehir, Turkey; Anadolu University, Eskisehir, Turkey","IEEE Transactions on Cybernetics","14 Jan 2014","2014","44","2","228","239","Among various approaches to eye tracking systems, light-reflection based systems with non-imaging sensors, e.g., photodiodes or phototransistors, are known to have relatively low complexity; yet, they provide moderately accurate estimation of the point of gaze. In this paper, a low-computational approach on gaze estimation is proposed using the Eye Touch system, which is a light-reflection based eye tracking system, previously introduced by the authors. Based on the physical implementation of Eye Touch, the sensor measurements are now utilized in low-computational least-squares algorithms to estimate arbitrary gaze directions, unlike the existing light reflection-based systems, including the initial Eye Touch implementation, where only limited predefined regions were distinguished. The system also utilizes an effective pattern classification algorithm to be able to perform left, right, and double clicks based on respective eye winks with significantly high accuracy. In order to avoid accuracy problems for sensitive sensor biasing hardware, a robust custom microcontroller-based data acquisition system is developed. Consequently, the physical size and cost of the overall Eye Touch system are considerably reduced while the power efficiency is improved. The results of the experimental analysis over numerous subjects clearly indicate that the proposed eye tracking system can classify eye winks with 98% accuracy, and attain an accurate gaze direction with an average angular error of about 0.93 °. Due to its lightweight structure, competitive accuracy and low-computational requirements relative to video-based eye tracking systems, the proposed system is a promising human-computer interface for both stationary and mobile eye tracking applications.","2168-2275","","10.1109/TCYB.2013.2252792","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6517478","Assistive technology;eye tracking;gaze estimation;human-computer interface","Gaze tracking;Hardware;Sensor systems;Light emitting diodes;Cameras;Estimation","gaze tracking;human computer interaction;least squares approximations","low-computational approach;gaze estimation;eye touch system;nonimaging sensors;phototransistors;photodiodes;light-reflection based eye tracking system;sensor measurements;low-computational least-squares algorithms;arbitrary gaze direction estimation;effective pattern classification algorithm;eye winks;sensitive sensor biasing hardware;robust custom microcontroller-based data acquisition system;power efficiency;video-based eye tracking systems;mobile eye tracking;stationary eye tracking;human-computer interface","Algorithms;Equipment Design;Equipment Failure Analysis;Fixation, Ocular;Humans;Pattern Recognition, Automated;Photometry;Reproducibility of Results;Sensitivity and Specificity;Visual Fields","12","","49","IEEE","20 May 2013","","","IEEE","IEEE Journals"
"Gaze Estimation in Children's Peer-Play Scenarios","D. Duan; L. Tian; J. Cui; L. Wang; H. Zha; H. Aghajan","Key Lab. of Machine Perception, Peking Univ., Beijing, China; Key Lab. of Machine Perception, Peking Univ., Beijing, China; Key Lab. of Machine Perception, Peking Univ., Beijing, China; Dept. of Psychol., Peking Univ., Beijing, China; Key Lab. of Machine Perception, Peking Univ., Beijing, China; Wireless Sensor Networks Lab., Stanford Univ., Stanford, CA, USA","2013 2nd IAPR Asian Conference on Pattern Recognition","27 Mar 2014","2013","","","760","764","Gaze is a powerful cue for children's social behavior analysis. In this paper, a novel method is proposed to estimate children's gaze orientation in the experimental data of developmental psychology based on head pose estimation. In consideration of the possible errors of head pose estimation results, temporal information and potential targets are both introduced to improve the results of gaze estimation. At last, this method is evaluated by a dataset of children's peer-play scenarios and the results show that this method has a good performance. According to the experimental valuation and analysis, in a certain peer-play scenario, potential targets are powerful spatial cues for children's gaze estimation and temporal information also provides some cues to improve the estimation results.","0730-6512","978-1-4799-2190-4","10.1109/ACPR.2013.178","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778426","children's social behavior analysis;gaze estimation;head pose estimation","Head;Magnetic heads;Feature extraction;Visualization;Observers;Psychology","pose estimation;psychology","gaze estimation;children peer-play scenarios;social behavior analysis;children gaze orientation;developmental psychology;head pose estimation;temporal information","","","","17","","27 Mar 2014","","","IEEE","IEEE Conferences"
"Preliminary Analysis of Gait Changes That Correspond to Gaze Directions","T. Okada; H. Yamazoe; I. Mitsugami; Y. Yagi","Osaka Univ., Suita, Japan; Osaka Univ., Suita, Japan; Osaka Univ., Suita, Japan; Osaka Univ., Suita, Japan","2013 2nd IAPR Asian Conference on Pattern Recognition","27 Mar 2014","2013","","","788","792","Human gait (way of walking) can be changed to correspond to human gaze direction. We aim to realize a gait-based gaze estimation scheme by modeling the relations between gait and gaze. In this paper, as a first step to our goal, we show the preliminary analysis results of the relations between gait and gaze. From the results, we confirmed that arm swing is affected by the gaze direction. We confirmed a tendency for the opposite arm's amplitude in the gazing direction to decrease. For these analysis, we constructed an immersive walking environment in which we measured subject gaits in various gazing situations by a motion capturing system and an eye tracker. The environment consisted of a treadmill and 180 multi-screens for presenting the gazing target. Our analysis results suggest the possibility of gaze estimation based on gait.","0730-6512","978-1-4799-2190-4","10.1109/ACPR.2013.184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778432","gait;gaze;motion capture","Legged locomotion;Three-dimensional displays;Cameras;Estimation;Target tracking","gait analysis;gaze tracking;image motion analysis","gazing target;multiscreens;treadmill;eye tracker;motion capturing system;subject gait measurement;immersive walking environment;gait-based gaze estimation scheme;human gaze directions;human gait change analysis","","6","","11","","27 Mar 2014","","","IEEE","IEEE Conferences"
"Considerations of Self-Motion in Motion Saliency","A. Hiratani; R. Nakashima; K. Matsumiya; I. Kuriki; S. Shioiri","Grad. Sch. of Inf. Sci., Tohoku Univ. Sendai, Sendai, Japan; Res. Inst. of Electr. Commun., Tohoku Univ. Sendai, Sendai, Japan; Res. Inst. of Electr. Commun., Tohoku Univ. Sendai, Sendai, Japan; Res. Inst. of Electr. Commun., Tohoku Univ. Sendai, Sendai, Japan; Res. Inst. of Electr. Commun., Tohoku Univ. Sendai, Sendai, Japan","2013 2nd IAPR Asian Conference on Pattern Recognition","27 Mar 2014","2013","","","783","787","Despite a number of studies of computational models of visual saliency, it is still difficult to predict where human would attend in a scene. One of the problems is the effect of self-motion on retinal images. Usually saliency is calculated with local motion signals, and high degree of saliency is found in motion components caused by self-motion. Since human observers usually ignore the motion caused by self-motion, the prediction accuracy of attention locations falls off. We developed a framework that reduces the saliency of the motion components caused self-motion, using a physiological model of optic flow processing that extracts object motion among self-motion signals.","0730-6512","978-1-4799-2190-4","10.1109/ACPR.2013.183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778431","gaze estimation;saliency map;self-motion;global motion","Visualization;Motion pictures;Predictive models;Observers;Optical filters;Videos;Visual systems","feature extraction;image motion analysis;image sequences","self-motion;motion saliency;visual saliency;retinal image;attention location;physiological model;optic flow processing;object motion extraction","","2","","21","","27 Mar 2014","","","IEEE","IEEE Conferences"
"I See What You See: Point of Gaze Estimation from Corneal Images","C. Nitschke; A. Nakazawa; T. Nishida","Grad. Sch. of Inf., Kyoto Univ., Kyoto, Japan; Grad. Sch. of Inf., Kyoto Univ., Kyoto, Japan; Grad. Sch. of Inf., Kyoto Univ., Kyoto, Japan","2013 2nd IAPR Asian Conference on Pattern Recognition","27 Mar 2014","2013","","","298","304","Eye-gaze tracking (EGT) is an important problem with a long history and various applications. However, state-of-the-art geometric vision-based techniques still suffer from major limitations, especially (1) the requirement for calibration of a static relationship between eye camera and scene, and (2) a parallax error that occurs when the depth of the scene varies. This paper introduces a novel concept for EGT that overcomes these limitations using corneal imaging. Based on the observation that the cornea reflects the surrounding scene over a wide field of view, it is shown how to extract that information and determine the point of gaze (PoG) directly in an eye image. To realize this, a closed-form solution is developed to obtain the gaze-reflection point (GRP), where light from the PoG reflects at the corneal surface into a camera. This includes compensation for the individual offset between optical and visual axis. Quantitative and qualitative evaluation shows that the strategy achieves considerable accuracy and successfully supports depth-varying environments. The novel approach provides important practical advantages, including reduced intrusiveness and complexity, and support for flexible dynamic setups, non-planar scenes and outdoor application.","0730-6512","978-1-4799-2190-4","10.1109/ACPR.2013.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6778329","human computer interaction;eye gaze tracking;point of gaze;gaze reflection point;calibration;corneal imaging;spherical mirror;analytic forward projection","Calibration;Cameras;Estimation;Optical imaging;Visualization;Cornea;Three-dimensional displays","calibration;cameras;computer vision;gaze tracking","gaze estimation;corneal images;eye gaze tracking;EGT;geometric vision-based techniques;calibration;eye camera;parallax error;corneal imaging;point of gaze;eye image;closed-form solution;gaze-reflection point;GRP;corneal surface;depth-varying environments;reduced intrusiveness;flexible dynamic setups;nonplanar scenes;outdoor application","","5","","37","","27 Mar 2014","","","IEEE","IEEE Conferences"
"Eyelid and iris tracking method with novel eye models","K. Tamura; Y. Aoki","Keio Univ., Japan; Keio Univ., Japan","Proceedings of the 2013 IEEE/SICE International Symposium on System Integration","24 Mar 2014","2013","","","449","453","These days, useful gaze estimation system is getting expected as a new interface. However previous most gaze estimation systems require special equipment or complex calibration. In this report, we propose the method which tracks user's eyelid and iris automatically with novel eyelid model and eyeball model, aiming at useful gaze estimation as a future work. Eyelid shape tracking is also useful for more accurate iris tracking and user's state estimation.","","978-1-4799-2625-1","10.1109/SII.2013.6776739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6776739","","Eyelids;Iris;Estimation;Shape;Head;Computational modeling;Image edge detection","calibration;gaze tracking;iris recognition;shape recognition;state estimation","eyelid tracking method;iris tracking method;eye models;gaze estimation system;complex calibration;equipment calibration;automatic user eyelid tracking;automatic user iris tracking;eyeball model;eyelid shape tracking;user state estimation","","1","","10","","24 Mar 2014","","","IEEE","IEEE Conferences"
"Calibration-Free Gaze Estimation Using Human Gaze Patterns","F. Alnajar; T. Gevers; R. Valenti; S. Ghebreab","Univ. of Amsterdam, Amsterdam, Netherlands; Univ. of Amsterdam, Amsterdam, Netherlands; Univ. of Amsterdam, Amsterdam, Netherlands; Univ. of Amsterdam, Amsterdam, Netherlands","2013 IEEE International Conference on Computer Vision","3 Mar 2014","2013","","","137","144","We present a novel method to auto-calibrate gaze estimators based on gaze patterns obtained from other viewers. Our method is based on the observation that the gaze patterns of humans are indicative of where a new viewer will look at. When a new viewer is looking at a stimulus, we first estimate a topology of gaze points (initial gaze points). Next, these points are transformed so that they match the gaze patterns of other humans to find the correct gaze points. In a flexible uncalibrated setup with a web camera and no chin rest, the proposed method was tested on ten subjects and ten images. The method estimates the gaze points after looking at a stimulus for a few seconds with an average accuracy of 4:3°. Although the reported performance is lower than what could be achieved with dedicated hardware or calibrated setup, the proposed method still provides a sufficient accuracy to trace the viewer attention. This is promising considering the fact that auto-calibration is done in a flexible setup, without the use of a chin rest, and based only on a few seconds of gaze initialization data. To the best of our knowledge, this is the first work to use human gaze patterns in order to auto-calibrate gaze estimators.","2380-7504","978-1-4799-2840-8","10.1109/ICCV.2013.24","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6751126","","Estimation;Pattern matching;Cameras;Calibration;Manifolds;Accuracy;Visualization","gaze tracking;image processing","calibration-free gaze estimation;human gaze patterns;flexible uncalibrated setup;Web camera","","28","2","18","","3 Mar 2014","","","IEEE","IEEE Conferences"
"Person independent 3D gaze estimation from remote RGB-D cameras","K. A. Funes Mora; J. Odobez","Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland","2013 IEEE International Conference on Image Processing","13 Feb 2014","2013","","","2787","2791","We address the problem of person independent 3D gaze estimation using a remote, low resolution, RGB-D camera. The approach relies on a sparse technique to reconstruct normalized eye test images from a gaze appearance model (a set of eye image/gaze pairs) and infer their gaze accordingly. In this context, the paper makes three contributions: (i) unlike most previous approaches, we exploit the coupling (and constraints) between both eyes to infer their gaze jointly; (ii) we show that a generic gaze appearance model built from the aggregation of person-specific models can be used to handle unseen users and compensate for appearance variations across people, since a test user eyes' appearance will be reconstructed from similar users within the generic model. (iii) we propose an automatic model selection method that leads to comparable performance with a reduced computational load.","2381-8549","978-1-4799-2341-0","10.1109/ICIP.2013.6738574","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738574","3D gaze estimation;appearance based methods;sparse reconstruction;person-independence","","cameras;eye;image colour analysis;image reconstruction;image resolution","person independent 3D gaze estimation;remote RGB-D cameras;remote-low-resolution-RGB-D camera;sparse technique;normalized eye test image reconstruction;eye image-gaze pairs;gaze inference;generic gaze appearance model;person-specific model aggregation;unseen user handling;appearance variation compensation;test user eye appearance reconstruction;generic model;automatic model selection method;computational load reduction","","13","","11","","13 Feb 2014","","","IEEE","IEEE Conferences"
"Explore New Eye Tracking and Gaze Locating Methods","A. Kadyrov; H. Yu; J. Eyles; H. Liu","Sch. of Creative Technol., Univ. of Portsmouth, Portsmouth, UK; Sch. of Creative Technol., Univ. of Portsmouth, Portsmouth, UK; Sch. of Creative Technol., Univ. of Portsmouth, Portsmouth, UK; Sch. of Creative Technol., Univ. of Portsmouth, Portsmouth, UK","2013 IEEE International Conference on Systems, Man, and Cybernetics","27 Jan 2014","2013","","","2866","2871","Eye tracking has been used extensively in research, often for plotting the gaze location of research participants. Many of the existing methods on tracking gaze locations, however, require special equipment. This equipment can be very expensive and/or cumbersome to use, restricting the use of it to specialist labs. By producing a method of eye tracking which requires minimal equipment and set up time, eye tracking might be more widely used as a research tool, especially in exploratory areas where the outcomes of the research are not known. This paper introduces two novel eye tracking methods which use only a web cam feed as input and require no specialist equipment. The camera used in this research is a standard web cam built into a normal laptop, any current commercial web cam could be used. Experiments using the proposed method showed more accurate tracking results than some existing eye trackers.","1062-922X","978-1-4799-0652-9","10.1109/SMC.2013.489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6722242","Eye tracking;gaze estimation;player experience;circle detection","Head;Webcams;Vectors;Iris;Correlation;Estimation","cameras;gaze tracking;image matching","template matching;ray image method;gradient intersections;laptop;camera;web cam feed;gaze location tracking;eye tracking method","","1","","26","","27 Jan 2014","","","IEEE","IEEE Conferences"
"The head mouse — Head gaze estimation ""In-the-Wild"" with low-cost inertial sensors for BMI use","N. Sim; C. Gavriel; W. W. Abbott; A. A. Faisal","Brain & Behaviour Lab - Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, UK; Brain & Behaviour Lab - Department of Computing, Imperial College London, South Kensington Campus, London SW7 2AZ, UK; Brain & Behaviour Lab - Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, UK; Brain & Behaviour Lab - Department of Bioengineering, Imperial College London, South Kensington Campus, London SW7 2AZ, UK","2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)","2 Jan 2014","2013","","","735","738","We present a wearable head-tracking device using inexpensive inertial sensors as an alternative head movement tracking system. This can be used as indicator of human movement intentions for Brain-Machine Interface (BMI) applications. Our system is capable of tracking head movements at high rates (100 Hz) and achieves R<sup>2</sup> = 0.99 with a 2.5° RMSE against a ground-truth motion tracking system. The system tracks head movements over periods in the order of tens of minutes with little drift. The accuracy and precision of our system, together with its low response latency of ≈ 20 ms make it an unconventional but effective system for human-computer interfacing: the ""head mouse"" controls the mouse cursor on a display based on head orientation alone, so that it matches the centre of a straight-onward looking user. Our head mouse is suitable for amputees and spinal chord injury patients who have lost control of their upper extremities. We show that naive test subjects are capable to write text using our system and an on-screen keyboard at a rate of 4.65 words/minute, compared to able bodied users using a physical computer mouse which reached 7.85 words/minute. Crucially we measure the natural head movements of able bodied computer users, and show that our approach falls within the range of natural head movement parameters.","1948-3554","978-1-4673-1969-0","10.1109/NER.2013.6696039","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6696039","","Magnetic heads;Head;Tracking;Mice;Computers;Magnetic sensors","brain-computer interfaces;gaze tracking;sensors","head mouse-head gaze estimation;low-cost inertial sensors;BMI applications;brain-machine interface;head movement tracking;RMSE;root mean square error;ground-truth motion tracking system;human-computer interface;amputees;spinal chord injury patients","","14","","17","","2 Jan 2014","","","IEEE","IEEE Conferences"
"Appearance-based gaze estimation using kinect","J. Choi; B. Ahn; J. Parl; I. S. Kweon","Korea Advanced Institute of Science and Technology, Daejeon, 305-701, Korea; Korea Advanced Institute of Science and Technology, Daejeon, 305-701, Korea; Korea Advanced Institute of Science and Technology, Daejeon, 305-701, Korea; Korea Advanced Institute of Science and Technology, Daejeon, 305-701, Korea","2013 10th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)","2 Dec 2013","2013","","","260","261","Human gaze tracking has gathered much attention due to its capability to detect intuitive attention. Appearance-based methods can work with a single camera in ordinary conditions to track human gaze. An effective way to generate eye appearances is proposed using the Kinect. The head pose information is obtained from the Kinect after a series of calibrations. The Eye Appearance features are collected through ASM and KLT feature tracker. With 23 training samples, the error is found to be 1.07°. This paper proposes an efficient scheme for gaze tracking using a single Kinect device.","","978-1-4799-1197-4","10.1109/URAI.2013.6677362","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6677362","List keywords here. No more than 5","Vectors;Training;Head;Cameras;Computer vision;Computational modeling;Estimation","feature extraction;object tracking;sensors;shape recognition","appearance-based gaze estimation;human gaze tracking;head pose information;eye appearance features;ASM feature tracker;KLT feature tracker;Kinect device;active shape model","","5","","6","","2 Dec 2013","","","IEEE","IEEE Conferences"
"Design and implementation of gaze tracking headgear for Nvidia 3D Vision<sup>®</sup>","S. Wibirama; K. Hamamoto","Graduate, School of Science and Technology, Tokai University, Tokyo, Japan 108-8619; Graduate, School of Science and Technology, Tokai University, Tokyo, Japan 108-8619","2013 International Conference on Information Technology and Electrical Engineering (ICITEE)","2 Dec 2013","2013","","","84","87","The usage of Nvidia 3D Vision<sup>®</sup> is increasing rapidly, ranging from gaming to research purposes. However, researchers in human computer interaction and virtual reality are constrained by hardware configuration since current commercial gaze tracking systems are not specifically designed to be used with Nvidia 3D Vision<sup>®</sup>. In this paper, we present a novel prototype of gaze tracking headgear which can be used appropriately with Nvidia 3D Vision<sup>®</sup>. We explain design consideration and detail implementation of our gaze tracking headgear. We also evaluate our gaze tracking system by measuring gaze accuracy on stereoscopic display. Experimental result shows that the average gaze estimation error is less than one degree visual angle.","","978-1-4799-0425-9","10.1109/ICITEED.2013.6676216","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6676216","gaze tracking;active 3D technology;virtual reality;Nvidia 3D Vision®","Three-dimensional displays;Eye protection;Welding;Image quality;Atmospheric measurements;Particle measurements","computer vision;human computer interaction;iris recognition;virtual reality","Nvidia 3D Vision;human computer interaction;virtual reality;gaze tracking systems;gaze tracking headgear design;gaze accuracy measurement;stereoscopic display;average gaze estimation error;gaze tracking implementation","","6","","19","","2 Dec 2013","","","IEEE","IEEE Conferences"
"A novel remote eye gaze tracking approach with dynamic calibration","K. Han; X. Wang; Z. Zhang; H. Zhao","Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, 518055, China; Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, 518055, China; Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, 518055, China; Computer Application Research Center, Harbin Institute of Technology Shenzhen Graduate School, 518055, China","2013 IEEE 15th International Workshop on Multimedia Signal Processing (MMSP)","11 Nov 2013","2013","","","111","116","Point of gaze estimation is the most important part of remote eye gaze tracking techniques. Though there are various point of gaze detection and estimation methods, few of them can satisfy the expectation of widely use. One primary reason is lack of accurate calibration method. In order to deal with this problem, an adaptive calibration technique is proposed based on cross-ratio. It can compensate estimation bias much better. Also, efficient remote eye gaze tracking parameters estimation procedures are employed in this paper. The eye gaze tracking approach in this paper allows users to have free head motion and has high accuracy rate, and it is based on infrared radiation (IR) light sources reflection on cornea. Experimental results demonstrate that considerable improvement can be achieved.","","978-1-4799-0125-8","10.1109/MMSP.2013.6659273","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6659273","","Calibration;Estimation;Feature extraction;Cameras;Accuracy;Light sources;Cornea","calibration;feature extraction;infrared sources;motion estimation","remote eye gaze tracking approach;dynamic calibration method;gaze estimation;gaze detection;estimation method;head motion;infrared radiation light source reflection;IR light source reflection;cross-ratio mapping model;fast feature extraction method;cognitive processes","","4","","13","","11 Nov 2013","","","IEEE","IEEE Conferences"
"Context-based perception and understanding of human intentions","J. Quintas; P. Menezes; J. Dias","Instituto Pedro Nunes, Coimbra, Portugal; University of Coimbra, Portugal; University of Coimbra, Portugal","2013 IEEE RO-MAN","15 Oct 2013","2013","","","346","347","This work focus in the importance of context awareness and intention understanding capabilities in modern robots when faced with different situations. The objective is to be capable of providing new features for robots, which enable new real-world applications, and extend their autonomy, in terms of self-management and cooperation with humans or other systems. Gaze estimation and gesture interpretation are modalities, closely related with context-dependent human intention understanding, that are addressed in this work.","1944-9437","978-1-4799-0509-6","10.1109/ROMAN.2013.6628489","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6628489","","Context;Robots;Educational institutions;Abstracts;Context-aware services;Computer architecture;Human-robot interaction","behavioural sciences;gesture recognition;human-robot interaction;ubiquitous computing","context-based perception;context awareness;human intention understanding capabilities;self-management;human-robot cooperation;gaze estimation;gesture interpretation;context-dependent human intention understanding","","3","","10","","15 Oct 2013","","","IEEE","IEEE Conferences"
"Mouse and Keyboard Cursor Warping to Accelerate and Reduce the Effort of Routine HCI Input Tasks","D. Rozado","Autonomous Systems Lab, CSIRO Computational Informatics, Pullenvale, Australia","IEEE Transactions on Human-Machine Systems","18 Oct 2013","2013","43","5","487","493","Gaze tracking has been suggested as an alternative to traditional computer pointing mechanisms. However, the accuracy limitations of gaze estimation algorithms and the fatigue imposed on users when overloading the visual perceptual channel with a motor control task have prevented the widespread adoption of gaze as a pointing modality. Rather than using gaze as a complete pointing mechanism, this study investigates the usage of gaze to complement traditional keyboard/mouse cursor positioning methods during standard human-computer interaction (HCI). With this approach, bringing the mouse/keyboard cursor to a target still requires a manual action, but the time and effort involved are substantially reduced in terms of mouse movement amplitude incurred or number of keystrokes pressed. This is accomplished by the cursor warping from its original position on the screen to the estimated point of regard of the user on the screen as estimated by video-oculography gaze tracking when a keystroke or mouse movement event is detected. The user adjusts the final fine-grained positioning of the cursor manually. The results of the user study carried out here on the effects of cursor warping in common computer input operations that involve cursor repositioning when using one or several monitors as well as on its learning dynamics over time show that cursor warping can speed up and/or reduce the physical effort required to complete tasks such as mouse/trackpad target acquisition, keyboard text cursor positioning, mouse/keyboard based text selection, and drag and drop operations.","2168-2305","","10.1109/THMS.2013.2281852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6626613","Attentive user interface;cursor warping;eye tracking;gaze aware interfaces;gaze tracking;human computer interaction (HCI);human factors;repetitive stress injury (RSI)","Keyboards;Computers;Tracking;Human computer interaction;Target tracking","human computer interaction;keyboards;mouse controllers (computers)","mouse-keyboard cursor warping;routine HCI input tasks;video-oculography gaze tracking;computer pointing mechanisms;gaze estimation algorithms;fatigue;visual perceptual channel;motor control task;keyboard-mouse cursor positioning methods;human-computer interaction;computer input operations;learning dynamics;mouse-trackpad target acquisition;keyboard text cursor positioning;mouse-keyboard based text selection;drag-drop operations","","10","","11","","9 Oct 2013","","","IEEE","IEEE Journals"
"Calibration-free gaze tracking using particle filter","P. Nguyen; J. Fleureau; C. Chamaret; P. Guillotel","Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France; Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France; Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France; Technicolor, 975 Ave. des Champs Blancs, 35576 Cesson-Sevigne, France","2013 IEEE International Conference on Multimedia and Expo (ICME)","26 Sep 2013","2013","","","1","6","This paper presents a novel approach for gaze estimation using only a low-cost camera and requiring no calibration. The main idea is based on the center-bias property of human gaze distribution to get a coarse estimate of the current gaze position as well as benefit from temporal information to enhance this rough gaze estimate. Firstly, we propose a method for detecting the eye center location and a mapping model based on the center-bias effect to convert it to gaze position. This initial gaze estimate then serves to construct the likelihood model of the eye-appearance. The final gaze position is estimated by fusing the likelihood model with the prior information obtained from previous observations on the basis of the particle filtering framework. Extensive experiments demonstrate the good performance of the proposed system with an average estimation error of 3.43° which outperforms state-of-the-art methods. Furthermore, the low complexity of the proposed system makes it suitable for real-time applications.","1945-788X","978-1-4799-0015-2","10.1109/ICME.2013.6607532","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6607532","HCI;gaze estimation;eye center detection;calibration-free;particle filter","Accuracy;Estimation;Computational modeling;Calibration;Motion pictures;Cameras;Mathematical model","eye;object detection;particle filtering (numerical methods)","calibration-free gaze tracking;center-bias property;human gaze distribution;eye center location;particle filtering framework;real-time applications;likelihood model;gaze estimation","","4","3","12","","26 Sep 2013","","","IEEE","IEEE Conferences"
"A gaze tracking scheme with low resolution image","Y. Fu; W. Zhu; D. Massicotte","Electrical and Computer Engineering Department Concordia University Montreal, Canada; Electrical and Computer Engineering Department Concordia University Montreal, Canada; Electrical and Computer Engineering Department Université du Québec à Trois-Rivières Trois-Rivières, Canada","2013 IEEE 11th International New Circuits and Systems Conference (NEWCAS)","5 Aug 2013","2013","","","1","4","To estimate the eye gaze accurately, a high-resolution and good quality image of the eye region is essential. Therefore, one or multiple narrow angle cameras with long lens are commonly used. In case of poor image resolution or absence of narrow angle focusing, the gaze tracking would be inaccurate. This paper looks into the possibility of using low resolution wide angle camera to track gaze. In order to combat the limitation of poor eye-region image which might only be defined by a few pixels, a precise corneal reflection vector estimation based on local image reconstruction method is proposed. By using 2D bilinear interpolation, the local eye-region image is enhanced for precise reflection vector estimation. Although the accuracy of the proposed method can barely be compared with that of the state of art techniques, it is yet noteworthy that the performance gain has been achieved with low resolution images only, indicating the possibility of tracking gaze with low-cost simple cameras.","","978-1-4799-0620-8","10.1109/NEWCAS.2013.6573660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6573660","","Cameras;Image resolution;Estimation;Image edge detection;Tracking;Transforms;Accuracy","cameras;eye;image enhancement;image reconstruction;image resolution;interpolation","gaze tracking scheme;low resolution image;eye gaze estimation;high-resolution image;eye region image quality;narrow angle cameras;narrow angle focusing;corneal reflection vector estimation;local image reconstruction method;2D bilinear interpolation;local eye-region image enhancement;state of art techniques","","3","1","18","","5 Aug 2013","","","IEEE","IEEE Conferences"
"Art Critic: Multisignal Vision and Speech Interaction System in a Gaming Context","M. J. Reale; P. Liu; L. Yin; S. Canavan","Department of Computer Science, State University of New York at Binghamton, Binghamton, NY, USA; Department of Computer Science, State University of New York at Binghamton, Binghamton, NY, USA; Department of Computer Science, State University of New York at Binghamton, Binghamton, NY, USA; Department of Computer Science, State University of New York at Binghamton, Binghamton, NY, USA","IEEE Transactions on Cybernetics","19 Nov 2013","2013","43","6","1546","1559","True immersion of a player within a game can only occur when the world simulated looks and behaves as close to reality as possible. This implies that the game must correctly read and understand, among other things, the player's focus, attitude toward the objects/persons in focus, gestures, and speech. In this paper, we proposed a novel system that integrates eye gaze estimation, head pose estimation, facial expression recognition, speech recognition, and text-to-speech components for use in real-time games. Both the eye gaze and head pose components utilize underlying 3-D models, and our novel head pose estimation algorithm uniquely combines scene flow with a generic head model. The facial expression recognition module uses the local binary patterns with three orthogonal planes approach on the 2-D shape index domain rather than the pixel domain, resulting in improved classification. Our system has also been extended to use a pan-tilt-zoom camera driven by the Kinect, allowing us to track a moving player. A test game, Art Critic, is also presented, which not only demonstrates the utility of our system but also provides a template for player/non-player character (NPC) interaction in a gaming context. The player alters his/her view of the 3-D world using head pose, looks at paintings/NPCs using eye gaze, and makes an evaluation based on the player's expression and speech. The NPC artist will respond with facial expression and synthetic speech based on its personality. Both qualitative and quantitative evaluations of the system are performed to illustrate the system's effectiveness.","2168-2275","","10.1109/TCYB.2013.2271606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6572826","Expression recognition;gaming interaction;gaze tracking;head pose estimation;speech recognition;text-to-speech","Estimation;Solid modeling;Cameras;Games;Face;Speech","cameras;computer games;face recognition;feature extraction;pose estimation;solid modelling;speech recognition","Art Critic game;multisignal vision interaction system;speech interaction system;gaming context;player immersion;player focus;player attitude;eye gaze estimation;head pose estimation;facial expression recognition;speech recognition;text-to-speech components;3D models;generic head model;scene flow;local binary patterns;orthogonal planes approach;2D shape index domain;pixel domain;pan-tilt-zoom camera;Kinect;player-nonplayer character interaction;3D world;player expression;player speech;qualitative evaluation;quantitative evaluation","Algorithms;Eye Movements;Facial Expression;Head Movements;Humans;Imaging, Three-Dimensional;Male;Sound Spectrography;Speech Recognition Software;User-Computer Interface;Video Games;Young Adult","4","","47","","31 Jul 2013","","","IEEE","IEEE Journals"
"Combining first-person and third-person gaze for attention recognition","F. Martinez; A. Carbone; E. Pissaloux","ISIR, Université Pierre et Marie Curie - CNRS UMR 7222, France; ISIR, Université Pierre et Marie Curie - CNRS UMR 7222, France; ISIR, Université Pierre et Marie Curie - CNRS UMR 7222, France","2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","15 Jul 2013","2013","","","1","6","This paper presents a method to recognize attentional behaviors from a head-mounted binocular eye tracker in triadic interactions. By taking advantage of the first-person view, we simultaneously estimate the first-person and third-person gaze. The first-person gaze is computed using an appearance-based method relying on local features. In parallel, head pose tracking allows determining the coarse gaze of people in the scene camera. Finally, knowing the first- and third-person gaze direction, scores are computed which permit to assign attention patterns to each frame. Our contributions are the followings: (i) head pose estimation based on localized regression, (ii) attention analysis, in particular mutual and shared gaze, including the first-person gaze, (iii) experiments conducted using a head-mounted appearance-based gaze tracker. Experiments on recorded data show encouraging results.","","978-1-4673-5546-9","10.1109/FG.2013.6553735","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6553735","","Estimation;Feature extraction;Face;Kernel;Pattern recognition;Calibration","behavioural sciences;cameras;feature extraction;helmet mounted displays;object tracking;pose estimation;regression analysis","first-person gaze;third-person gaze;attentional behavior recognition;head-mounted binocular eye tracker;triadic interaction;first-person view;appearance-based method;local feature;head pose tracking;coarse gaze;scene camera;attention pattern;head pose estimation;localized regression;attention analysis;mutual gaze;shared gaze;head-mounted appearance-based gaze tracker","","9","1","24","","15 Jul 2013","","","IEEE","IEEE Conferences"
"A Calibration Method for Eye-Gaze Estimation Systems Based on 3D Geometrical Optics","H. Lee; N. Iqbal; W. Chang; S. Lee","Korea Advanced Institute of Science and Technology, Daejeon, Korea; Korea Advanced Institute of Science and Technology, Daejeon, Korea; Korea Advanced Institute of Science and Technology, Daejeon, Korea; Korea Advanced Institute of Science and Technology, Daejeon, Korea","IEEE Sensors Journal","31 Jul 2013","2013","13","9","3219","3225","A new calibration method is presented for robust eye-gaze estimation systems which are utilized to understand the human mind. Although current eye-gaze systems with one camera and a few infrared light sources have been developed to allow users' head motion, they still require one to know the relative positions between the camera and light sources with very high accuracy. The developed calibration method utilizes a three-dimensional geometrical relationship between the light source positions and corresponding camera images reflected on a mirror at several positions and rotations. The best estimates of the light source positions are obtained from noisy measurements by minimizing a cost function, which ensures the integrity of the camera and light sources. The developed calibration method makes it possible to convert many camera-and-display devices into robust eye-gaze estimation systems.","1558-1748","","10.1109/JSEN.2013.2268247","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6530610","Eye-gaze estimation system;sensor system calibration;model-based eye-gaze estimation","","biological techniques;biomedical measurement;calibration;cameras;eye;geometrical optics;mirrors","eye gaze estimation system calibration;3D geometrical optics;infrared light sources;user head motion;camera-light source relative position;3D geometrical relationship;reflected camera images;noisy measurements;cost function minimisation","","8","","19","","12 Jun 2013","","","IEEE","IEEE Journals"
"Gaze Estimation Using Electrooculogram Signals and Its Mathematical Modeling","M. Yan; H. Tamura; K. Tanno","Interdiscipl. Grad. Sch. of Agric. & Eng., Univ. of Miyazaki, Miyazaki, Japan; Dept. of Electr. & Electron. Eng., Univ. of Miyazaki, Miyazaki, Japan; Dept. of Electr. & Electron. Eng., Univ. of Miyazaki, Miyazaki, Japan","2013 IEEE 43rd International Symposium on Multiple-Valued Logic","10 Jun 2013","2013","","","18","22","The aim of this study is to present electrooculogram signals that can be used for human computer interface efficiently. Establishing an efficient alternative channel for communication without overt speech and hand movements is important to increase the quality of life for patients suffering from Amyotrophic Lateral Sclerosis or other illnesses that prevent correct limb and facial muscular responses. In this paper, we introduce the gaze estimation system of electrooculogram signals. Using this system, the electrooculogram signals can be recorded when the patients focused on each direct. All these recorded signals could be analyzed using math-method and the mathematical model will be set up. Gaze estimation can be recognized using electrooculogram signals follow these models.","0195-623X","978-0-7695-4976-7","10.1109/ISMVL.2013.31","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6524633","Electrooculogram Signal;Gaze estimation;Mathematical model;Drift","Electrooculography;Estimation;Mice;Mathematical model;Regression analysis;Control systems","diseases;electro-oculography;human computer interaction;medical computing;patient care","gaze estimation;mathematical modeling;electrooculogram signals;human computer interface;quality of life;patients;amyotrophic lateral sclerosis;facial muscular responses;EOG;math-method","","","","6","","10 Jun 2013","","","IEEE","IEEE Conferences"
"Long-Range Gaze Tracking System for Large Movements","D. Cho; W. Kim","Department of Electronics and Computer Engineering, Hanyang University, Seoul, Korea; Department of Electronics and Computer Engineering, Hanyang University, Seoul, Korea","IEEE Transactions on Biomedical Engineering","19 Nov 2013","2013","60","12","3432","3440","In the vision-based remote gaze tracking systems, the most challenging topics are to allow natural movement of a user and to increase the working volume and distance of the system. Several eye gaze estimation methods considering the natural movement of a user have been proposed. However, their working volume and distance are narrow and close. In this paper, we propose a novel 2-D mapping-based gaze estimation method that allows large-movement of user. Conventional 2-D mapping-based methods utilize mapping function between calibration points on the screen and pupil center corneal reflection (PCCR) vectors obtained in user calibration step. However, PCCR vectors and their associated mapping function are only valid at or near to the position where the user calibration is performed. The proposed movement mapping function, complementing the user's movement, estimates scale factors between two PCCR vector sets: one obtained at the user calibration position and another obtained at the new user position. The proposed system targets a longer range gaze tracking which operates from 1.4 to 3 m. A narrow-view camera mounted on a pan and tilt unit is used by the proposed system to capture high-resolution eye image, providing a wide and long working volume of about 100 cm × 40 cm × 100 cm. The experimental results show that the proposed method successfully compensated the poor performance due to user's large movement. Average angular error was 0.8° and only 0.07° of angular error was increased while the user moved around 81 cm.","1558-2531","","10.1109/TBME.2013.2266413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6525350","Gaze tracking;infrared;long range;natural movement","Cameras;Calibration;Vectors;Tracking;Estimation;Erbium;Head","calibration;cameras;eye;vision","user natural movement;high-resolution eye image;narrow-view camera;calibration;PCCR vectors;pupil center corneal reflection vectors;2-D mapping-based gaze estimation method;eye gaze estimation methods;vision-based remote gaze tracking systems;long-range gaze tracking system","Adult;Eye Movements;Female;Fixation, Ocular;Humans;Image Processing, Computer-Assisted;Imaging, Three-Dimensional;Male;Models, Statistical;Pupil;Young Adult","14","","22","","6 Jun 2013","","","IEEE","IEEE Journals"
"Smart user interface for mobile consumer devices using model-based eye-gaze estimation","N. Iqbal; H. Lee; S. Lee","Department of Bio and Brian Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, 305-701 Republic of Korea; Brain Science Research Center, KAIST, Daejeon, 305-701, Republic of Korea; Member; Department of Electrical Engineering and Brain Science Research Center, KAIST, Daejeon, 305-701 Republic of Korea","IEEE Transactions on Consumer Electronics","4 Apr 2013","2013","59","1","161","166","A smart user-interface for mobile consumer devices was developed using a robust eye-gaze system without any hand motion. Using one camera and one display already available in popular mobile devices, the eye-gaze system estimates the visual angle, which shows the area of interest on the display to indicate the position of the cursor. Three novel techniques were developed to make the system robust, userindependent, and head/device motion invariant. First, by carefully investigating the geometric relation between the device and the user's cornea, a new algorithm was developed to estimate the cornea center position, which is directly related to the optical axis of the eye. Unlike previous algorithms, it does not utilize the user-dependent cornea radius. Second, to make the system robust for practical application, an algorithm was developed to compensate for imaging position errors due to the finite camera resolution. Third, a binocular algorithm was developed to estimate the user-dependent angular offsets between the optical and visual axes with only single point calibration. The proposed system was demonstrated to be accurate enough for many practical mobile user interfaces.","1558-4127","","10.1109/TCE.2013.6490255","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6490255","Human-computer interface;multimodaluser interface;model-based eye-gaze estimation;binoculargaze relation","Cornea;Cameras;Light sources;Estimation;Visualization;Adaptive optics;Mobile handsets","mobile computing;user interfaces","smart user interface;mobile consumer device;model-based eye-gaze estimation;hand motion;eye-gaze system;visual angle estimation;cursor position;head-device motion;geometric relation;cornea center position estimation;binocular algorithm;user-dependent angular offset;point calibration","","15","2","21","","4 Apr 2013","","","IEEE","IEEE Journals"
"Estimating the Gaze of a Virtuality Human","D. J. Roberts; J. Rae; T. W. Duckworth; C. M. Moore; R. Aspin",University of Salford; University of Roehampton; University of Salford; University of Salford; University of Salford,"IEEE Transactions on Visualization and Computer Graphics","13 Mar 2013","2013","19","4","681","690","The aim of our experiment is to determine if eye-gaze can be estimated from a virtuality human: to within the accuracies that underpin social interaction; and reliably across gaze poses and camera arrangements likely in every day settings. The scene is set by explaining why Immersive Virtuality Telepresence has the potential to meet the grand challenge of faithfully communicating both the appearance and the focus of attention of a remote human participant within a shared 3D computer-supported context. Within the experiment n=22 participants rotated static 3D virtuality humans, reconstructed from surround images, until they felt most looked at. The dependent variable was absolute angular error, which was compared to that underpinning social gaze behaviour in the natural world. Independent variables were 1) relative orientations of eye, head and body of captured subject; and 2) subset of cameras used to texture the form. Analysis looked for statistical and practical significance and qualitative corroborating evidence. The analysed results tell us much about the importance and detail of the relationship between gaze pose, method of video based reconstruction, and camera arrangement. They tell us that virtuality can reproduce gaze to an accuracy useful in social interaction, but with the adopted method of Video Based Reconstruction, this is highly dependent on combination of gaze pose and camera arrangement. This suggests changes in the VBR approach in order to allow more flexible camera arrangements. The work is of interest to those wanting to support expressive meetings that are both socially and spatially situated, and particular those using or building Immersive Virtuality Telepresence to accomplish this. It is also of relevance to the use of virtuality humans in applications ranging from the study of human interactions to gaming and the crossing of the stage line in films and TV.","1941-0506","","10.1109/TVCG.2013.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6479209","Cinematography;virtual worlds;virtual environments;camera placement;hierarchical finite state machines.","Cameras;Estimation;Accuracy;Image reconstruction;Face;Visualization","image reconstruction;iris recognition","virtuality humans;flexible camera arrangement;video based reconstruction;gaze pose;qualitative corroborating evidence;social gaze behaviour;angular error;image reconstruction;3D virtuality human;3D computer supported context;immersive virtuality telepresence;social interaction;eye gaze;gaze estimation","Algorithms;Attention;Computer Graphics;Computer Simulation;Fixation, Ocular;Humans;Image Interpretation, Computer-Assisted;Imaging, Three-Dimensional;Models, Biological;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface","14","","25","","13 Mar 2013","","","IEEE","IEEE Journals"
"Joint Attention by Gaze Interpolation and Saliency","Z. Yücel; A. A. Salah; Ç. Meriçli; T. Meriçli; R. Valenti; T. Gevers","Intelligent Robotics and Communication Laboratories, Advanced Telecommunications Research Institute International, Kyoto, Japan; Department of Computer Engineering, Boðaziçi University, Yacute;stanbul, Turkey; Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computer Engineering, Boðaziçi University, Yacute;stanbul, Turkey; Intelligent Systems Lab Amsterdam, University of Amsterdam, Amsterdam, The Netherlands; Intelligent Systems Lab Amsterdam, University of Amsterdam, Amsterdam, The Netherlands","IEEE Transactions on Cybernetics","13 May 2013","2013","43","3","829","842","Joint attention, which is the ability of coordination of a common point of reference with the communicating party, emerges as a key factor in various interaction scenarios. This paper presents an image-based method for establishing joint attention between an experimenter and a robot. The precise analysis of the experimenter's eye region requires stability and high-resolution image acquisition, which is not always available. We investigate regression-based interpolation of the gaze direction from the head pose of the experimenter, which is easier to track. Gaussian process regression and neural networks are contrasted to interpolate the gaze direction. Then, we combine gaze interpolation with image-based saliency to improve the target point estimates and test three different saliency schemes. We demonstrate the proposed method on a human-robot interaction scenario. Cross-subject evaluations, as well as experiments under adverse conditions (such as dimmed or artificial illumination or motion blur), show that our method generalizes well and achieves rapid gaze estimation for establishing joint attention.","2168-2275","","10.1109/TSMCB.2012.2216979","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6320663","Developmental robotics;gaze following;head pose estimation;joint visual attention;saliency;selective attention","Joints;Estimation;Vectors;Robot kinematics;Face","Gaussian processes;human-robot interaction;image resolution;interpolation;neural nets;regression analysis;robot vision","joint attention;gaze interpolation;communicating party;image-based method;robot;experimenter eye region;high-resolution image acquisition;regression-based interpolation;gaze direction;head pose;Gaussian process regression;neural networks;gaze direction interpolation;image-based saliency;human-robot interaction scenario;cross-subject evaluations;joint attention","Algorithms;Artificial Intelligence;Attention;Biomimetics;Communication;Fixation, Ocular;Humans;Man-Machine Systems;Pattern Recognition, Automated;Robotics","34","","63","","7 Mar 2013","","","IEEE","IEEE Journals"
"Appearance-Based Gaze Estimation Using Visual Saliency","Y. Sugano; Y. Matsushita; Y. Sato","The University of Tokyo, Tokyo; Microsoft Research Asia, Beijing; The University of Tokyo, Tokyo","IEEE Transactions on Pattern Analysis and Machine Intelligence","20 Dec 2012","2013","35","2","329","341","We propose a gaze sensing method using visual saliency maps that does not need explicit personal calibration. Our goal is to create a gaze estimator using only the eye images captured from a person watching a video clip. Our method treats the saliency maps of the video frames as the probability distributions of the gaze points. We aggregate the saliency maps based on the similarity in eye images to efficiently identify the gaze points from the saliency maps. We establish a mapping between the eye images to the gaze points by using Gaussian process regression. In addition, we use a feedback loop from the gaze estimator to refine the gaze probability maps to improve the accuracy of the gaze estimation. The experimental results show that the proposed method works well with different people and video clips and achieves a 3.5-degree accuracy, which is sufficient for estimating a user's attention on a display.","1939-3539","","10.1109/TPAMI.2012.101","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6193107","Gaze estimation;visual attention;face and gesture recognition","Visualization;Estimation;Calibration;Feature extraction;Accuracy;Face;Humans","computer vision;eye;face recognition;feedback;Gaussian processes;gesture recognition;object recognition;regression analysis;statistical distributions","appearance-based gaze estimation;gaze sensing method;visual saliency map;eye image capture;video frames;probability distribution;eye image similarity;gaze point identification;Gaussian process regression;feedback loop;gaze probability map;user attention estimation","Algorithms;Artificial Intelligence;Attention;Biomimetics;Computer Simulation;Eye Movements;Fixation, Ocular;Humans;Image Interpretation, Computer-Assisted;Models, Biological;Nonlinear Dynamics;Pattern Recognition, Automated;Pattern Recognition, Visual;Reproducibility of Results;Sensitivity and Specificity","95","10","38","","1 May 2012","","","IEEE","IEEE Journals"
"A 3D gaze estimation method based on facial feature tracking","X. Zhao; X. Zou; Z. Chi","School of Computer Science and Engineering, Northwestern Polytechnical, University, NWPU, Xi'an, China; School of Electronics and Information, Northwestern Polytechnical, University, NWPU, Xi'an, China; Department of Electronic and Information Engineering, The Hong Kong Polytechnic, University, PolyU, China","2012 International Conference on Computerized Healthcare (ICCH)","27 Jan 2014","2012","","","9","12","A 3D gaze estimation and tracking algorithm based on facial feature tracking is present in this paper. By using two cameras, 2D facial feature points are detected and tracked with Active Shape Models (ASM). Then, the 3D coordinates of the feature points on a face can be obtained by stereo vision. The full 3D pose of head is obtained by comparing the feature points of current pose with initial head pose. The head pose estimation eliminate the restraint of head movement. Based on a 3D eye model with facial feature points, 3D visual axis can be obtained by estimating the 3D pupil center and head pose after a one-time personal calibration. The experimental results show the accuracy of our gaze tracking system achieves less than 3 degree.","","978-1-4673-5129-4","10.1109/ICCH.2012.6724462","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6724462","gaze tracking;gaze estimation;head pose;non-intrusive;stereo cameras","Cameras;Three-dimensional displays;Head;Estimation;Facial features;Calibration;Visualization","calibration;eye;gaze tracking;iris recognition;pose estimation;stereo image processing","3D gaze estimation method;facial feature tracking;cameras;2D facial feature point detection;active shape models;ASM;3D coordinates;stereo vision;head pose estimation;3D eye model;facial feature points;3D visual axis;3D pupil center estimation;one-time personal calibration;gaze tracking system","","1","","13","","27 Jan 2014","","","IEEE","IEEE Conferences"
"Human intention recognition via eye tracking based on fuzzy inference","M. Wang; Y. Maeda; Y. Takahashi","Dept. of System Design Engineering, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan; Dept. of Human and Artificial Intelligent Systems, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan; Dept. of Human and Artificial Intelligent Systems, Graduate School of Engineering, University of Fukui, 3-9-1 Bunkyo, Fukui 910-8507, Japan","The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems","22 Apr 2013","2012","","","846","851","Intention recognition has been paid much attention on by researchers in intelligent systems recent years because it can make the interaction between human and intelligent agents to be more convenient. Intention recognition can use multiple factors as inputs such as gestures, face images and eye gaze position. In this paper, we propose user's eye gaze estimation position as input of a fuzzy system to achieve intention recognition. Our approach can be divided into two parts: user gaze estimation and intention recognition. In the user gaze estimation part, neural network has been used as the decision making unit, and then the user gaze position at the computer screen is estimated. In the intention recognition part, the intention is recognized by using of a fuzzy system after an initial intention region set has been found. An illustrative example of user's intention region recognition is given and discussed in details.","","978-1-4673-2743-5","10.1109/SCIS-ISIS.2012.6505330","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6505330","","","cooperative systems;decision making;estimation theory;eye;fuzzy set theory;gesture recognition;human computer interaction;inference mechanisms;neural nets","human intention recognition;eye tracking;fuzzy inference;intelligent systems;human interaction;intelligent agents;gestures;face images;eye gaze position;user eye gaze estimation position;fuzzy system;user gaze estimation;neural network;decision making unit;user gaze position;computer screen","","3","","11","","22 Apr 2013","","","IEEE","IEEE Conferences"
"A video database of human faces under near Infra-Red illumination for human computer interaction applications","S. L. Happy; A. Dasgupta; A. George; A. Routray","Department of Electrical Engineering, IIT Kharagpur, India; Department of Electrical Engineering, IIT Kharagpur, India; Department of Electrical Engineering, IIT Kharagpur, India; Department of Electrical Engineering, IIT Kharagpur, India","2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)","21 Mar 2013","2012","","","1","4","Human Computer Interaction (HCI) is an evolving area of research for coherent communication between computers and human beings. Some of the important applications of HCI as reported in literature are face detection, face pose estimation, face tracking and eye gaze estimation. Development of algorithms for these applications is an active field of research. However, availability of standard database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under Near Infra-Red (NIR) illumination. NIR illumination has gained its popularity for night mode applications since prolonged exposure to Infra-Red (IR) lighting may lead to many health issues. The database contains NIR videos of 60 subjects in different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms on face detection, eye detection, head tracking, eye gaze tracking etc. in NIR lighting.","","978-1-4673-4369-5","10.1109/IHCI.2012.6481868","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481868","Near Infra-Red(NIR) illumination;face database;face detection;face tracking;head pose","Databases;Lighting;Head;Cameras;Magnetic heads;Human computer interaction;Face detection","face recognition;human computer interaction;pose estimation;video signal processing;visual databases","video database;human faces;near infrared illumination;human computer interaction applications;HCI;face detection;face pose estimation;face tracking;eye gaze estimation;NIR;facial expressions;head orientations;illumination variation;facial occlusions","","5","","9","","21 Mar 2013","","","IEEE","IEEE Conferences"
"Video & EOG based investigation of pure saccades in human subjects","A. Chaudhuri; A. Dasgupta; A. Routray","Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India; Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India; Department of Electrical Engineering, IIT Kharagpur, Kharagpur, India","2012 4th International Conference on Intelligent Human Computer Interaction (IHCI)","21 Mar 2013","2012","","","1","6","Human Computer Interaction (HCI) is the methodology through which computing systems understand actions of human beings. Some important applications of HCI as reported in literature are eye gaze estimation, eye tracking, alertness and health monitoring etc. Of them major applications are based upon eye movements. However, accurate estimation of these measurements is still a challenge in research community. A particular type of eye movement i.e. saccadic eye movement has potential applications in HCI such as alertness assessment, disease diagnosis etc. Electrooculography (EOG) based measurement of saccadic movements has been reported to be an accurate method. However being a contact based method, alternative method as suggested by literature is image based approach. This paper aims to investigate the correlation between video and EOG based observation of pure horizontal directional saccades to establish a relationship between image and EOG signals.","","978-1-4673-4369-5","10.1109/IHCI.2012.6481872","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6481872","saccade;EOG;high speed imaging;pupil position;correlation of EOG and video","Electrooculography;Human computer interaction;Correlation;Electrodes;Electric potential;Conferences;Electroencephalography","electro-oculography;human computer interaction;medical image processing;video signal processing","video-based investigation;EOG-based investigation;human subjects;human computer interaction;HCI;computing systems;human being action;eye gaze estimation;eye tracking;health monitoring;saccadic eye movement;alertness assessment;disease diagnosis;electrooculography;contact-based method;image-based approach;horizontal directional saccades","","10","1","23","","21 Mar 2013","","","IEEE","IEEE Conferences"
"Lightweight, low-cost, side-mounted mobile eye tracking system","A. K. A. Hong; J. Pelz; J. Cockburn","Rochester Institute of Technology, Department of Computer Engineering, 83 Lomb Memorial Dr. Rochester, NY 14623; Rochester Institute of Technology, Center of Imaging Science, 83 Lomb Memorial Dr. Rochester, NY 14623; Rochester Institute of Technology, Department of Computer Engineering, 83 Lomb Memorial Dr. Rochester, NY 14623","2012 Western New York Image Processing Workshop","21 Feb 2013","2012","","","1","4","Commercial mobile eye tracking systems are readily available, but are costly and complex. They have an additional disadvantage in that the eye cameras are placed directly in the field of view of the subject in order to obtain a clear frontal view of the eye. We propose a lightweight, low-cost, side-mounted mobile eye tracking system that uses side-view eye images to estimate the gaze of the subject. Cameras are mounted on the side of the head using curved mirrors to split the captured frames into scene and eye images. A hybrid algorithm using both feature-based models and appearance-based models is designed to accommodate this novel system. Image sequences, consisting of 4339 frames from seven subjects are analyzed by the algorithm, resulting in a successful gaze estimation rate of 95.7%.","","978-1-4673-5600-8","10.1109/WNYIPW.2012.6466645","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6466645","Eye tracking;computer vision;image processing;side-mounted;mobile","Eyelids;Feature extraction;Calibration;Cameras;Image color analysis;Mobile communication;Image sequences","eye;image sequences;iris recognition;tracking","mobile eye tracking system;eye camera;side-view eye image;curved mirror;feature-based model;appearance-based model;image sequence;gaze estimation rate","","3","","8","","21 Feb 2013","","","IEEE","IEEE Conferences"
"Gaze estimation using local features and non-linear regression","F. Martinez; A. Carbone; E. Pissaloux","ISIR, Université Pierre et Marie Curie (UPMC), 4 place Jussieu, 75005 Paris, France; ISIR, Université Pierre et Marie Curie (UPMC), 4 place Jussieu, 75005 Paris, France; ISIR, Université Pierre et Marie Curie (UPMC), 4 place Jussieu, 75005 Paris, France","2012 19th IEEE International Conference on Image Processing","21 Feb 2013","2012","","","1961","1964","In this paper, we present an appearance-based gaze estimation method for a head-mounted eye tracker. The idea is to extract discriminative image descriptors with respect to gaze before applying a regression scheme. We employ multilevel Histograms of Oriented Gradients (HOG) features as our appearance descriptor. To learn the mapping between eye appearance and gaze coordinates, two learning-based approaches are evaluated : Support Vector Regression (SVR) and Relevance Vector Regression (RVR). Experimental results demonstrate that, despite the high dimensionality, our method works well and RVR provides a more efficient and generalized solution than SVR by retaining a low number of basis functions.","2381-8549","978-1-4673-2533-2","10.1109/ICIP.2012.6467271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6467271","gaze estimation;features extraction;nonlinear regression","Training;Estimation;Feature extraction;Calibration;Support vector machines;Databases;Linear regression","eye;feature extraction;gradient methods;learning (artificial intelligence);object tracking;regression analysis;relevance feedback;support vector machines","local features;nonlinear regression;appearance-based gaze estimation method;head-mounted eye tracker;discriminative image descriptor extraction;multilevel histogram-of-oriented gradient features;HOG features;eye appearance descriptor;gaze coordinates;learning-based approaches;support vector regression;SVR;relevance vector regression;RVR;high-dimensionality function;basis functions;mapping function","","22","","13","","21 Feb 2013","","","IEEE","IEEE Conferences"
"Model based 3D gaze estimation for provision of virtual eye contact","W. Waizenegger; N. Atzpadin; O. Schreer; I. Feldmann; P. Eisert","Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany; Fraunhofer Institute for Telecommunications, Heinrich Hertz Institute, Berlin, Germany","2012 19th IEEE International Conference on Image Processing","21 Feb 2013","2012","","","1973","1976","In recent years, video communication has received a rapidly increasing interest on the market. Still unsolved is the problem of eye contact. The conferee still needs to decide whether to look into the camera or directly to the screen. Recently, a solution to this problem was presented which is based on a real-time 3D modeling of the conferees [1]. In order to achieve direct eye contact the authors defined a virtual camera directly on the screen in the eyes of the remote conferee. This paper discusses the problem of adequately positioning this virtual camera. A new approach will be presented which performs an eye and gaze tracking directly on the real-time 3D model rather than on the 2D image. Our methods not only provides robust and highly accurate results but is also able to additionally measure the distance between the conferees eye and the display with high precision.","2381-8549","978-1-4673-2533-2","10.1109/ICIP.2012.6467274","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6467274","stereo gaze tracking;3D;video communication;camera calibration","Cameras;Iris;Estimation;Computational modeling;Solid modeling;Streaming media;Real-time systems","cameras;object tracking;solid modelling;stereo image processing;video communication","model based 3D gaze estimation;virtual eye contact provision;video communication;camera;real-time 3D modeling;remote conferee;virtual camera;gaze tracking;eye tracking;2D image","","6","","10","","21 Feb 2013","","","IEEE","IEEE Conferences"
"Head pose-free appearance-based gaze sensing via eye image synthesis","F. Lu; Y. Sugano; T. Okabe; Y. Sato","Institute of Industrial Science, the University of Tokyo, Japan; Institute of Industrial Science, the University of Tokyo, Japan; Institute of Industrial Science, the University of Tokyo, Japan; Institute of Industrial Science, the University of Tokyo, Japan","Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)","14 Feb 2013","2012","","","1008","1011","This paper addresses the problem of estimating human gaze from eye appearance under free head motion. Allowing head motion remains challenging because eye appearance changes significantly for different head poses, and thus new head poses require new training images. To avoid repetitive training, we propose to produce synthetic training images for varying head poses. First, we model pixel displacements between head-moving eye images as 1D pixel flows, and then produce such flows to synthesize new training images from the original training images captured under a fixed default head pose. Specifically, we produce all the required 1D flows by using only four additionally captured images. Our method was successfully tested with extensive experiments to demonstrate its effectiveness.","1051-4651","978-4-9906441-0-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460306","","Head;Training;Cameras;Estimation;Accuracy;Image generation;Feature extraction","eye;pose estimation","head pose-free appearance-based gaze sensing;eye image synthesis;human gaze estimation;eye appearance;free head motion;synthetic training images;pixel displacements;head-moving eye images;1D pixel flows","","3","","13","","14 Feb 2013","","","IEEE","IEEE Conferences"
"Robust multi-pose face tracking by multi-stage tracklet association","M. Roth; M. Bäuml; R. Nevatia; R. Stiefelhagen","Institute for Anthropomatics Karlsruhe Institute of Technology; Institute for Anthropomatics Karlsruhe Institute of Technology; Institute for Robotics and Intelligent Systems, University of Southern California, Los Angeles, CA 90089; Institute for Anthropomatics Karlsruhe Institute of Technology","Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)","14 Feb 2013","2012","","","1012","1016","We propose an approach for multi-pose face tracking by association of face detection responses in two stages using multiple cues. The low-level stage uses a two-threshold strategy to merge detection responses based on location, size and pose, resulting in short but reliable tracklets. The high-level stage uses different cues for computing a joint similarity measure between tracklets. The facial cue compares facial features of the most frontal face detections in pairs of tracklets. The classifier cue learns a discriminative appearance model for each tracklet, using detection pairs within reliable tracklets and between overlapping tracklets as training data. The constraint cue observes the compatibility of motion of two tracklets. The association of tracklets is globally optimized with the Hungarian algorithm. We validate our approach on two challenging episodes of two TV series and report a Multiple Object Tracking Accuracy (MOTA) of 82% and 68.2%, respectively.","1051-4651","978-4-9906441-0-9","","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460307","","Face;Face detection;Detectors;Reliability;Joints;Target tracking","combinatorial mathematics;emotion recognition;face recognition;image classification;image motion analysis;image segmentation;object detection;object tracking;optimisation;pose estimation","robust multipose face tracking;multistage tracklet association;face detection responses;multiple cues;two-threshold strategy;similarity measure;facial cue;classifier cue;discriminative appearance model;detection pairs;overlapping tracklets;motion compatibility;Hungarian algorithm;TV series;multiple object tracking accuracy;MOTA;face recognition;gaze estimation;emotion recognition;interaction analysis","","2","1","11","","14 Feb 2013","","","IEEE","IEEE Conferences"
"Automatic Actor Recognition for Video Services on Mobile Devices","L. Cheok; S. Y. Heo; D. Mitrani; A. Tewari","Dallas Technol. Lab., Samsung Telecommun. America, Richardson, TX, USA; Dallas Technol. Lab., Samsung Telecommun. America, Richardson, TX, USA; Dallas Technol. Lab., Samsung Telecommun. America, Richardson, TX, USA; Dallas Technol. Lab., Samsung Telecommun. America, Richardson, TX, USA","2012 IEEE International Symposium on Multimedia","31 Jan 2013","2012","","","384","385","Face recognition is one of the most promising and successful applications of image analysis and understanding. Applications include biometrics identification, gaze estimation, emotion recognition, human computer interface, among others. A closed system trained to recognize only a predetermined number of faces will become obsolete very easily. In this paper, we describe a demo that we have developed using face detection and recognition algorithms for recognizing actors/actresses in movies. The demo runs on a Samsung tablet to recognize actors/actresses in the video. We also present our proposed method that allows user to interact with the system during training while watching video. New faces are tracked and trained into new face classifiers as video is continuously playing and the face database is updated dynamically.","","978-1-4673-4370-1","10.1109/ISM.2012.80","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6424694","face detection;face recognition;image analysis;image understanding","Face recognition;Face;Training data;Training;Graphical user interfaces;Accuracy;Computer architecture","face recognition;notebook computers;object detection;video signal processing","automatic actor recognition;video services;mobile devices;face recognition algorithm;image analysis;biometrics identification;gaze estimation;emotion recognition;human computer interface;face detection;Samsung tablet;face database","","1","","5","","31 Jan 2013","","","IEEE","IEEE Conferences"
"Long range eye gaze tracking system for a large screen","D. Cho; W. Yap; H. Lee; I. Lee; W. Kim","Electronics Computer Engineering, Hanyang University, Seoul, Korea; Electronics Computer Engineering, Hanyang University, Seoul, Korea; Electronics and Telecommunications Research Institute, Daejeon, Korea; Electronics and Telecommunications Research Institute, Daejeon, Korea; Electronics Computer Engineering, Hanyang University, Seoul, Korea","IEEE Transactions on Consumer Electronics","24 Jan 2013","2012","58","4","1119","1128","Eye gaze tracking system has been widely researched for the replacement of the conventional computer interfaces such as the mouse and keyboard. In this paper, we propose the long range binocular eye gaze tracking system that works from 1.5 m to 2.5 m with allowing a head displacement in depth. The 3D position of the user's eye is obtained from the two wide angle cameras. A high resolution image of the eye is captured using the pan, tilt, and focus controlled narrow angle camera. The angles for maneuvering the pan and tilt motor are calculated by the proposed calibration method based on virtual camera model. The performance of the proposed calibration method is verified in terms of speed and convenience through the experiment. The narrow angle camera keeps tracking the eye while the user moves his head freely. The point-of-gaze (POG) of each eye onto the screen is calculated by using a 2D mapping based gaze estimation technique and the pupil center corneal reflection (PCCR) vector. PCCR vector modification method is applied to overcome the degradation in accuracy with displacements of the head in depth. The final POG is obtained by the average of the two POGs. Experimental results show that the proposed system robustly works for a large screen TV from 1.5 m to 2.5 m distance with displacements of the head in depth (+20 cm) and the average angular error is 0.69°.","1558-4127","","10.1109/TCE.2012.6414976","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6414976","Eye gaze tracking;human computerinteraction;gaze estimation;pupil-corneal","Cameras;Calibration;Face;Optical imaging;Tracking;Image resolution","calibration;cameras;eye;image resolution","long range eye gaze tracking system;large screen;keyboard;mouse;binocular eye gaze tracking system;3D position;high resolution image;calibration method;angle camera;point-of-gaze;2D mapping;pupil center corneal reflection;PCCR vector;large screen TV","","24","1","26","","24 Jan 2013","","","IEEE","IEEE Journals"
"Gaze estimation using Kinect/PTZ camera","R. Jafari; D. Ziou","Departement d'informatique, Universite de Sherbrooke, Quebec, Canada, J1K 2R1; Departement d'informatique, Universite de Sherbrooke, Quebec, Canada, J1K 2R1","2012 IEEE International Symposium on Robotic and Sensors Environments Proceedings","7 Jan 2013","2012","","","13","18","This paper describes a novel method for eye-gaze estimation under normal head movement. In this method, head position and orientation are acquired by Kinect while eye direction is obtained by PTZ camera. We propose the Bayesian multinomial logistic regression based on a variational approximation to construct a gaze mapping function from head and eyes features. Our proposed method eliminates stationary head position, awkward personal calibration procedure and active light source as three common drawbacks in most conventional techniques. The efficiency of the proposed method is validated by performance evaluation for different users under varying head position and orientation.","","978-1-4673-2706-0","10.1109/ROSE.2012.6402633","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6402633","","Cameras;Head;Estimation;Vectors;Iris;Logistics;Bayesian methods","approximation theory;Bayes methods;cameras;eye;iris recognition;regression analysis;variational techniques","eye-gaze estimation;Kinect camera;normal head movement;head position;head movement;head orientation;PTZ camera;Bayesian multinomial logistic regression;variational approximation;gaze mapping function;head features;eyes features;performance evaluation;pan-tilt-zoom camera","","12","","24","","7 Jan 2013","","","IEEE","IEEE Conferences"
"Accurate iris and eyelid tracking method for Gaze Estimation without calibration","K. Tamura; K. Hashimoto; Y. Aoki","Keio University, Kanagawa, Japan; Keio University, Kanagawa, Japan; Keio University, Kanagawa, Japan","2012 International Symposium on Optomechatronic Technologies (ISOT 2012)","7 Jan 2013","2012","","","1","4","Today, there is a great demand for an easier and more useful Gaze Estimation system, whereas most gaze estimation systems require special equipment or complex calibration. In this report, we propose a new method which tracks iris and eyelid automatically without any calibration, which will be useful gaze estimation in daily lives. Eyelid shape tracking is useful for more accurate iris tracking and user's state estimation.","","978-1-4673-2877-7","10.1109/ISOT.2012.6403288","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6403288","Iris Tracking;Eyelid Tracking;Eyelid Model;Particle Filter","Eyelids;Iris recognition","iris recognition;object tracking;particle filtering (numerical methods)","iris tracking method;gaze estimation system;complex calibration;eyelid shape tracking method;state estimation;particle filter","","","","6","","7 Jan 2013","","","IEEE","IEEE Conferences"
"Context-based understanding of interaction intentions","J. Quintas; L. Almeida; M. Brito; G. Quintela; P. Menezes; J. Dias","Institute of Systems and Robotics - University of Coimbra, Portugal; Institute of Systems and Robotics - University of Coimbra, Portugal; Institute of Systems and Robotics - University of Coimbra, Portugal; Institute of Systems and Robotics - University of Coimbra, Portugal; Institute of Systems and Robotics - University of Coimbra, Portugal; Institute of Systems and Robotics - University of Coimbra, Portugal","2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication","10 Nov 2012","2012","","","515","520","This paper focus in the importance of context awareness and intention understanding capabilities in modern robots when faced with different situations. The inclusion of such requirements in robot design aim for more intelligent robots capable to adapt its behaviours to the faced situations. Gaze estimation and gesture interpretation are modalities, closely related with context-depent human intention understanding, that are addressed in this work.","1944-9437","978-1-4673-4606-1","10.1109/ROMAN.2012.6343803","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6343803","","Context;Estimation;Humans;Vectors;Robot kinematics;Head","gesture recognition;human-robot interaction;intelligent robots;pose estimation;robot vision;ubiquitous computing","context-based understanding;interaction intentions;context awareness;robot design;intelligent robots;context-depent human intention understanding;gaze estimation;gesture interpretation","","6","","22","","10 Nov 2012","","","IEEE","IEEE Conferences"
"Driver head pose and gaze estimation based on multi-template ICP 3-D point cloud alignment","T. Bär; J. F. Reuter; J. M. Zöllner","Intelligent Systems and Production Engineering (ISPE), FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; Intelligent Systems and Production Engineering (ISPE), FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany; Intelligent Systems and Production Engineering (ISPE), FZI Forschungszentrum Informatik, Haid-und-Neu-Str. 10-14, 76131 Karlsruhe, Germany","2012 15th International IEEE Conference on Intelligent Transportation Systems","25 Oct 2012","2012","","","1797","1802","Head movements, combined with the line of gaze, play a fundamental role in predicting the driver's actions and in inferring his intention. However, a gaze tracking system for automotive applications needs to satisfy high demands: It must not disturb the driver in his freedom of movements, it must cover large and fast head turns in yaw and pitch, be resistant to changing illumination conditions, be fast enough to recognize fast mirror checks, which are performed almost exclusively through eye rather than head movements, and be accurate and reliable enough to derive high quality information for driver assistance systems relying on their output. In this work a multi-template, ICP-based gaze tracking system is introduced. The system determines the head pose and subsequently estimates the driver's line of gaze by analyzing the angles of the eyes. Due to a fast search of correspondences, and switching between point-to-point and point-to-plane alignment, real-time performance and high accuracy can be achieved. The system is compared with other state of the art head pose estimation systems based on a publicly available benchmark database, where a classification rate of 92% at a tolerance of 10 degrees in yaw could be achieved. We further show in the experiments section, that head rotations up to 4 radians per seconds can be handled. Taking the angles of the eyes into account, rather than the head pose only, the driver's line of sight could be successfully mapped to particular regions of interest.","2153-0017","978-1-4673-3063-3","10.1109/ITSC.2012.6338678","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6338678","","Face;Vehicles;Accuracy;Iterative closest point algorithm;Cameras;Estimation","driver information systems;pose estimation","driver head pose estimation;driver gaze estimation;multitemplate ICP 3D point cloud alignment;head movements;gaze tracking system;driver assistance systems;point-to-point alignment;point-to-plane alignment","","14","2","18","","25 Oct 2012","","","IEEE","IEEE Conferences"
"Robust pose-invariant eye gaze estimation using geometrical features of iris and pupil images","M. R. Mohammadi; A. Raie","Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran; Electrical Engineering Department, Amirkabir University of Technology, Tehran, Iran","20th Iranian Conference on Electrical Engineering (ICEE2012)","3 Sep 2012","2012","","","593","598","The “Gaze Estimation” problem, due to its manifold applications including human computer interaction (HCI) especially for the handicapped, has been a topic of research since many years ago. Recently, the non-intrusive methods, based on image processing, are more addressed which are obtained by development of the technology and can become more applicable. In this paper, we propose a non-intrusive algorithm for eye gaze estimation that works with video input from an inexpensive camera and without special lighting. The main contribution of this paper is to propose a new geometrical model for eye region that only requires image of one iris for gaze estimation. Essential parameters for this system are the best fitted ellipse of iris and pupil center. A novel algorithm is also proposed to estimate the best fitted ellipse of iris which uses most of iris boundary points and thus achieves high accuracy. Moreover, this algorithm, unlike previous ones, poses no pre-assumptions on the head pose. All in all, the achievement of this paper is the robustness of the proposed system to the head pose variations. The performance of the method has been evaluated on both synthetic and real images leading to errors of 2.12 and 3.48 degrees, respectively.","2164-7054","978-1-4673-1148-9","10.1109/IranianCEE.2012.6292425","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6292425","Gaze estimation;Projective geometry;Ellipse fitting;Pupil center","Iris;Lead;Geometry;Databases","geometry;human computer interaction;iris recognition;pose estimation","robust pose-invariant eye gaze estimation;geometrical features;iris images;pupil images;human computer interaction;HCI;image processing;nonintrusive algorithm;iris boundary points;head pose variations","","2","","21","","3 Sep 2012","","","IEEE","IEEE Conferences"
"Gaze estimation from multimodal Kinect data","K. A. Funes Mora; J. Odobez","Idiap Research Institute, Martigny, Switzerland; Idiap Research Institute, Martigny, Switzerland","2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","16 Jul 2012","2012","","","25","30","This paper addresses the problem of free gaze estimation under unrestricted head motion. More precisely, unlike previous approaches that mainly focus on estimating gaze towards a small planar screen, we propose a method to estimate the gaze direction in the 3D space. In this context the paper makes the following contributions: (i) leveraging on Kinect device, we propose a multimodal method that rely on depth sensing to obtain robust and accurate head pose tracking even under large head pose, and on the visual data to obtain the remaining eye-in-head gaze directional information from the eye image; (ii) a rectification scheme of the image that exploits the 3D mesh tracking, allowing to conduct a head pose free eye-in-head gaze directional estimation; (iii) a simple way of collecting ground truth data thanks to the Kinect device. Results on three users demonstrate the great potential of our approach.","2160-7516","978-1-4673-1612-5","10.1109/CVPRW.2012.6239182","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6239182","","Magnetic heads;Vectors;Face;Estimation;Solid modeling;Visualization","image motion analysis;interactive devices;mesh generation;object tracking;pose estimation","gaze estimation;multimodal Kinect data;head motion;3D space;Kinect device;multimodal method;depth sensing;head pose tracking;visual data;eye-in-head gaze directional information;image rectification scheme;3D mesh tracking","","38","9","16","","16 Jul 2012","","","IEEE","IEEE Conferences"
"Gaze Estimation Interpolation Methods Based on Binocular Data","L. Sesma-Sanchez; A. Villanueva; R. Cabeza","Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, Spain; Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, Spain; Department of Electrical and Electronic Engineering, Public University of Navarra, Pamplona, Spain","IEEE Transactions on Biomedical Engineering","12 Jul 2012","2012","59","8","2235","2243","Video oculography (VOG) is one of the most commonly used techniques for gaze tracking because it enables nonintrusive eye detection and tracking. Improving the eye tracking's accuracy and tolerance to user head movements is a common task in the field of gaze tracking; thus, a thorough study of how binocular information can improve a gaze tracking system's accuracy and tolerance to user head movements has been carried out. The analysis is focused on interpolation-based methods and systems with one and two infrared lights. New mapping features are proposed based on the commonly used pupil-glint vector using different distances as the normalization factor. For this study, an experimental procedure with six users based on a real VOG gaze tracking system was performed, and the results were contrasted with an eye simulator. Important conclusions have been obtained in terms of configuration, equation, and mapping features, such as the outperformance of the interglint distance as the normalization factor. Furthermore, the binocular gaze tracking system was found to have a similar or improved level of accuracy compared to that of the monocular gaze tracking system.","1558-2531","","10.1109/TBME.2012.2201716","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6208829","Binocular;gaze tracking;infrared;interpolation based;video oculography (VOG)","Calibration;Estimation;Accuracy;Vectors;Cameras;Polynomials","biomechanics;eye;feature extraction;interpolation;medical computing;object detection;object tracking;video signal processing","gaze estimation interpolation methods;video oculography;gaze tracking;nonintrusive eye detection;nonintrusive eye tracking;head movements;interpolation-based methods;infrared lights;mapping features;pupil-glint vector;normalization factor;VOG gaze tracking system;eye simulator;interglint distance;normalization factor;binocular gaze tracking system;monocular gaze tracking system","Algorithms;Biometry;Eye Movements;Fixation, Ocular;Head Movements;Humans;Image Processing, Computer-Assisted;Infrared Rays;Video Recording;Vision, Binocular","30","3","19","IEEE","30 May 2012","","","IEEE","IEEE Journals"
"Gaze Fixation System for the Evaluation of Driver Distractions Induced by IVIS","P. Jiménez; L. M. Bergasa; J. Nuevo; N. Hernández; I. G. Daza","Gamesa, Zamudio, Spain; Department of Electronics, University of Alcalá, Madrid, Spain; CSIRO ICT Centre Computer Vision Group, Brisbane, Australia; Department of Electronics, University of Alcalá, Madrid, Spain; Computer Engineering Department, University of Alcalá, 28805, Madrid, Spain","IEEE Transactions on Intelligent Transportation Systems","28 Aug 2012","2012","13","3","1167","1178","We present a method to monitor driver distraction based on a stereo camera to estimate the face pose and gaze of a driver in real time. A coarse eye direction is composed of face pose estimation to obtain the gaze and driver's fixation area in the scene, which is a parameter that gives much information about the distraction pattern of the driver. The system does not require any subject-specific calibration; it is robust to fast and wide head rotations and works under low-lighting conditions. The system provides some consistent statistics, which help psychologists to assess the driver distraction patterns under influence of different in-vehicle information systems (IVISs). These statistics are objective, as the drivers are not required to report their own distraction states. The proposed gaze fixation system has been tested on a set of challenging driving experiments directed by a team of psychologists in a naturalistic driving simulator. This simulator mimics conditions present in real driving, including weather changes, maneuvering, and distractions due to IVISs. Professional drivers participated in the tests.","1558-0016","","10.1109/TITS.2012.2187517","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165366","Distraction monitoring;driver;gaze fixation;inattention;in-vehicle information systems (IVISs);naturalistic simulator;percent road center (PRC)","Information systems;Face;Estimation;Cameras;Road safety;Monitoring;Algorithm design and analysis","behavioural sciences computing;driver information systems;face recognition;pose estimation;statistics","gaze fixation system;driver distraction evaluation;face pose estimation;gaze estimation;coarse eye direction;driver distraction pattern;IVIS;in-vehicle information system;head rotation;low-lighting condition;statistic;naturalistic driving simulator;weather change;maneuvering","","33","","43","IEEE","6 Mar 2012","","","IEEE","IEEE Journals"
"Combining Head Pose and Eye Location Information for Gaze Estimation","R. Valenti; N. Sebe; T. Gevers","Intelligent Systems Laboratorium Amsterdam, University of Amsterdam, Amsterdam, The Netherlands; Department of Information Engineering and Computer Science, University of Trento, Trento, Italy; Intelligent Systems Laboratorium Amsterdam, University of Amsterdam, Amsterdam, The Netherlands","IEEE Transactions on Image Processing","16 Jan 2012","2012","21","2","802","815","Head pose and eye location for gaze estimation have been separately studied in numerous works in the literature. Previous research shows that satisfactory accuracy in head pose and eye location estimation can be achieved in constrained settings. However, in the presence of nonfrontal faces, eye locators are not adequate to accurately locate the center of the eyes. On the other hand, head pose estimation techniques are able to deal with these conditions; hence, they may be suited to enhance the accuracy of eye localization. Therefore, in this paper, a hybrid scheme is proposed to combine head pose and eye location information to obtain enhanced gaze estimation. To this end, the transformation matrix obtained from the head pose is used to normalize the eye regions, and in turn, the transformation matrix generated by the found eye location is used to correct the pose estimation procedure. The scheme is designed to enhance the accuracy of eye location estimations, particularly in low-resolution videos, to extend the operative range of the eye locators, and to improve the accuracy of the head pose tracker. These enhanced estimations are then combined to obtain a novel visual gaze estimation system, which uses both eye location and head information to refine the gaze estimates. From the experimental results, it can be derived that the proposed unified scheme improves the accuracy of eye estimations by 16% to 23%. Furthermore, it considerably extends its operating range by more than 15<sup>°</sup> by overcoming the problems introduced by extreme head poses. Moreover, the accuracy of the head pose tracker is improved by 12% to 24%. Finally, the experimentation on the proposed combined gaze estimation system shows that it is accurate (with a mean error between 2<sup>°</sup> and 5<sup>°</sup>) and that it can be used in cases where classic approaches would fail without imposing restraints on the position of the head.","1941-0042","","10.1109/TIP.2011.2162740","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5959981","Eye center location;gaze estimation;head pose estimation","Head;Estimation;Three dimensional displays;Accuracy;Cameras;Solid modeling;Visualization","eye;image resolution;matrix algebra;mean square error methods;pose estimation;video signal processing","eye location information;satisfactory accuracy;constrained settings;nonfrontal faces;eye locators;head pose estimation techniques;transformation matrix;eye regions;found eye location;pose estimation procedure;eye location estimations;low-resolution videos;head pose tracker;visual gaze estimation system;head information;mean error","Eye;Eye Movements;Head;Head Movements;Humans;Image Processing, Computer-Assisted;Posture;Video Recording","169","15","51","IEEE","21 Jul 2011","","","IEEE","IEEE Journals"
"Face pose estimation using distance transform and normalized cross-correlation","M. Shafi; F. Iqbal; I. Ali","Department of Computer Software, Engineering, U.E.T Peshawar Mardan Campus, Charsaddah Road Mardan; Department of Telecommunication, Engineering, U.E.T Peshawar Mardan Campus, Charsaddah Road Mardan; Department of Telecommunication, Engineering, U.E.T Peshawar Mardan Campus, Charsaddah Road Mardan","2011 IEEE International Conference on Signal and Image Processing Applications (ICSIPA)","2 Feb 2012","2011","","","186","191","Face pose estimation plays a vital role in human-computer interaction, automatic human behavior analysis, pose-independent face recognition, gaze estimation, virtual reality applications etc. A novel face pose estimation method using distance transform and normalized cross-correlation, is presented in this paper. The use of distance transform has two main advantages: first, unlike intensity image, distance transform is relatively invariant to intensity and illumination of image. Secondly, unlike edge-map the distance transform has a smoother distribution. The distance transform property of being invariant to intensity makes the proposed method suitable for different skin colors. Since distance transform is relatively invariant to illumination, the proposed method is also suitable for different illumination conditions. Further advantages are that the proposed method is relatively invariant to facial hairs and whether the subject is wearing glasses. The proposed method has been tested using the CAS-PEAL pose database with very good results.","","978-1-4577-0242-6","10.1109/ICSIPA.2011.6144086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6144086","Distance Transform;Face Pose Estimation;Normalized Cross-Correlation;Template Matching","Transforms;Face;Lighting;Databases;Estimation;Image edge detection;Mouth","behavioural sciences computing;face recognition;human computer interaction;image colour analysis;pose estimation;virtual reality;visual databases","distance transform;normalized cross correlation;human computer interaction;automatic human behavior analysis;pose independent face recognition;gaze estimation;virtual reality;face pose estimation method;intensity image;smoother distribution;skin color;illumination condition;wearing glass;CAS-PEAL pose database","","3","","22","","2 Feb 2012","","","IEEE","IEEE Conferences"
"Unsupervised learning of a scene-specific coarse gaze estimator","B. Benfold; I. Reid","Department of Engineering Science, University of Oxford, UK; Department of Engineering Science, University of Oxford, UK","2011 International Conference on Computer Vision","26 Jan 2012","2011","","","2344","2351","We present a method to estimate the coarse gaze directions of people from surveillance data. Unlike previous work we aim to do this without recourse to a large hand-labelled corpus of training data. In contrast we propose a method for learning a classifier without any hand labelled data using only the output from an automatic tracking system. A Conditional Random Field is used to model the interactions between the head motion, walking direction, and appearance to recover the gaze directions and simultaneously train randomised decision tree classifiers. Experiments demonstrate performance exceeding that of conventionally trained classifiers on two large surveillance datasets.","2380-7504","978-1-4577-1102-2","10.1109/ICCV.2011.6126516","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126516","","Angular velocity;Head;Vegetation;Legged locomotion;Data models;Image color analysis;Optimization","decision trees;image classification;unsupervised learning","unsupervised learning;scene-specific coarse gaze estimator;coarse gaze direction estimation;classifier learning;conditional random field;head motion;walking direction;appearance;randomised decision tree classifier","","43","","19","","26 Jan 2012","","","IEEE","IEEE Conferences"
"Illumination-free gaze estimation method for first-person vision wearable device","A. Tsukada; M. Shino; M. Devyver; T. Kanade","Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA","2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)","16 Jan 2012","2011","","","2084","2091","Gaze estimation is a key technology to understand a person's interests and intents, and it is becoming more popular in daily situations such as driving scenarios. Wearable gaze estimation devices are use for long periods of time, therefore non-active sources are not desirable from a safety point of view. Gaze estimation that does not rely on active source, is performed by locating iris position. To estimate the iris position accurately, most studies use ellipse fitting in which the ellipse is defined by 5 parameters(position (x,y), rotation angle, semi-major axis and semi-minor axis). We claim that, for iris position estimation, 5 parameters are redundant because they might be influenced by non-iris edges. Therefore, we propose to use 2 parameters(position) introducing a 3D eye model(the transformation between eye and camera coordinate and eyeball/iris size). Given 3D eye model, projected ellipse that represents iris shape can be specified only by position under weak-perspective approximation. We quantitatively evaluate our method on both iris position and gaze estimation. Our results show that our method outperforms other state-of-the-art's iris estimation and is competitive to commercial product that use infrared ray with respect to both accuracy and robustness.","","978-1-4673-0063-6","10.1109/ICCVW.2011.6130505","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130505","","Iris;Estimation;Image edge detection;Shape;Three dimensional displays;Robustness;Solid modeling","approximation theory;iris recognition;pose estimation","illumination-free gaze estimation method;first-person vision wearable device;wearable gaze estimation device;nonactive source;iris position estimation;noniris edge;3D eye model;iris shape representation;infrared ray","","28","1","18","","16 Jan 2012","","","IEEE","IEEE Conferences"
"Robust validation of Visual Focus of Attention using adaptive fusion of head and eye gaze patterns","S. Asteriadis; K. Karpouzis; S. Kollias","National Technical University of Athens, Greece, Electrical and Computer Engineering Department; National Technical University of Athens, Greece, Electrical and Computer Engineering Department; National Technical University of Athens, Greece, Electrical and Computer Engineering Department","2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)","16 Jan 2012","2011","","","414","421","We propose a framework for inferring the focus of attention of a person, utilizing information coming both from head rotation and eye gaze estimation. To this aim, we use fuzzy logic to estimate confidence on the gaze of a person towards a specific point, and results are compared to human annotation. For head pose we propose Bayesian modality fusion of both local and holistic information, while for eye gaze we propose a methodology that calculates eye gaze directionality, removing the influence of head rotation, using a simple camera. For local information, feature positions are used, while holistic information makes use of face region. Holistic information uses Convolutional Neural Networks which have been shown to be immune to small translations and distortions of test data. This is vital for an application in an unpretending environment, where background noise should be expected. The ability of the system to estimate focus of attention towards specific areas, for unknown users, is grounded at the end of the paper.","","978-1-4673-0063-6","10.1109/ICCVW.2011.6130271","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6130271","","Face;Estimation;Reliability;Training;Bayesian methods;Vectors","Bayes methods;cameras;feature extraction;fuzzy logic;inference mechanisms;iris recognition;neural nets","eye gaze pattern estimation;adaptive head fusion;visual focus;robust validation;fuzzy logic;confidence estimation;human annotation;Bayesian modality fusion;eye gaze directionality;feature positions;holistic information;convolutional neural networks;background noise","","4","","33","","16 Jan 2012","","","IEEE","IEEE Conferences"
"Inferring human gaze from appearance via adaptive linear regression","F. Lu; Y. Sugano; T. Okabe; Y. Sato","Institute of Industrial Science, the University of Tokyo, Japan; Institute of Industrial Science, the University of Tokyo, Japan; Institute of Industrial Science, the University of Tokyo, Japan; Institute of Industrial Science, the University of Tokyo, Japan","2011 International Conference on Computer Vision","12 Jan 2012","2011","","","153","160","The problem of estimating human gaze from eye appearance is regarded as mapping high-dimensional features to low-dimensional target space. Conventional methods require densely obtained training samples on the eye appearance manifold, which results in a tedious calibration stage. In this paper, we introduce an adaptive linear regression (ALR) method for accurate mapping via sparsely collected training samples. The key idea is to adaptively find the subset of training samples where the test sample is most linearly representable. We solve the problem via l<sup>1</sup>-optimization and thoroughly study the key issues to seek for the best solution for regression. The proposed gaze estimation approach based on ALR is naturally sparse and low-dimensional, giving the ability to infer human gaze from variant resolution eye images using much fewer training samples than existing methods. Especially, the optimization procedure in ALR is extended to solve the subpixel alignment problem simultaneously for low resolution test eye images. Performance of the proposed method is evaluated by extensive experiments against various factors such as number of training samples, feature dimensionality and eye image resolution to verify its effectiveness.","2380-7504","978-1-4577-1102-2","10.1109/ICCV.2011.6126237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6126237","","Feature extraction;Training;Equations;Image resolution;Estimation;Manifolds;Mathematical model","calibration;image resolution;optimisation;regression analysis","inferring human gaze;adaptive linear regression;human gaze estimation;eye appearance;calibration stage;optimization;eye image resolution","","60","2","19","","12 Jan 2012","","","IEEE","IEEE Conferences"
"Display control based on eye gaze estimation","B. Fu; R. Yang","Computer Science Department, University of Kentucky, Lexington, USA; Computer Science Department, University of Kentucky, Lexington, USA","2011 4th International Congress on Image and Signal Processing","12 Dec 2011","2011","1","","399","403","Detecting and tracking eye gaze is an active research area and significant progress has been made in this area in the past decades. However, challenge remains due to the differences between individual's eyes, variability in light condition, scale and occlusion. Works on eye location and eye movements have a large number of applications and is an important part of biometrics, human-computer interaction and face detection. Based on the current works and applications on eye gaze detection, the author notice, however, applying eye gaze information on human-computer interaction still lacks of enough work on it. This paper proposes a novel pipeline of employing eye gaze information for display control based on the video captured by integrated camera. The proposed pipeline shows that, despite the low quality of the video and light condition, eye gaze can still be estimated and display of the screen can be controlled accordingly.","","978-1-4244-9306-7","10.1109/CISP.2011.6099973","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6099973","eye tracking;eye gaze;display control","Tracking;Estimation;Face;Iris;Cameras;Shape;Computer vision","biometrics (access control);human computer interaction;object detection","display control;eye gaze estimation;eye location;eye movements;biometrics;human-computer interaction;face detection;eye gaze detection","","12","","21","","12 Dec 2011","","","IEEE","IEEE Conferences"
"Research on eye gaze estimation technique base on 3D model","K. Cen; M. Che","School of Computer Science and Technology, Tianjin University, Tianjin, China; School of Computer Science and Technology, Tianjin University, Tianjin, China","2011 International Conference on Electronics, Communications and Control (ICECC)","3 Nov 2011","2011","","","1623","1626","This paper proposes a 3D model gaze estimation method based on a single-camera that can widely use as the computer input device for human computer interaction. To avoid using extra light sources and allow free head movements, our method creates an accurate model of the eye and gaze estimation algorithm, which based on the spatial relationships of the eye and the screen. Establishment of 3D gaze estimation model allows the user at any distance and under natural head movement. Compared with the present gaze estimation methods, our proposed method allows the head have a substantial movement, even the head of rotating. The proposed system is easy to implement and adaptable, and the experimental results show that the accuracy of this algorithm is less than 5° and have a good stability.","","978-1-4577-0321-8","10.1109/ICECC.2011.6067739","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6067739","gaze estimation;3D model;face calibration;ditance estimation;human computer interaction","Estimation;Accuracy;Cameras;Face;Three dimensional displays;Optical imaging","eye;human computer interaction;solid modelling","3D model eye gaze estimation method;single-camera;computer input device;human computer interaction;rotating head","","4","","11","","3 Nov 2011","","","IEEE","IEEE Conferences"
"Realtime AAM based user attention estimation","S. Hommel; U. Handmann","Computer Science Institute, Hochschule Ruhr West, Germany, Bottrop 46240; Computer Science Institute, Hochschule Ruhr West, Germany, Bottrop 46240","2011 IEEE 9th International Symposium on Intelligent Systems and Informatics","3 Oct 2011","2011","","","201","206","In this paper a method of automatic real-time capable visual user attention for a face to face human machine interaction is described. This method based on Active Appearance Models (AAMs) and Multilayer Perceptrons (MLPs) to map the Active Appearance Parameters (AAM-Parameters) onto the current head pose. Afterwards, the chronology of the head pose becomes classified to attention or inattention. This visual attention estimation will be used in service robotic by human-robotic interaction to get a feedback whether the user is interested in the current dialog and for correct interpretation of the current emotional condition. To allow a more natural dialog the head pose is also very efficient interpreted as head nodding or shaking by the use of adaptive statistical moments. Especially, the head movement of many demented people are restricted, so they often only use their eyes to look around. For that reason this paper examine a simple gaze estimation with the help of an ordinary webcam.","1949-0488","978-1-4577-1974-5","10.1109/SISY.2011.6034322","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6034322","Active Appearance Model;Head Gesture Recognition;Visual Attention;Gaze Estimation","Magnetic heads;Estimation;Active appearance model;Shape;Visualization;Face","cameras;human-robot interaction;multilayer perceptrons;pose estimation;robot vision;service robots;statistical analysis","realtime AAM;user attention estimation;visual user attention;face to face human machine interaction;active appearance models;multilayer perceptrons;active appearance parameters;head pose;visual attention estimation;service robotic;human robotic interaction;head nodding;head shaking;adaptive statistical moments;gaze estimation;webcam","","2","","13","","3 Oct 2011","","","IEEE","IEEE Conferences"
"Remote point-of-gaze estimation with single-point personal calibration based on the pupil boundary and corneal reflections","E. D. Guestrin; M. Eizenman","Department of Electrical and Computer Engineering, Toronto, Ontario, Canada; Department of Electrical and Computer Engineering, Toronto, Ontario, Canada","2011 24th Canadian Conference on Electrical and Computer Engineering(CCECE)","29 Sep 2011","2011","","","000971","000976","This paper describes a new method for remote, non-contact point-of-gaze (PoG) estimation that tolerates head movements and requires a simple personal calibration procedure in which the subject has to fixate a single calibration point. This method uses the pupil boundary and at least two corneal reflections (virtual images of light sources) that are extracted from eye images captured by at least two video cameras. Experimental results obtained with a prototype system exhibited RMS PoG estimation errors of approx. 0.4-0.6° of visual angle. Such accuracy is comparable to that of the best commercially available systems, which use multiple-point personal calibration procedures, and significantly better than that of any other systems that use a single-point personal calibration procedure and have been previously described in the literature.","0840-7789","978-1-4244-9789-8","10.1109/CCECE.2011.6030604","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030604","Point-of-gaze;pupil boundary;corneal reflections;single-point personal calibration;remote gaze estimation","Cameras;Reflection;Light sources;Calibration;Cornea;Visualization;Estimation","feature extraction;iris recognition;light reflection;light sources;mean square error methods;video cameras","remote noncontact point of gaze estimation;single point personal calibration;pupil boundary;corneal reflection;head movements;virtual images;light source;visual angle;RMS PoG estimation errors;prototype system;video cameras;eye images","","4","1","16","","29 Sep 2011","","","IEEE","IEEE Conferences"
"User-calibration-free remote eye-gaze tracking system with extended tracking range","D. Model; M. Eizenman","Department of Electrical and Computer Engineering, University of Toronto; Department of Electrical and Computer Engineering, University of Toronto","2011 24th Canadian Conference on Electrical and Computer Engineering(CCECE)","29 Sep 2011","2011","","","001268","001271","A novel general method to extend the tracking range of user-calibration-free remote eye-gaze tracking (REGT) systems that are based on the analysis of stereo-images from multiple cameras is presented. The method consists of two distinct phases. In the brief initial phase, estimates of the center of the pupil and corneal reflections in pairs of stereo images are used to estimate automatically a set of subject-specific eye parameters. In the second phase, these subject-specific eye parameters are used with estimates of the center of the pupil and corneal reflections in images from any one of the systems' cameras to compute the Point-of-Gaze (PoG). Experiments with a system that includes two cameras show that the tracking range for horizontal gaze directions can be extended from ±23.2° when the two cameras are used as a stereo pair to ±35.5° when the two cameras are used independently to estimate the POG.","0840-7789","978-1-4244-9789-8","10.1109/CCECE.2011.6030667","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6030667","Eye Tracking;Remote Gaze Estimation;Extended Range;Calibration-Free;Distributed Eye-gaze Tracker","Cameras;Estimation;Optical reflection;Optical imaging;Calibration;Cornea","calibration;eye;stereo image processing","User-calibration-free remote eye-gaze tracking system;extended tracking range;stereo-image analysis;pupil-corneal reflection center estimation;subject-specific eye parameters;point-of-gaze","","6","2","19","","29 Sep 2011","","","IEEE","IEEE Conferences"
"Gaze and body pose estimation from a distance","N. Krahnstoever; M. Chang; W. Ge","GE Global Research Center, One Research Circle, Niskayuna, NY, USA; GE Global Research Center, One Research Circle, Niskayuna, NY, USA; GE Global Research Center, One Research Circle, Niskayuna, NY, USA","2011 8th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)","26 Sep 2011","2011","","","11","16","We present a comprehensive approach to track gaze by estimating location, body pose, and head pose direction of multiple individuals in unconstrained environments. The approach combines person detections from fixed cameras with directional face detections obtained from actively controlled pan tilt zoom (PTZ) cameras. The main contribution of this work is to estimate both body pose and head pose (gaze) direction independently from motion direction, using a combination of sequential Monte Carlo Filtering and MCMC sampling. There are numerous benefits in tracking body pose and gaze in surveillance. It allows to track people's focus of attention, can optimize the control of active cameras for biometric face capture, and can provide better interaction metrics between pairs of people. The availability of gaze and face detection information also improves localization and data association for tracking in crowded environments. The performance of the system will be demonstrated on data captured at a real-time surveillance site.","","978-1-4577-0845-9","10.1109/AVSS.2011.6027285","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6027285","","Face;Cameras;Face detection;Target tracking;Estimation","cameras;face recognition;Monte Carlo methods;motion estimation;pose estimation","body pose estimation;gaze estimation;pan tilt zoom cameras;PTZ camera;sequential Monte Carlo Filtering;MCMC sampling;motion direction;real-time surveillance site;biometric face detection information","","5","1","18","","26 Sep 2011","","","IEEE","IEEE Conferences"
"Detecting human behavior emotional cues in Natural Interaction","G. Caridakis; S. Asteriadis; K. Karpouzis; S. Kollias","Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece; Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece; Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece; Intelligent Systems, Content and Interaction Lab, National Technical University of Athens, Iroon Polytexneiou 9, 15780 Zografou, Greece","2011 17th International Conference on Digital Signal Processing (DSP)","29 Aug 2011","2011","","","1","6","Current work focuses on the detection of human behavior emotional cues and their incorporation into affect aware Natural Interaction. Techniques for extracting emotional cues based on visual non verbal human behavior are presented. Namely, gesture qualitative expressivity features and head pose and eye gaze estimation are derived from hand and facial movement respectively. Extracted emotional cues are employed in expressive synthesis on virtual agents, based on the analysis of actions performed by human users, in a Human-Virtual Agent Interaction setting and in Assistive Technologies aiming to infer in real time the degree of attention or frustration of children with reading difficulties.","2165-3577","978-1-4577-0274-7","10.1109/ICDSP.2011.6004962","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6004962","Affective computing;Natural Interaction;Gesture expressivity;Eye Gaze","Humans;Estimation;Feature extraction;Markov processes;Computational modeling;Visualization;Emotion recognition","emotion recognition;human computer interaction;multi-agent systems;object detection","human behavior emotional cues;affect aware natural interaction;visual nonverbal human behavior;gesture qualitative expressivity feature;head pose estimation;eye gaze estimation;human-virtual agent interaction;assistive technology","","2","","24","","29 Aug 2011","","","IEEE","IEEE Conferences"
"A parallel architecture of AdaBoost-based face detection for gaze estimation","Xueyi Liu; Ming Che","School of Computer Science and Technology, Tianjin University, China; School of Computer Science and Technology, Tianjin University, China","2011 International Conference on Multimedia Technology","25 Aug 2011","2011","","","2888","2891","Face detection is an essential application in image processing and computer vision. In this paper, a parallel architecture for AdaBoost algorithm is proposed to support face detection in the embedded system. Some optimizations for the AdaBoost algorithm are applied to accelerate the process. Based on the software/hardware co-design methodology, a configurable high performance CPU (Nios) can take charge of task scheduling, image scaling and simple processing. Dedicated hardware accelerators are designed to deal with data intensive computing tasks through multi-level parallelism to meet the real-time requirement in embedded applications. The architecture is prototyped on Altera Cyclone II FPGA The evaluation result shows that the system performs real-time detection with low resource consumption and high detection rate (96.7%). The detection speed of image consisting 640 × 480 pixels is 12 frames/s, and 41 frames/s for image consisting 320 × 240 pixels. It meets the requirement of gaze estimation.","","978-1-61284-774-0","10.1109/ICMT.2011.6003069","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6003069","AdaBoost;parallelized architecture;FPGA","Face detection;Computer architecture;Hardware;Pipelines;Face;Real time systems;Field programmable gate arrays","computer vision;embedded systems;face recognition;field programmable gate arrays;hardware-software codesign;multiprocessing systems;parallel architectures","parallel architecture;AdaBoost-based face detection;gaze estimation;image processing;computer vision;embedded system;software-hardware codesign methodology;configurable high performance CPU;task scheduling;image scaling;hardware accelerator;data intensive computing;multilevel parallelism;Altera Cyclone II FPGA;image detection speed","","","","8","","25 Aug 2011","","","IEEE","IEEE Conferences"
"Probabilistic gaze estimation without active personal calibration","J. Chen; Q. Ji","Department of Electrical, Computer and System Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180; Department of Electrical, Computer and System Engineering, Rensselaer Polytechnic Institute, Troy, NY 12180","CVPR 2011","22 Aug 2011","2011","","","609","616","Existing eye gaze tracking systems typically require an explicit personal calibration process in order to estimate certain person-specific eye parameters. For natural human computer interaction, such a personal calibration is often cumbersome and unnatural. In this paper, we propose a new probabilistic eye gaze tracking system without explicit personal calibration. Unlike the traditional eye gaze tracking methods, which estimate the eye parameter deterministically, our approach estimates the probability distributions of the eye parameter and the eye gaze, by combining image saliency with the 3D eye model. By using an incremental learning framework, the subject doesn't need personal calibration before using the system. His/her eye parameter and gaze estimation can be improved gradually when he/she is naturally viewing a sequence of images on the screen. The experimental result shows that the proposed system can achieve less than three degrees accuracy for different people without calibration.","1063-6919","978-1-4577-0395-9","10.1109/CVPR.2011.5995675","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995675","","Optical imaging;Calibration;Estimation;Three dimensional displays;Visualization;Probabilistic logic;Head","calibration;eye;human computer interaction;image sequences;learning (artificial intelligence);object tracking;parameter estimation;statistical distributions","probabilistic gaze estimation;personal calibration;eye gaze tracking systems;human computer interaction;probability distributions;3D eye model;incremental learning;parameter estimation;image sequences","","52","6","21","","22 Aug 2011","","","IEEE","IEEE Conferences"
"A general method for the point of regard estimation in 3D space","F. Pirri; M. Pizzoli; A. Rudi","Dipartimento di Informatica e Sistemistica, Sapienza Università di Roma, via Ariosto 25, 00185 Rome, Italy; Dipartimento di Informatica e Sistemistica, Sapienza Università di Roma, via Ariosto 25, 00185 Rome, Italy; Dipartimento di Informatica e Sistemistica, Sapienza Università di Roma, via Ariosto 25, 00185 Rome, Italy","CVPR 2011","22 Aug 2011","2011","","","921","928","A novel approach to 3D gaze estimation for wearable multi-camera devices is proposed and its effectiveness is demonstrated both theoretically and empirically. The proposed approach, firmly grounded on the geometry of the multiple views, introduces a calibration procedure that is efficient, accurate, highly innovative but also practical and easy. Thus, it can run online with little intervention from the user. The overall gaze estimation model is general, as no particular complex model of the human eye is assumed in this work. This is made possible by a novel approach, that can be sketched as follows: each eye is imaged by a camera; two conics are fitted to the imaged pupils and a calibration sequence, consisting in the subject gazing a known 3D point, while moving his/her head, provides information to 1) estimate the optical axis in 3D world; 2) compute the geometry of the multi-camera system; 3) estimate the Point of Regard in 3D world. The resultant model is being used effectively to study visual attention by means of gaze estimation experiments, involving people performing natural tasks in wide-field, unstructured scenarios.","1063-6919","978-1-4577-0395-9","10.1109/CVPR.2011.5995634","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5995634","","Estimation;Cameras;Three dimensional displays;Manifolds;Calibration;Visualization;Equations","calibration;cameras;eye","point of regard estimation;3D space;3D gaze estimation;wearable multicamera device;geometry;calibration sequence;optical axis estimation;visual attention;pupils;human eye","","20","4","25","","22 Aug 2011","","","IEEE","IEEE Conferences"
"Gaze directed camera control for face image acquisition","E. Sommerlade; B. Benfold; I. Reid","Active Vision Lab, Department of Engineering Science, University of Oxford, Parks Road, United Kingdom; Active Vision Lab, Department of Engineering Science, University of Oxford, Parks Road, United Kingdom; Active Vision Lab, Department of Engineering Science, University of Oxford, Parks Road, United Kingdom","2011 IEEE International Conference on Robotics and Automation","18 Aug 2011","2011","","","4227","4233","Face recognition in surveillance situations usually requires high resolution face images to be captured from remote active cameras. Since the recognition accuracy is typically a function of the face direction with frontal faces more likely to lead to reliable recognition we propose a system which optimises the capturing of such images by using coarse gaze estimates from a static camera. By considering the potential information gain from observing each target, our system automatically sets the pan, tilt and zoom values (i.e. the field of view) of multiple cameras observing different tracked targets in order to maximise the likelihood of correct identification. The expected gain in information is influenced by the controllable field of view, and by the false positive and negative rates of the identification process, which are in turn a function of the gaze angle. We validate the approach using a combination of simulated situations and real tracking output to demonstrate superior performance over alternative approaches, notably using no gaze information, or using gaze inferred from direction of travel (i.e. assuming each person is always looking directly ahead).We also show results from a live implementation with a static camera and two pan-tilt-zoom devices, involving real-time tracking, processing and control.","1050-4729","978-1-61284-385-8","10.1109/ICRA.2011.5979585","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5979585","","Cameras;Mutual information;Face;Uncertainty;Target tracking;Mathematical model;Delay","active vision;cameras;estimation theory;face recognition;image resolution;real-time systems;surveillance;target tracking","gaze directed camera control;face image acquisition;face recognition;surveillance situations;high resolution face images;remote active cameras;recognition accuracy;face direction;frontal faces;reliable recognition;coarse gaze estimates;static camera;potential information gain;multiple cameras;tracked targets;correct identification;controllable field of view;false positive rates;false negative rates;identification process;gaze angle;real tracking output;no gaze information;pan-tilt-zoom devices;real-time tracking","","2","2","24","","18 Aug 2011","","","IEEE","IEEE Conferences"
"The Importance of Eye Gaze and Head Pose to Estimating Levels of Attention","S. Asteriadis; K. Karpouzis; S. Kollias","Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece; Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece; Sch. of Electr. & Comput. Eng., Nat. Tech. Univ. of Athens, Athens, Greece","2011 Third International Conference on Games and Virtual Worlds for Serious Applications","28 Jul 2011","2011","","","186","191","This paper explores, from a theoretical and technical point of view, the role of head rotation and eye gaze directionality to the human perception of attention from non-verbal cues. We have annotated two different versions of the same dataset, in order to correlate the above parameters with the degree people have considered people in the dataset pay attention to a hypothetical task they have in front of them. Based on our findings, we investigate the role of eye gaze directionality in relation to head rotations and, based on previous studies, we developed an algorithm for estimating attention levels from head pose, eye gaze and facial feature spatial locations. With the help of our AI system and people's annotation, we have made a first attempt towards quantifying the role of each cue to the overall estimation of attention. One of the important properties of the technical part of this work is that all systems we used were non-intrusive and did not demand any personal training or calibration phase, constituting themselves ideal for Game playing. Knowing the behavioral state of a player can be of vital importance for adapting the game design during interaction, or building personal profiles, leading to appropriate game features aiming at maximizing player satisfaction.","","978-1-4577-0316-4","10.1109/VS-GAMES.2011.38","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5962089","Head pose estimation;eye gaze estimation;user attention estimation;user modelling","Magnetic heads;Estimation;Cameras;Games;Humans;Face","artificial intelligence;computer games;pose estimation;user centred design","head pose;attention level estimation;head rotation;eye gaze directionality;nonverbal cues;facial feature spatial locations;AI system;game playing;game design;player satisfaction","","10","1","36","","28 Jul 2011","","","IEEE","IEEE Conferences"
"Eye-based HCI with Full Specification of Mouse and Keyboard Using Pupil Knowledge in the Gaze Estimation","K. Arai; R. Mardiyanto","Saga Univ., Saga, Japan; Saga Univ., Saga, Japan","2011 Eighth International Conference on Information Technology: New Generations","11 Jul 2011","2011","","","423","428","Eye-based Human Computer Interaction (HCI) with full specification of mouse and keyboard by human eyes only is proposed for, in particular, disabled person and for input device of wearable computing. It utilizes pupil knowledge in gaze estimation process. Existing conventional eye-mouse, gaze-mouse does not robust against various types of users with different features of color, shape and size of eyes and also is affected by illumination changes, users' movement (attitude changes), etc. Using knowledge about features, the influences are eliminated. Also the proposed eye-based HCI system allows simultaneous key input of more than three keys, for instance, ""Ctl+Alt+Del"" for initiating task manager, left/right click and drag/drop of mouse event capabilities. Although currently commercially available screen keyboard allows simultaneous key input for two keys such as ""shit+*"", ""Alt+*"", it does not allow three keys input at once. Such these full specification of mouse events and keyboard functions are available for the proposed HCI system. Experimental results with six different nationalities of users with the different features shows effectiveness of using the knowledge for improvement of gaze estimation accuracy.","","978-1-61284-427-5","10.1109/ITNG.2011.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945273","human computer interaction;gaze estimation;eye-mouse;keyboard operation;mouse event","Estimation;Mice;Keyboards;Cameras;Shape;Mathematical model;Lighting","human computer interaction;keyboards;mouse controllers (computers)","eye-based HCI;human computer interaction;pupil knowledge;gaze estimation process;mouse specification;keyboard specification;wearable computing","","5","","29","","11 Jul 2011","","","IEEE","IEEE Conferences"
"Comparative Study on Blink Detection and Gaze Estimation Methods for HCI, in Particular, Gabor Filter Utilized Blink Detection Method","K. Arai; R. Mardiyanto","Saga Univ., Saga, Japan; Saga Univ., Saga, Japan","2011 Eighth International Conference on Information Technology: New Generations","11 Jul 2011","2011","","","441","446","Blink detection is used for a variety of applications such as Human-Computer Interaction, wearable computing, etc. Blink detection method with Gabor filter is proposed to realize high accurate blink detection. The blink detection accuracy is evaluated by several people and compare to the other existing conventional methods. Through the comparison, it is found that the proposed blink detection method with Gabor filter is superior to the other methods.","","978-1-61284-427-5","10.1109/ITNG.2011.84","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945276","blink detection;Gabor filter;HCI;wearable computing;gaze detection","Gabor filters;Accuracy;Real time systems;Eyelids;Iris;Head;Robustness","Gabor filters;human computer interaction;iris recognition;wearable computers","gaze estimation method;HCI;human-computer interaction;wearable computing;Gabor filter;blink detection accuracy","","5","1","24","","11 Jul 2011","","","IEEE","IEEE Conferences"
"Gaze tracking based on run length coding","S. V. Sheela; P. A. Vijaya","B M S College of Engineering, Bangalore, India; Malnad College of Engineering, Hassan, India","2011 International Conference on Multimedia Computing and Systems","11 Jul 2011","2011","","","1","6","Eye gaze determines the view point, focus of attention and interest of an individual. Gaze estimation systems compute the direction of eye gaze. These systems play an important role in human-computer interaction. The gaze pointing systems is a replacement for existing input devices in gaze communication applications. The gaze tracking methods are based on interpolation, 2D geometric transform mapping, edge change ratio in successive frames and singular value decomposition. This paper discusses a robust, less computational gaze tracking method using run length coding. 94.5% recognition rate of gaze direction is achieved using run length coding.","","978-1-61284-732-0","10.1109/ICMCS.2011.5945682","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5945682","Similarity score;Run length coding;Region growing;Gaze direction","Pixel;Iris;Iris recognition;Shape;Head;Estimation;Encoding","estimation theory;face recognition;geometry;human computer interaction;image coding;singular value decomposition;transforms","gaze tracking;gaze estimation system;eye gaze direction;human-computer interaction;gaze pointing systems;gaze communication applications;2D geometric transform mapping;singular value decomposition;computational gaze tracking method;run length coding","","","4","11","","11 Jul 2011","","","IEEE","IEEE Conferences"
"Low cost human computer interface voluntary eye movement as communication system for disabled people with limited movements","L. J. G. Vazquez; M. A. Minor; A. J. H. Sossa","CINVESTAV-IPN, Mexico D.F., Mexico; CINVESTAV-IPN, Mexico D.F., Mexico; CIC-IPN, Mexico D.F., Mexico","2011 Pan American Health Care Exchanges","9 Jun 2011","2011","","","165","170","The ability to identify the location of human pupil offers us the possibility to know the eye gaze target of a person. This infers the possibility to create new methods of human-machine interaction. Nevertheless there are some barriers and restrictions that are shown in the integrations, as intrusion, robustness, availability and price of the eye track systems. This paper describes the develop and usability of an eye gaze input system using a web cam with infrared LEDs over a head mounting system. Most available eye gazing with head mounting have two characteristic that hinder being widely used as human computer interfaces. The first is represented by the need of a calibration stage before the use; the second is about the low tolerance for head movements during all the using time. This work solved that problem using a 3 axis accelerometer as head tilt sensor to make a compensation. Our main objective is to create a communication system for people with motor disabilities based exclusively in self-inflicted ocular movement.","2327-817X","978-1-61284-918-8","10.1109/PAHCE.2011.5871871","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5871871","eye gaze tracking;gaze estimation;head tilt sensor;human-computer interface","Head;Computers;Calibration;Accelerometers;Accuracy;Graphical user interfaces;Medical services","accelerometers;biocommunications;eye;handicapped aids;human computer interaction;light emitting diodes","low cost human computer interface voluntary eye movement;communication system;disabled people;limited movements;human pupil;eye gaze input system;web cam;infrared LED;head mounting system;3 axis accelerometer;head tilt sensor;motor disability;self-inflicted ocular movement","","3","","15","","9 Jun 2011","","","IEEE","IEEE Conferences"
"Constraint-based gaze estimation without active calibration","W. Maio; J. Chen; Q. Ji","Dept. Of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY; Dept. Of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY; Dept. Of Electrical, Computer, and Systems Engineering, Rensselaer Polytechnic Institute, Troy, NY","2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)","19 May 2011","2011","","","627","631","Existing eye gaze tracking systems typically require an explicit personal calibration process in order to estimate certain person-specific eye parameters. For natural human computer interaction, such a personal calibration is often cumbersome and unnatural. In this paper, we introduce a new method that estimates a person's gaze without active personal calibration. By exploiting the binocular constraint that the gaze point is produced by the intersection of visual axes of two eyes and some generic person-independent constraints on eye parameters, our method is able to estimate the required person-specific parameters implicitly and naturally without active participation from the user and without use of any special calibration object. Experiments with different subjects show that the proposed method achieves good gaze estimation comparable to the conventional 9 point method.","","978-1-4244-9141-4","10.1109/FG.2011.5771469","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5771469","","Calibration;Estimation;Visualization;Optimization;Three dimensional displays;Cameras;Head","eye;human computer interaction;optical tracking","constraint based gaze estimation;eye gaze tracking system;person specific eye parameter;natural human computer interaction;binocular constraint;person specific parameter","","1","","19","","19 May 2011","","","IEEE","IEEE Conferences"
"A Multi-Gesture Interaction System Using a 3-D Iris Disk Model for Gaze Estimation and an Active Appearance Model for 3-D Hand Pointing","M. J. Reale; S. Canavan; L. Yin; K. Hu; T. Hung","Department of Computer Science, State University of New York at Binghamton, Binghamton; Department of Computer Science, State University of New York at Binghamton, Binghamton; Department of Computer Science, State University of New York at Binghamton, Binghamton; Department of Computer Science, State University of New York at Binghamton, Binghamton; Corning Corp., Taiwan","IEEE Transactions on Multimedia","16 May 2011","2011","13","3","474","486","In this paper, we present a vision-based human-computer interaction system, which integrates control components using multiple gestures, including eye gaze, head pose, hand pointing, and mouth motions. To track head, eye, and mouth movements, we present a two-camera system that detects the face from a fixed, wide-angle camera, estimates a rough location for the eye region using an eye detector based on topographic features, and directs another active pan-tilt-zoom camera to focus in on this eye region. We also propose a novel eye gaze estimation approach for point-of-regard (POR) tracking on a viewing screen. To allow for greater head pose freedom, we developed a new calibration approach to find the 3-D eyeball location, eyeball radius, and fovea position. Moreover, in order to get the optical axis, we create a 3-D iris disk by mapping both the iris center and iris contour points to the eyeball sphere. We then rotate the fovea accordingly and compute the final, visual axis gaze direction. This part of the system permits natural, non-intrusive, pose-invariant POR estimation from a distance without resorting to infrared or complex hardware setups. We also propose and integrate a two-camera hand pointing estimation algorithm for hand gesture tracking in 3-D from a distance. The algorithms of gaze pointing and hand finger pointing are evaluated individually, and the feasibility of the entire system is validated through two interactive information visualization applications.","1941-0077","","10.1109/TMM.2011.2120600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720546","Gaze estimation;hand tracking;human–computer interaction (HCI)","Iris;Three dimensional displays;Cameras;Estimation;Calibration;Face;Adaptive optics","cameras;data visualisation;gesture recognition;human computer interaction;iris recognition;solid modelling","multigesture interaction system;3D iris disk model;gaze estimation;active appearance model;3D hand pointing;vision-based human-computer interaction system;eye gaze gesture;head pose gesture;hand pointing gesture;mouth motions gesture;two-camera system;eye detector;pan-tilt-zoom camera;point-of-regard tracking;3D eyeball location;eyeball radius;fovea position;iris center;iris contour points;information visualization","","54","4","49","IEEE","28 Feb 2011","","","IEEE","IEEE Journals"
"Real-Time Gaze Estimator Based on Driver's Head Orientation for Forward Collision Warning System","S. J. Lee; J. Jo; H. G. Jung; K. R. Park; J. Kim","School of Electrical and Electronic Engineering, Biometrics Engineering Research Center, Yonsei University, Seoul, Korea; School of Electrical and Electronic Engineering, Biometrics Engineering Research Center, Yonsei University, Seoul, Korea; Mando Corporation, Yongin, Korea; Division of Electronics and Electrical Engineering and with the Biometrics Engineering Research Center, Dongguk University, Seoul, Korea; School of Electrical and Electronic Engineering, Biometrics Engineering Research Center, Yonsei University, Seoul, Korea","IEEE Transactions on Intelligent Transportation Systems","24 Feb 2011","2011","12","1","254","267","This paper presents a vision-based real-time gaze zone estimator based on a driver's head orientation composed of yaw and pitch. Generally, vision-based methods are vulnerable to the wearing of eyeglasses and image variations between day and night. The proposed method is novel in the following four ways: First, the proposed method can work under both day and night conditions and is robust to facial image variation caused by eyeglasses because it only requires simple facial features and not specific features such as eyes, lip corners, and facial contours. Second, an ellipsoidal face model is proposed instead of a cylindrical face model to exactly determine a driver's yaw. Third, we propose new features-the normalized mean and the standard deviation of the horizontal edge projection histogram-to reliably and rapidly estimate a driver's pitch. Fourth, the proposed method obtains an accurate gaze zone by using a support vector machine. Experimental results from 200 000 images showed that the root mean square errors of the estimated yaw and pitch angles are below 7 under both daylight and nighttime conditions. Equivalent results were obtained for drivers with glasses or sunglasses, and 18 gaze zones were accurately estimated using the proposed gaze estimation method.","1558-0016","","10.1109/TITS.2010.2091503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5688323","Driver monitoring system;forward collision warning (FCW) system;gaze estimation;head orientation estimation;precrash system","Driver circuits;Face;Feature extraction;Estimation;Shape;Magnetic heads","alarm systems;edge detection;eye protection;face recognition;feature extraction;mean square error methods;real-time systems;support vector machines;traffic engineering computing","real time gaze estimation;driver head orientation;forward collision warning system;facial image variation;eyeglass;ellipsoidal face model;edge projection histogram;support vector machine;root mean square error;yaw and pitch angle","","69","2","39","IEEE","17 Jan 2011","","","IEEE","IEEE Journals"
"A Wide-View Parallax-Free Eye-Mark Recorder with a Hyperboloidal Half-Silvered Mirror and Appearance-Based Gaze Estimation","H. Mori; E. Sumiya; T. Mashita; K. Kiyokawa; H. Takemura","Osaka University, Toyonaka; Osaka University, Toyonaka; Osaka University, Toyonaka; Osaka University, Toyonaka; Osaka University, Toyonaka","IEEE Transactions on Visualization and Computer Graphics","5 May 2011","2011","17","7","900","912","In this paper, we propose a wide-view parallax-free eye-mark recorder with a hyperboloidal half-silvered mirror and a gaze estimation method suitable for the device. Our eye-mark recorder provides a wide field-of-view video recording of the user's exact view by positioning the focal point of the mirror at the user's viewpoint. The vertical angle of view of the prototype is 122 degree (elevation and depression angles are 38 and 84 degree, respectively) and its horizontal view angle is 116 degree (nasal and temporal view angles are 38 and 78 degree, respectively). We implemented and evaluated a gaze estimation method for our eye-mark recorder. We use an appearance-based approach for our eye-mark recorder to support a wide field-of-view. We apply principal component analysis (PCA) and multiple regression analysis (MRA) to determine the relationship between the captured images and their corresponding gaze points. Experimental results verify that our eye-mark recorder successfully captures a wide field-of-view of a user and estimates gaze direction with an angular accuracy of around 2 to 4 degree.","1941-0506","","10.1109/TVCG.2010.113","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5557872","Eye-mark recorder;parallax free;wide field-of-view;gaze estimation;half-silvered hyperboloidal mirror;head-mounted camera.","Mirrors;Cameras;Lenses;Erbium;Estimation;Optical imaging;Strontium","image processing;mirrors;principal component analysis;recorders;regression analysis","hyperboloidal half-silvered mirror;appearance-based gaze estimation;parallax-free eye-mark recorder;video recording;principal component analysis;multiple regression analysis","Equipment Design;Eye Movements;Fixation, Ocular;Humans;Image Processing, Computer-Assisted;Optics and Photonics;Video Recording","5","2","22","","26 Aug 2010","","","IEEE","IEEE Journals"
"Driver Distraction Detection and Identity Recognition in Real-Time","J. Zeng; Y. Sun; L. Jiang","Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China; Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China; Dept. of Comput. Sci. & Technol., Tongji Univ., Shanghai, China","2010 Second WRI Global Congress on Intelligent Systems","4 Feb 2011","2010","3","","43","46","Drivers attending to primary driving tasks show specific eye and head movement behaviours, while the distracted drive generally covers the states including drivers' eyes off the road and long-term eye closure. This paper presents a distraction detection system by using the strategy of ``attention budget''. The states of eyes off the road and face with closed eyes are used to lessen the ``attention budget'' while the reversed conditions gain it. Drivers' gaze estimation is derived from the head motion, and the stage classifiers working with haar-like features are used to detect head movements and eye states. With regard to the factors of drivers' personal characteristics in distraction detection, the recognition of drivers is implemented by extraction and matching of scale invariant feature transform features in detected frontal face. The results of experiments validate the effectiveness and robustness of the system.","2155-6091","978-1-4244-9247-3","10.1109/GCIS.2010.83","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5709318","driver distraction detection;identity recognition;attention budget;scale invariant feature transform","Driver circuits;Face;Feature extraction;Databases;Lighting;Roads","eye;face recognition;feature extraction;Haar transforms;image matching","driver distraction detection;identity recognition;eye movement behaviours;head movement behaviours;long-term eye closure;gaze estimation;haar-like features;personal characteristics;feature extraction;feature matching;frontal face detection","","11","","16","","4 Feb 2011","","","IEEE","IEEE Conferences"
"Point-of-Regard Measurement via Iris Contour with One Eye from Single Image","S. Huang; Y. Wu; W. Hung; C. Tang","Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan; Dept. of Comput. Sci. & Inf. Eng., Nat. Taiwan Univ. of Sci. & Technol., Taipei, Taiwan; Dept. of Inf. Manage., Huafan Univ., Taipei, Taiwan","2010 IEEE International Symposium on Multimedia","20 Jan 2011","2010","","","336","341","Eye gaze tracking is a technique which is commonly used in human-computer interaction. By determining the eye gaze, the point-of-regard can be estimated by intersecting the gaze line and the target plane. A particular assumption is that irises are regarded as ellipses rather than circles in our approach. Besides, only one eye is required in the processes of the approach. The unique eye gaze can be computed via some image processing techniques such as edge detection and ellipse fitting and exploiting geometric properties of circles and ellipses in the 3D space. One of the key phases in our approach is the ellipse fitting process due to the noise of the image. Conventional algorithms of ellipse fitting are sensitive to noise. In this paper we have improved the randomized Hough transform to reduce the effect of noise. It is also confirmed to be robust by experimenting with real images. The estimation results of point-of-regard are illustrated and four distinct clusters can be separated robustly with both the influences of resolution and eyelids.","","978-1-4244-8672-4","10.1109/ISM.2010.68","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5693863","gaze estimattion;point-of-regard measurement;iris contour fitting;3D locations of circle/ellipse;human-computer interaction","Iris;Image resolution;Image edge detection;Eyelids;Cameras;Fitting;Accuracy","eye;Hough transforms;human computer interaction;image denoising;image resolution;iris recognition","point-of-regard measurement;iris contour;eye gaze tracking;human-computer interaction;image processing techniques;geometric properties;ellipse fitting process;image noise;randomized Hough transform;eyelids;image resolution;target plane","","3","","15","","20 Jan 2011","","","IEEE","IEEE Conferences"
"Directional Binary Pattern (DBP): A novel object representation approach for gaze estimation","H. Ge; X. Chen","School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China","2010 3rd International Congress on Image and Signal Processing","29 Nov 2010","2010","2","","912","915","This paper presents a novel object descriptor, Directional Binary Pattern (DBP), for robust gaze estimation. In DBP, the local directional derivations are proposed to encode the binary patterns in the given orientations. As an object descriptor, DBP has many advantages: noise restrain, robustness to illumination, contrast enhancement on the boundary, and local texture encoded as the directional information. DBP features of eyes are finally fed into Support Vector Regression (SVR) to match the gaze mapping function, which is then used to predict the gaze direction with respect to the camera coordinate system. In our experiments, an eye gaze dataset includes 4089 training samples of 11 persons. Experimental results show that our method can achieve an accuracy of less than 2°.","","978-1-4244-6516-3","10.1109/CISP.2010.5646899","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5646899","object descriptor;Directional Binary Pattern (DBP);local directional derivations;gaze estimation","Robustness;Pixel;Estimation;Cameras;Receivers;Histograms;Head","computer vision;pattern recognition;support vector machines;visual perception","directional binary pattern;gaze estimation;contrast enhancement;support vector regression;SVR;gaze mapping function","","","","9","","29 Nov 2010","","","IEEE","IEEE Conferences"
"Gazing estimation and correction from elliptical features of one iris","W. Zhang; T. Zhang; S. Chang","Institute of Modern Optics Nankai University Tianjin, P.R. China; Institute of Modern Optics Nankai University Tianjin, P.R. China; Institute of Modern Optics Nankai University Tianjin, P.R. China","2010 3rd International Congress on Image and Signal Processing","29 Nov 2010","2010","4","","1647","1652","The accuracy of eye gaze estimation by image information is affected by several objective factors, including the image resolution, anatomical structure of eye, posture change, etc. Especially, the irregular movements of head and eye are the main problem and key technology being researched. We describe an effective way of estimating the eye gazing from the elliptical features of one iris under the conditions without auxiliary source, head fixing equipment or multiple-camera. Firstly, we give the preliminary estimations of the gazing direction and then obtain the vectors describing translation and rotation of eyeball movement using central projection on the cross section passing through the line-of-sight, which avoids the complex computations involved in known methods. We also disambiguate the solution on the basis of the experimental findings. Secondly, the error correction is carried on the BP neural network trained by a sample collection of the translation and rotation vectors. In our simulations, we achieve an accuracy of 0.8° on the test images which are different from the training images. The result is found to be better than that of the existing non-intrusive method with single-camera. The performance of the algorithm proves that the proposed method has excellent generalized abilities.","","978-1-4244-6516-3","10.1109/CISP.2010.5647733","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5647733","eye gazing;central projection;translation estimation;neural network;iris contour;ellipse fitting","Iris;Estimation;Image edge detection;Cameras;Artificial neural networks;Training;Fitting","backpropagation;estimation theory;iris recognition;neural nets;vectors","eye gazing estimation;gazing correction;elliptical features;iris;gazing direction;vectors;eyeball movement translation;eyeball movement rotation;BP neural network","","","","24","","29 Nov 2010","","","IEEE","IEEE Conferences"
"Gaze Probing: Event-Based Estimation of Objects Being Focused On","R. Yonetani; H. Kawashima; T. Hirayama; T. Matsuyama","Dept. of Intell. Sci. & Technol., Kyoto Univ., Kyoto, Japan; Dept. of Intell. Sci. & Technol., Kyoto Univ., Kyoto, Japan; Dept. of Intell. Sci. & Technol., Kyoto Univ., Kyoto, Japan; Dept. of Intell. Sci. & Technol., Kyoto Univ., Kyoto, Japan","2010 20th International Conference on Pattern Recognition","7 Oct 2010","2010","","","101","104","We propose a novel method to estimate the object that a user is focusing on by using the synchronization between the movements of objects and a user's eyes as a cue. We first design an event as a characteristic motion pattern, and we then embed it within the movement of each object. Since the user's ocular reactions to these events are easily detected using a passive camera-based eye tracker, we can successfully estimate the object that the user is focusing on as the one whose movement is most synchronized with the user's eye reaction. Experimental results obtained from the application of this system to dynamic content (consisting of scrolling images) demonstrate the effectiveness of the proposed method over existing methods.","1051-4651","978-1-4244-7541-4","10.1109/ICPR.2010.33","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597638","event-based gaze estimation;synchronization;dynamic content;eye movement","Tracking;Estimation;Accuracy;Head;Synchronization;Cameras;Image edge detection","eye;motion estimation;object recognition;synchronisation","gaze probing;event-based object estimation;synchronization;objects movements;user's eyes;characteristic motion pattern;user's ocular reactions;passive camera-based eye tracker","","3","","9","","7 Oct 2010","","","IEEE","IEEE Conferences"
"Visual Gaze Estimation by Joint Head and Eye Information","R. Valenti; A. Lablack; N. Sebe; C. Djeraba; T. Gevers","Intell. Syst. Lab. Amsterdam, Univ. of Amsterdam, Amsterdam, Netherlands; Lab. d'Inf. Fondamentale de Lille, Univ. of Lille, Lille, France; Dept. of Inf. Eng. & Comput. Sci., Univ. of Trento, Trento, Italy; Lab. d'Inf. Fondamentale de Lille, Univ. of Lille, Lille, France; Intell. Syst. Lab. Amsterdam, Univ. of Amsterdam, Amsterdam, Netherlands","2010 20th International Conference on Pattern Recognition","7 Oct 2010","2010","","","3870","3873","In this paper, we present an unconstrained visual gaze estimation system. The proposed method extracts the visual field of view of a person looking at a target scene in order to estimate the approximate location of interest (visual gaze). The novelty of the system is the joint use of head pose and eye location information to fine tune the visual gaze estimated by the head pose only, so that the system can be used in multiple scenarios. The improvements obtained by the proposed approach are validated using the Boston University head pose dataset, on which the standard deviation of the joint visual gaze estimation improved by 61:06% horizontally and 52:23% vertically with respect to the gaze estimation obtained by the head pose only. A user study shows the potential of the proposed system.","1051-4651","978-1-4244-7541-4","10.1109/ICPR.2010.1160","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5597628","gaze estimation;head pose estimation;eye localization","Visualization;Estimation;Magnetic heads;Head;Joints;Monitoring;Humans","pose estimation;visual perception","unconstrained visual gaze estimation system;interest approximate location;eye location information;Boston university head pose dataset;head pose information","","7","","21","","7 Oct 2010","","","IEEE","IEEE Conferences"
"A Novel Simple 2D Model of Eye Gaze Estimation","G. Shao; M. Che; B. Zhang; K. Cen; W. Gao","Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China; Sch. of Comput. Sci. & Technol., Tianjin Univ., Tianjin, China","2010 Second International Conference on Intelligent Human-Machine Systems and Cybernetics","30 Sep 2010","2010","1","","300","304","With the development of image process technology, camera-based eye gaze estimation methods make a possible nonintrusive way of human computer interaction (HCI). Most of them base on Pupil-Cornea Reflection Technique (PCRT) which uses extra light sources and estimates gaze point on screen by a polynomial mapping function. In this paper, the input vector of classical PCRT is adjusted to a new one which can be precisely extracted in the system using only one camera. The adjusted PCRT is called Pupil-Corner Technique (PCT). Meanwhile, a novel simple 2D Geometric Model (GM) is proposed. It bases on the geometric relationship of eyeballs and screen and simulates the movement of eyeballs when user looks through the screen with his head still. Furthermore, a fitting method is used to promote the accuracy of GM. At last, the combined technique called Geometric Fitting Technique (GFT) is compared with PCT in experiments and the results show that GFT has an acceptable accuracy which can be applied in HCI applications and future geometric model is suggested by the guide concluded from the paper to promote accuracy.","","978-1-4244-7869-9","10.1109/IHMSC.2010.81","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5590853","eye gaze estimation;eye gaze tracking;eye gaze model;mapping function;human computer interaction","Calibration;Accuracy;Estimation;Face;Fitting;Cameras","cameras;eye;feature extraction;human computer interaction;image processing;light sources;polynomials;reflection","image process technology;camera based eye gaze estimation method;human computer interaction;pupil-cornea reflection technique;extra light source;polynomial mapping function;2D geometric model;geometric fitting technique","","12","","15","","30 Sep 2010","","","IEEE","IEEE Conferences"
"Pointing with the eyes: Gaze estimation using a static/active camera system and 3D iris disk model","M. Reale; T. Hung; L. Yin","Department of Computer Science, State University of New York at Binghamton; Department of Computer Science, State University of New York at Binghamton; Department of Computer Science, State University of New York at Binghamton","2010 IEEE International Conference on Multimedia and Expo","23 Sep 2010","2010","","","280","285","The ability to capture the direction the eyes point in while the subject is a distance away from the camera offers the potential for intuitive human-computer interfaces, allowing for a greater interactivity, more intelligent HCI behavior, and increased flexibility. In this paper, we present a two-camera system that detects the face from a fixed, wide-angle camera, estimates a rough location for the eye region using an eye detector based on topographic features, and directs another active pan-tilt-zoom camera to focus in on this eye region. We also propose a novel eye gaze estimation approach for point-of-regard (PoG) tracking on a large viewing screen. To allow for greater head pose freedom, we developed a new calibration approach to find the 3D eyeball location, eyeball radius, and fovea position. Moreover, we map both the iris center and iris contour points to the eyeball sphere (creating a 3D iris disk) to get the optical axis; we then rotate the fovea accordingly and compute our final, visual axis gaze direction. We intend to integrate this gaze estimation approach with our two-camera system, permitting natural, non-intrusive, pose-invariant PoG estimation in distance and allowing user translational freedom without resorting to infrared or complex hardware setups such as stereo-cameras or “smart rooms"".","1945-788X","978-1-4244-7493-6","10.1109/ICME.2010.5583014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5583014","Eye tracking;eye gaze estimation;static/active dual-camera system","Iris;Cameras;Three dimensional displays;Calibration;Estimation;Face;Pixel","cameras;face recognition;human computer interaction;iris recognition","static camera system;active camera system;3D iris disk model;face detection;eye detector;topographic features;pan-tilt-zoom camera;eye gaze estimation approach;point-of-regard tracking;3D eyeball location;eyeball radius;fovea position;eye point direction;human-computer interface","","10","1","22","","23 Sep 2010","","","IEEE","IEEE Conferences"
"Viewing direction estimation based on 3D eyeball construction for HRI","M. Reale; T. Hung; L. Yin","Department of Computer Science, State University of New York at Binghamton, USA; Department of Computer Science, State University of New York at Binghamton, USA; Department of Computer Science, State University of New York at Binghamton, USA","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops","9 Aug 2010","2010","","","24","31","Natural human-robot interaction requires leveraging viewing direction information in order to recognize, respond to, and even emulate human behavior. Knowledge of the eye gaze and point of regard gives us insight into what the subject is interested in and/or who the subject is addressing. In this paper, we present a novel eye gaze estimation approach for point-of-regard (PoG) tracking. To allow for greater head pose freedom, we introduce a new calibration approach to find the 3D eyeball location, eyeball radius, and fovea position. To estimate gaze direction, we map both the iris center and iris contour points to the eyeball sphere (creating a 3D iris disk), giving us the optical axis. We then rotate the fovea accordingly and compute our final, visual axis gaze direction. Our intention is to integrate this eye gaze approach with a dual-camera system we have developed that detects the face and eyes from a fixed, wide-angle camera and directs another active pan-tilt-zoom camera to focus in on this eye region. The final system will permit natural, non-intrusive, pose-invariant PoG estimation in distance and allow user translational freedom without resorting to infrared equipment or complex hardware setups.","2160-7516","978-1-4244-7030-3","10.1109/CVPRW.2010.5543784","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5543784","Eye tracking;eye gaze estimation;iris modeling","Iris;Eyes;Humans;Calibration;Face detection;Cameras;Hardware;Mobile robots;Educational robots;Retina","calibration;cameras;eye;face recognition;human-robot interaction;iris recognition;pose estimation;tracking;vision","viewing direction estimation;3D eyeball construction;HRI;human-robot interaction;human behavior;eye gaze;point-of-regard tracking;head pose freedom;calibration approach;eyeball radius;fovea position;iris center;iris contour points;eyeball sphere;optical axis;visual axis gaze direction;dual-camera system;face detection;eye detection;pan-tilt-zoom camera;eye region;infrared equipment;complex hardware setups","","10","7","24","","9 Aug 2010","","","IEEE","IEEE Conferences"
"Attention estimation by simultaneous observation of viewer and view","A. Doshi; M. M. Trivedi","Computer Vision and Robotics Research Lab, University of California, San Diego, La Jolla, 92093-0434, USA; Computer Vision and Robotics Research Lab, University of California, San Diego, La Jolla, 92093-0434, USA","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition - Workshops","9 Aug 2010","2010","","","21","27","We introduce a new approach to analyzing the attentive state of a human subject, given cameras focused on the subject and their environment. In particular, the task of analyzing the focus of attention of a human driver is of primary concern. Up to 80% of automobile crashes are related to driver inattention; thus it is important for an Intelligent Driver Assistance System (IDAS) to be aware of the driver state. We present a new Bayesian paradigm for estimating human attention specifically addressing the problems arising in dynamic situations. The model incorporates vision-based gaze estimation, “top-down”- and “bottom-up”-based visual saliency maps, and cognitive considerations such as inhibition of return and center bias that affect the relationship between gaze and attention. Results demonstrate the validity on real driving data, showing quantitative improvements over systems using only gaze or only saliency, and elucidate the value of such a model for any human-machine interface.","2160-7516","978-1-4244-7030-3","10.1109/CVPRW.2010.5543272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5543272","","Humans;Layout;State estimation;Bayesian methods;Vehicle dynamics;Robot vision systems;Man machine systems;Probability distribution;Computational modeling;Observers","Bayes methods;cameras;computer vision;traffic engineering computing;user interfaces","attention estimation;simultaneous observation;cameras;human driver;intelligent driver assistance system;Bayesian paradigm;vision based gaze estimation;visual saliency maps;human machine interface","","29","","20","","9 Aug 2010","","","IEEE","IEEE Conferences"
"Calibration-free gaze sensing using saliency maps","Y. Sugano; Y. Matsushita; Y. Sato","The University of Tokyo Tokyo, Japan, 153-8505; Microsoft Research Asia, Beijing, P. R. China, 100080; The University of Tokyo Tokyo, Japan, 153-8505","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","5 Aug 2010","2010","","","2667","2674","We propose a calibration-free gaze sensing method using visual saliency maps. Our goal is to construct a gaze estimator only using eye images captured from a person watching a video clip. The key is treating saliency maps of the video frames as probability distributions of gaze points. To efficiently identify gaze points from saliency maps, we aggregate saliency maps based on the similarity of eye appearances. We establish mapping between eye images to gaze points by Gaussian process regression. The experimental result shows that the proposed method works well with different people and video clips and achieves 6 degrees of accuracy, which is useful for estimating a person's attention on monitors.","1063-6919","978-1-4244-6985-7","10.1109/CVPR.2010.5539984","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5539984","","Calibration;Eyes;Computational modeling;Humans;Image analysis;Asia;Probability distribution;Aggregates;Gaussian processes;Interactive systems","eye;feature extraction;Gaussian processes;image sensors;motion estimation;neurophysiology","calibration-free gaze sensing;visual saliency maps;gaze estimator;image capturing;video clip;video frames;probability distributions;gaze points;eye images","","47","4","19","","5 Aug 2010","","","IEEE","IEEE Conferences"
"Reconstruction of display and eyes from a single image","D. Schnieders; X. Fu; K. K. Wong","Department of Computer Science, University of Hong Kong; Department of Computer Science, University of Hong Kong; Department of Computer Science, University of Hong Kong","2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","5 Aug 2010","2010","","","1442","1449","This paper introduces a novel method for reconstructing human eyes and visual display from reflections on the cornea. This problem is difficult because the camera is not directly facing the display, but instead captures the eyes of a person in front of the display. Reconstruction of eyes and display is useful for point-of-gaze estimation, which can be approximated from the 3D positions of the iris and display. It is shown that iris boundaries (limbus) and display reflections in a single intrinsically calibrated image provide enough information for such an estimation. The proposed method assumes a simplified geometric eyeball model with certain anatomical constants which are used to reconstruct the eye. A noise performance analysis shows the sensitivity of the proposed method to imperfect data. Experiments on various subjects show that it is possible to determine the approximate area of gaze on a display from a single image.","1063-6919","978-1-4244-6985-7","10.1109/CVPR.2010.5539799","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5539799","","Image reconstruction;Eyes;Three dimensional displays;Iris;Humans;Cornea;Cameras;Acoustic reflection;Solid modeling;Performance analysis","eye;image reconstruction","display reconstruction;human eye reconstruction;cornea;point-of-gaze estimation;iris boundaries;calibrated image;geometric eyeball model","","16","1","23","","5 Aug 2010","","","IEEE","IEEE Conferences"
"An Automatic Personal Calibration Procedure for Advanced Gaze Estimation Systems","D. Model; M. Eizenman","Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; Department of Electrical and Computer Engineering, the Department of Ophthalmology and Vision Sciences, and the Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, Canada","IEEE Transactions on Biomedical Engineering","15 Apr 2010","2010","57","5","1031","1039","Gaze estimation systems use calibration procedures to estimate subject-specific parameters that are needed for the calculation of the point-of-gaze. In these procedures, subjects are required to fixate on a specific point or points in space at specific time instances. Advanced remote gaze estimation systems can estimate the optical axis of the eye without any personal calibration procedure, but use a single calibration point to estimate the angle between the <i>optical</i> axis and the <i>visual</i> axis (line-of-gaze). This paper presents a novel calibration procedure that does not require active user participation. To estimate the angles between the optical and visual axes of each eye, this procedure minimizes the distance between the intersections of the visual axes of the left and right eyes with one or more observation surfaces (displays) while subjects look naturally at these displays (e.g., watching a video clip). Theoretical analysis and computer simulations show that the performance of the proposed procedure improves when the range of angles between the visual axes and vectors normal to the observation surfaces increases. Experiments with four subjects show that the subject-specific angles between the optical and visual axes can be estimated with an rms error of 0.5°.","1558-2531","","10.1109/TBME.2009.2039351","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5415599","Calibration free;optical axis;personal calibration;point-of-gaze (PoG);remote gaze estimation;visual axis","Calibration;Biomedical optical imaging;Parameter estimation;Computer displays;Humans;Eyes;Performance analysis;Computer simulation;Computer errors","calibration;eye;parameter estimation;vision","automatic personal calibration;advanced gaze estimation system;subject specific parameter estimation;optical axis;eye;visual axis","Algorithms;Calibration;Computer Simulation;Cornea;Diagnostic Techniques, Ophthalmological;Eye Movements;Fixation, Ocular;Humans;Models, Biological","33","1","22","IEEE","17 Feb 2010","","","IEEE","IEEE Journals"
"In the Eye of the Beholder: A Survey of Models for Eyes and Gaze","D. W. Hansen; Q. Ji","IT University, Copenhagen, Copenhagen; Rensselaer Polynechnic Institute, Troy","IEEE Transactions on Pattern Analysis and Machine Intelligence","19 Jan 2010","2010","32","3","478","500","Despite active research and significant progress in the last 30 years, eye detection and tracking remains challenging due to the individuality of eyes, occlusion, variability in scale, location, and light conditions. Data on eye location and details of eye movements have numerous applications and are essential in face detection, biometric identification, and particular human-computer interaction tasks. This paper reviews current progress and state of the art in video-based eye detection and tracking in order to identify promising techniques as well as issues to be further addressed. We present a detailed review of recent eye models and techniques for eye detection and tracking. We also survey methods for gaze estimation and compare them based on their geometric properties and reported accuracies. This review shows that, despite their apparent simplicity, the development of a general eye detection technique involves addressing many challenges, requires further theoretical developments, and is consequently of interest to many other domains problems in computer vision and beyond.","1939-3539","","10.1109/TPAMI.2009.30","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4770110","Eye;eye detection;eye tracking;gaze estimation;review paper;gaze tracking;object detection and tracking;human--computer interaction.","Eyes;Face detection;Humans;Tracking;Object detection;User interfaces;Biometrics;Computer vision;Robustness;Photometry","biometrics (access control);computer vision;estimation theory;eye;face recognition;human computer interaction;video signal processing","eye tracking;occlusion;eye movements;face detection;biometric identification;human-computer interaction;video-based eye detection;eye models;gaze estimation;eye detection technique;computer vision","Eye Movement Measurements;Eye Movements;Fixation, Ocular;Head Movements;Humans;Models, Biological;Regression Analysis","754","60","171","","2 Feb 2009","","","IEEE","IEEE Journals"
"Covert monitoring of the point-of-gaze","M. Eizenman; D. Model; E. D. Guestrin","Dept. of Electrical and Computer Engineering, Dept. of Ophthalmology and Vision Sciences, and Institute of Biomaterials and Biomedical Engineering, University of Toronto, ON, Canada; Dept. of Electrical and Computer Engineering, University of Toronto, ON, Canada; Dept. of Electrical and Computer Engineering and Institute of Biomaterials and Biomedical Engineering, University of Toronto, ON, Canada","2009 IEEE Toronto International Conference Science and Technology for Humanity (TIC-STH)","8 Apr 2010","2009","","","551","556","Gaze estimation systems use calibration procedures that require active subject participation to estimate the point-of-gaze accurately. Consequently, these systems do not support covert monitoring of visual scanning patterns. This paper presents a novel gaze estimation methodology that does not use calibration procedures that require active user participation. This methodology uses multiple infrared light sources for illumination and a stereo pair of video cameras to obtain images of the eyes. Each pair of images is analyzed and the centers of the pupils and the centers of curvature of the corneas are estimated. These points, which are estimated without a personal calibration procedure, define the optical axis of each eye. To estimate the point-of-gaze, which lies along the visual axis, the angle between the optical and visual axes is estimated by a procedure that minimizes the distance between the intersections of the visual axes of the left and right eyes with the surface of a display while subjects look naturally at the display (e.g., watching a video clip). Simulation results demonstrate that for a subject sitting 75 cm in front of an 80 cm × 60 cm display (40"" TV) the RMS error of the estimated point-of-gaze is 17.8 mm (1.3°).","","978-1-4244-3877-8","10.1109/TIC-STH.2009.5444437","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5444437","Covert gaze monitoring;Calibration-free gaze estimation;Remote gaze estimation","Monitoring;Calibration;Displays;Eyes;Infrared imaging;Light sources;Lighting;Cameras;Image analysis;Cornea","calibration;cameras;eye;stereo image processing;video signal processing","gaze estimation systems;covert monitoring;calibration procedures;active subject participation;visual scanning patterns;active user participation;multiple infrared light sources;illumination;video cameras stereo pair;curvature centers;calibration procedure;optical axis;visual axis;subject sitting;RMS error;point-of-gaze estimation;eyes;size 17.8 mm","","1","","15","","8 Apr 2010","","","IEEE","IEEE Conferences"
"Resolution of focus of attention using gaze direction estimation and saliency computation","Z. Yücel; A. A. Salah","CWI, Science Park 123, 1098 XG, Amsterdam, The Netherlands; ISLA, Science Park 107, 1098 XG, Amsterdam, The Netherlands","2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops","8 Dec 2009","2009","","","1","6","Modeling the user's attention is useful for responsive and interactive systems. This paper proposes a method for establishing joint visual attention between an experimenter and an intelligent agent. A rapid procedure is described to track the 3D head pose of the experimenter, which is used to approximate the gaze direction. The head is modeled with a sparse grid of points sampled from the surface of a cylinder. We then propose to employ a bottom-up saliency model to single out interesting objects in the neighborhood of the estimated focus of attention. We report results on a series of experiments, where a human experimenter looks at objects placed at different locations of the visual field, and the proposed algorithm is used to locate target objects automatically. Our results indicate that the proposed approach achieves high localization accuracy and thus constitutes a useful tool for the construction of natural human-computer interfaces.","2156-8111","978-1-4244-4800-5","10.1109/ACII.2009.5349547","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5349547","Head pose estimation;gaze estimation;joint attention modeling;saliency;intelligent interaction","Head;Humans;Face detection;Intelligent agent;Focusing;Interactive systems;Bayesian methods;Space exploration;Interpolation;Eyes","human computer interaction;pose estimation;software agents;solid modelling","gaze direction estimation;intelligent agent;human experimenter;human computer interfaces;3D head pose estimation;bottom-up saliency model;attention focus estimation","","5","1","12","","8 Dec 2009","","","IEEE","IEEE Conferences"
"An automatic calibration procedure for remote eye-gaze tracking systems","D. Model; E. D. Guestrin; M. Eizenman","Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON M5S 3G4, Canada; Department of Electrical and Computer Engineering, and the Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, ON M5S 3G9, Canada; Department of Electrical and Computer Engineering, the Department of Ophthalmology and Vision Sciences, and the Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, ON M5S 3G9, Canada","2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society","13 Nov 2009","2009","","","4751","4754","Remote gaze estimation systems use calibration procedures to estimate subject-specific parameters that are needed for the calculation of the point-of-gaze. In these procedures, subjects are required to fixate on a specific point or points at specific time instances. Advanced remote gaze estimation systems can estimate the optical axis of the eye without any personal calibration procedure, but use a single calibration point to estimate the angle between the optical axis and the visual axis (line-of-sight). This paper presents a novel automatic calibration procedure that does not require active user participation. To estimate the angles between the optical and visual axes of each eye, this procedure minimizes the distance between the intersections of the visual axes of the left and right eyes with the surface of a display while subjects look naturally at the display (e.g., watching a video clip). Simulation results demonstrate that the performance of the algorithm improves as the range of viewing angles increases. For a subject sitting 75 cm in front of an 80 cm times 60 cm display (40rdquo TV) the standard deviation of the error in the estimation of the angles between the optical and visual axes is 0.5deg.","1558-4615","978-1-4244-3296-7","10.1109/IEMBS.2009.5334183","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5334183","Remote gaze estimation;calibration","Calibration;Displays;Cornea;Parameter estimation;Eyes;Humans;Joining processes;USA Councils;TV;Estimation error","calibration;cognition;eye;parameter estimation;visual perception","automatic calibration procedure;remote eye-gaze tracking systems;remote gaze estimation system;subject-specific parameters estimation;optical axis;active user participation;visual axis;display surface","Algorithms;Calibration;Fixation, Ocular;Humans;Models, Theoretical","2","","13","","13 Nov 2009","","","IEEE","IEEE Conferences"
"Visual gaze projection in front of a target scene","A. Lablack; F. Maquet; N. Ihaddadene; C. Djeraba","Laboratoire Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France; Laboratoire Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France; Laboratoire Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France; Laboratoire Informatique Fondamentale de Lille (LIFL), Université de Lille 1, France","2009 IEEE International Conference on Multimedia and Expo","18 Aug 2009","2009","","","1839","1840","In this technical demonstration, we present a tool that projects on a target scene the visual gaze of the people passing in front of it. The target scene could be a large plasma screen, an advertising poster, a shelf or a shop window. The projection of the visual gaze corresponds to the coordinates of a region that represents the person's location of interest in the target scene. It is an important problem with many applications that try to understand human behavior in a controlled environment such as in security or customized marketing. The visual gaze projection is influenced by the parameters of the camera, the settings of the target scene, the method used for the estimation of the gaze (e.g. the information about the head pose, the location of eyes centers and corners, or any combination of them), and the visual gaze starting point.","1945-788X","978-1-4244-4290-4","10.1109/ICME.2009.5202883","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5202883","Visual gaze estimation;Projection;Region of interest","Layout;Eyes;Humans;Cameras;Plasma displays;Advertising;Information security;Estimation error;Data mining;Monitoring","cameras;image processing","visual gaze projection;target scene;large plasma screen;advertising poster;shelf;shop window;human behavior;gaze estimation;camera","","4","","3","","18 Aug 2009","","","IEEE","IEEE Conferences"
"A real time pupil location system using multiple illuminations clustering under varying environment","Mi Young Nam; P. K. Rhee; C. S. Lee","Inha University, Korea; Inha University, Korea; Samsung Electronics, South Korea","2009 Digest of Technical Papers International Conference on Consumer Electronics","29 May 2009","2009","","","1","2","In an eye gaze estimation system, pupil location is key part. And in order to get the position of pupil with high accuracy, edge detector algorithm is necessary. The Canny edge detector is widely used in computer vision to locate sharp intensity changes and to find object boundaries in an image. In this paper, we make a improvement based on dynamic threshold of edge detection algorithm. It makes up for the disadvantage of traditional canny algorithm, which would lose some edges during detecting images with different situation like illumination, glass-wear and so on using fixed threshold.","2158-4001","978-1-4244-4701-5","10.1109/ICCE.2009.5012398","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5012398","","Real time systems;Lighting;Image edge detection;Clustering algorithms;Detectors;Computer vision;Face detection;Humans;Object detection;Image analysis","computer vision;edge detection;object detection;pattern clustering","real time pupil location system;multiple illumination clustering;eye gaze estimation system;edge detector algorithm;Canny edge detector;computer vision;object boundary","","1","","5","","29 May 2009","","","IEEE","IEEE Conferences"
"Improving the Accuracy and Reliability of Remote System-Calibration-Free Eye-Gaze Tracking","C. A. Hennessey; P. D. Lawrence","Dept. of Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC; Dept. of Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC","IEEE Transactions on Biomedical Engineering","12 Jun 2009","2009","56","7","1891","1900","Remote eye-gaze tracking provides a means for nonintrusive tracking of the point-of-gaze (POG) of a user. For application as a user interface for the disabled, a remote system that is noncontact, reliable, and permits head motion is very desirable. The system-calibration-free pupil-corneal reflection (P-CR) vector technique for POG estimation is a popular method due to its simplicity, however, accuracy has been shown to be degraded with head displacement. Model-based POG-estimation methods were developed, which improve system accuracy during head displacement, however, these methods require complex system calibration in addition to user calibration. In this paper, the use of multiple corneal reflections and point-pattern matching allows for a scaling correction of the P-CR vector for head displacements as well as an improvement in system robustness to corneal reflection distortion, leading to improved POG-estimation accuracy. To demonstrate the improvement in performance, the enhanced multiple corneal reflection P-CR method is compared to the monocular and binocular accuracy of the traditional single corneal reflection P-CR method, and a model-based method of POG estimation for various head displacements.","1558-2531","","10.1109/TBME.2009.2015955","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4797862","Binocular;eye-gaze;multiple corneal reflection;pupil-corneal reflection (P-CR);remote;single camera;system-calibration free;tracking","Reflection;Calibration;Head;Degradation;Costs;User interfaces;Robustness;Cameras;Injuries;Keyboards","calibration;eye;feature extraction;gesture recognition;handicapped aids;image motion analysis;tracking","remote eye-gaze tracking;point-of-gaze estimation method;user interface;system-calibration-free pupil-corneal reflection vector technique;head displacement;complex system calibration;point-pattern matching;enhanced multiple corneal reflection P-CR method;monocular accuracy;binocular accuracy;image feature extraction","Adult;Analysis of Variance;Communication Aids for Disabled;Cornea;Eye Movements;Female;Fixation, Ocular;Humans;Image Processing, Computer-Assisted;Male;Models, Biological;Pattern Recognition, Automated;Pupil;Reproducibility of Results;Sensitivity and Specificity;User-Computer Interface;Video Recording","41","5","25","IEEE","4 Mar 2009","","","IEEE","IEEE Journals"
"Noncontact Binocular Eye-Gaze Tracking for Point-of-Gaze Estimation in Three Dimensions","C. Hennessey; P. Lawrence","Univ. of British Columbia, Vancouver, BC; Dept. of Electr. & Comput. Eng., Univ. of British Columbia, Vancouver, BC","IEEE Transactions on Biomedical Engineering","21 Apr 2009","2009","56","3","790","799","Binocular eye-gaze tracking can be used to estimate the point-of-gaze (POG) of a subject in real-world 3D space using the vergence of the eyes. In this paper, a novel noncontact model-based technique for 3D POG estimation is presented. The noncontact system allows people to select real-world objects in 3D physical space using their eyes, without the need for head-mounted equipment. Remote 3D POG estimation may be especially useful for persons with quadriplegia or Amyotrophic Lateral Sclerosis. It would also enable a user to select 3D points in space generated by 3D volumetric displays, with potential applications to medical imaging and telesurgery. Using a model-based POG estimation algorithm allows for free head motion and a single stage of calibration. It is shown that an average accuracy of 3.93 cm was achieved over a workspace volume of 30 times 23 times 25 cm (W times H times D) with a maximum latency of 1.5 s due to the digital filtering employed. The users were free to naturally move and reorient their heads while operating the system, within an allowable headspace of 3 cm times 9 cm times 14 cm.","1558-2531","","10.1109/TBME.2008.2005943","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4633699","Binocular, eye-gaze tracking;high speed;human--computer interface;noncontact;three-dimensional point-of-gaze (POG)","Three dimensional displays;Eyes;Biomedical imaging;Magnetic heads;Calibration;Two dimensional displays;Design engineering;Motion estimation;Delay;Digital filters","biomechanics;biomedical optical imaging;cognition;diseases;eye;human computer interaction;medical image processing;neurophysiology;surgery;telemedicine;visual perception","noncontact binocular eye-gaze tracking;point-of-gaze estimation;real-world 3D space model POG estimation;quadriplegia;amyotrophic lateral sclerosis;medical imaging;telesurgery;free-head motion;calibration;digital filtering;eye vergence","Adult;Algorithms;Calibration;Equipment Design;Eye Movement Measurements;Eye Movements;Female;Fixation, Ocular;Head Movements;Humans;Male;Models, Biological;Sensitivity and Specificity","39","7","37","IEEE","26 Sep 2008","","","IEEE","IEEE Journals"
"A fast and robust 3D head pose and gaze estimation system","K. Kinoshita; S. Lao","Core Technology Center, OMRON Corporation, 9-1, Kizugawadai, Kizugawa-city, Kyoto 619-0283, JAPAN; Core Technology Center, OMRON Corporation, 9-1, Kizugawadai, Kizugawa-city, Kyoto 619-0283, JAPAN","2008 8th IEEE International Conference on Automatic Face & Gesture Recognition","10 Apr 2009","2008","","","1","2","We developed a fast and robust head pose and gaze estimation system. This system can detect facial feature points and estimate 3D pose angles and gaze direction in various conditions including changes in facial expression, partial occlusion, etc. The system needs only one face image as input and doesn't need any special devices such as blinking LED or stereo camera. Moreover, no calibration process is needed. It shows 95% of head pose estimation accuracy and 81% of gaze estimation accuracy (when the error margin is 15 degrees). The processing time is approximately 15ms/frame (Pentium4 3.2 GHz). Acceptable range of facial pose is within +/- 60 degrees in yaw (left-right) and within +/- 30 degrees in pitch (up-down).","","978-1-4244-2153-4","10.1109/AFGR.2008.4813466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4813466","","Robustness;Face detection;Real time systems;Lighting;Head;Cameras;Facial features;Calibration;Universal Serial Bus;Streaming media","face recognition;pose estimation;stereo image processing","robust 3D head pose angle estimation;gaze estimation system;facial feature point detection;facial expression;partial occlusion;face image;LED;stereo camera","","","9","13","","10 Apr 2009","","","IEEE","IEEE Conferences"
"3D gaze estimation with a single camera without IR illumination","J. Chen; Q. Ji","Rensselaer Polytechnic Institute, Troy, NY, 12180-3590, USA; Rensselaer Polytechnic Institute, Troy, NY, 12180-3590, USA","2008 19th International Conference on Pattern Recognition","23 Jan 2009","2008","","","1","4","This paper proposes a 3D eye gaze estimation and tracking algorithm based on facial feature tracking using a single camera. Instead of using the infrared (IR) lights and the corneal reflections (glint), this algorithm estimates the 3D visual axis using the tracked facial feature points. For this, we first introduce an extended 3D eye model which includes both the eyeball and the eye-corners. Based on this eye model, we derive the equations to solve for the 3D eyeball center, the 3D pupil center and the 3D visual axis, from which we can solve for the point of gaze after a one-time personal calibration. The experimental results show the accuracy of this algorithm is less than 3deg. Compared with the existing IR-based eye tracking methods, the proposed method is simple to setup and can work both indoor and outdoor.","1051-4651","978-1-4244-2174-9","10.1109/ICPR.2008.4761343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4761343","","Cameras;Lighting;Iris;Facial features;Equations;Calibration;Shape;Face detection;Optical reflection;Eyelids","cameras;eye;face recognition;feature extraction;tracking;visual perception","3D eye gaze estimation;single camera;facial feature tracking algorithm;3D visual axis estimation","","44","3","12","","23 Jan 2009","","","IEEE","IEEE Conferences"
"A Human–Computer Interface Using Symmetry Between Eyes to Detect Gaze Direction","J. J. Magee; M. Betke; J. Gips; M. R. Scott; B. N. Waber","Dept. of Comput. Sci., Boston Univ., Boston, MA; Dept. of Comput. Sci., Boston Univ., Boston, MA; NA; NA; NA","IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans","5 Nov 2008","2008","38","6","1248","1261","In the cases of paralysis so severe that a person's ability to control movement is limited to the muscles around the eyes, eye movements or blinks are the only way for the person to communicate. Interfaces that assist in such communication are often intrusive, require special hardware, or rely on active infrared illumination. A nonintrusive communication interface system called EyeKeys was therefore developed, which runs on a consumer-grade computer with video input from an inexpensive Universal Serial Bus camera and works without special lighting. The system detects and tracks the person's face using multiscale template correlation. The symmetry between left and right eyes is exploited to detect if the person is looking at the camera or to the left or right side. The detected eye direction can then be used to control applications such as spelling programs or games. The game ldquoBlockEscaperdquo was developed to evaluate the performance of EyeKeys and compare it to a mouse substitution interface. Experiments with EyeKeys have shown that it is an easily used computer input and control device for able-bodied people and has the potential to become a practical tool for people with severe paralysis.","1558-2426","","10.1109/TSMCA.2008.2003466","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4648947","Assistive technology;disabled computer users;face detection;face tracking;gaze estimation;video-based human–computer interfaces;webcams","Eyes;Cameras;Face detection;Communication system control;Muscles;Hardware;Optical fiber communication;Lighting;Computer interfaces;Universal Serial Bus","eye;handicapped aids;human computer interaction;interactive devices;user interfaces","human-computer interface;eyes symmetry;detect gaze direction;eye movements;nonintrusive communication interface system;EyeKeys;multiscale template correlation;BlockEscape","","38","1","74","","5 Nov 2008","","","IEEE","IEEE Journals"
"A Novel Gaze Estimation System With One Calibration Point","A. Villanueva; R. Cabeza","Electr. & Electron. Eng. Dept., Public Univ. of Navarra, Pamplona; Electr. & Electron. Eng. Dept., Public Univ. of Navarra, Pamplona","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","16 Jul 2008","2008","38","4","1123","1138","The design of robust and high-performance gaze-tracking systems is one of the most important objectives of the eye-tracking community. In general, a subject calibration procedure is needed to learn system parameters and be able to estimate the gaze direction accurately. In this paper, we attempt to determine if subject calibration can be eliminated. A geometric analysis of a gaze-tracking system is conducted to determine user calibration requirements. The eye model used considers the offset between optical and visual axes, the refraction of the cornea, and Donder's law. This paper demonstrates the minimal number of cameras, light sources, and user calibration points needed to solve for gaze estimation. The underlying geometric model is based on glint positions and pupil ellipse in the image, and the minimal hardware needed for this model is one camera and multiple light-emitting diodes. This paper proves that subject calibration is compulsory for correct gaze estimation and proposes a model based on a single point for subject calibration. The experiments carried out show that, although two glints and one calibration point are sufficient to perform gaze estimation (error ~ 1deg), using more light sources and calibration points can result in lower average errors.","1941-0492","","10.1109/TSMCB.2008.926606","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4567549","Calibration;gaze estimation;line of sight (LOS);point of regard (POR);video oculography","Calibration;Optical refraction;Cameras;Light sources;Solid modeling;Robustness;Geometrical optics;Cornea;Hardware;Light emitting diodes","calibration;graphical user interfaces;video signal processing","gaze tracking system;eye tracking;subject calibration;gaze direction estimation;geometric analysis;user calibration requirement;eye model;user calibration point;geometric model;glint position;pupil ellipse","Artificial Intelligence;Computer Simulation;Computer Systems;Fixation, Ocular;Humans;Image Interpretation, Computer-Assisted;Models, Biological;Pattern Recognition, Automated;Posture;Pupil","97","46","28","","16 Jul 2008","","","IEEE","IEEE Journals"
"Infotainment devices control by eye gaze and gesture recognition fusion","T. Nawaz; M. S. Mian; H. A. Habib","Fac. of Telecommun. & Inf. Eng., Univ. of Eng. & Technol., Taxila; Fac. of Telecommun. & Inf. Eng., Univ. of Eng. & Technol., Taxila; Member","IEEE Transactions on Consumer Electronics","15 Jul 2008","2008","54","2","277","282","This paper presents a novel concept for controlling the consumer devices such as MP3 player or other daily life appliances by fusion of eye gaze and gesture recognition methodologies. Such system is deployable for virtually controlling the consumer devices anywhere in the home and office. The usability of the system is also tested with patients at the intensive care units of the hospitals where the patients may not able to operate the consumer devices in a regular way. The proposed system consists of a video processing based embedded system, a CCD camera and situated display. Physical control options are displayed over the situated display. Selection / de-selection process of displayed control option over situated display is accomplished by analyzing the video sequence captured by the CCD camera. Eye gaze estimation and head gesture recognition algorithms analyze the video sequence and reveal the control command that has to be sent to the infotainment or other consumer devices.","1558-4127","","10.1109/TCE.2008.4560086","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4560086","","Displays;Charge coupled devices;Charge-coupled image sensors;Video sequences;Digital audio players;Home appliances;Control systems;Usability;System testing;Hospitals","CCD image sensors;consumer electronics;embedded systems;gesture recognition;image fusion;image sequences;video signal processing","infotainment devices control;gesture recognition fusion;consumer devices controlling;video processing;embedded system;CCD camera;situated display;physical control options;selection process;de-selection process;video sequence;eye gaze estimation;head gesture recognition algorithms","","11","2","21","","15 Jul 2008","","","IEEE","IEEE Journals"
"Remote and head-motion-free gaze tracking for real environments with automated head-eye model calibrations","Hirotake Yamazoe; Akira Utsumi; Tomoko Yonezawa; Shinji Abe","ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan; ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan; ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan; ATR Intelligent Robotics and Communication Laboratories, 2-2-2 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0288, Japan","2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops","15 Jul 2008","2008","","","1","6","We propose a gaze estimation method that substantially relaxes the practical constraints possessed by most conventional methods. Gaze estimation research has a long history, and many systems including some commercial schemes have been proposed. However, the application domain of gaze estimation is still limited (e.g, measurement devices for HCI issues, input devices for VDT works) due to the limitations of such systems. First, users must be close to the system (or must wear it) since most systems employ IR illumination and/or stereo cameras. Second, users are required to perform manual calibrations to get geometrically meaningful data. These limitations prevent applications of the system that capture and utilize useful human gaze information in daily situations. In our method, inspired by a bundled adjustment framework, the parameters of the 3D head-eye model are robustly estimated by minimizing pixel-wise re-projection errors between single-camera input images and eye model projections for multiple frames with adjacently estimated head poses. Since this process runs automatically, users does not need to be aware of it. Using the estimated parameters, 3D head poses and gaze directions for newly observed images can be directly determined with the same error minimization manner. This mechanism enables robust gaze estimation with single-camera-based low resolution images without user-aware preparation tasks (i.e., calibration). Experimental results show the proposed method achieves 6deg accuracy with QVGA (320 times 240) images. The proposed algorithm is free from observation distances. We confirmed that our system works with long-distance observations (10 meters).","2160-7508","978-1-4244-2339-2","10.1109/CVPRW.2008.4563184","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4563184","","Calibration;Robustness;Head;History;Human computer interaction;Lighting;Cameras;Pixel;Parameter estimation;Image resolution","estimation theory;eye;human computer interaction;image motion analysis;image resolution;parameter estimation;pose estimation;stereo image processing","head-motion-free gaze tracking;automated head-eye model calibrations;gaze estimation method;IR illumination;stereo cameras;human gaze information;robust estimation;head pose estimation;parameter estimation;image resolution","","4","1","15","","15 Jul 2008","","","IEEE","IEEE Conferences"
"3D Gaze Estimation and Interaction","J. Ki; Y. Kwon","Imaging Media Research Center, Korea Institute of Science & Technology, Seoul, Korea, hau4014@imrc.kist.re.kr; Imaging Media Research Center, Korea Institute of Science & Technology, Seoul, Korea, ymk@kist.re.kr","2008 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video","20 Jun 2008","2008","","","373","376","There are several researches on 2D gaze tracking techniques for the 2D screen for the human-computer interaction. However, the researches for gaze-based interaction to the 3D stereo images or contents are not reported. The 3D display techniques are emerging now for the reality service. Moreover, the 3D interaction techniques are much more needed in the 3D contents service environments. This paper addresses gaze-based 3D interaction techniques on autostereoscopic display, such as parallax barrier or lenticular display. This paper presents our researches on 3D gaze estimation and gaze-based interaction to autostereoscopic display. The evaluation of our system is shown in terms of accuracy in gaze direction and gaze depth.","2161-203X","978-1-4244-1760-5","10.1109/3DTV.2008.4547886","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4547886","3D gaze;pupil center distance (PCD);3D interaction;autostereoscopic display","Three dimensional displays;Computer displays;Human computer interaction;Pervasive computing;Cameras;Magnetic heads;Communications technology;Keyboards;Mice;Auditory system","human computer interaction;stereo image processing;three-dimensional displays","3D gaze estimation algorithm;human-computer interaction;gaze-based 3D interaction techniques;3D stereo images;3D display techniques;3D contents;autostereoscopic display","","6","1","7","","20 Jun 2008","","","IEEE","IEEE Conferences"
"Fixation Precision in High-Speed Noncontact Eye-Gaze Tracking","C. Hennessey; B. Noureddin; P. Lawrence","Univ. of British Columbia, Vancouver; Univ. of British Columbia, Vancouver; Univ. of British Columbia, Vancouver","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","7 Mar 2008","2008","38","2","289","298","The precision of point-of-gaze (POG) estimation during a fixation is an important factor in determining the usability of a noncontact eye-gaze tracking system for real-time applications. The objective of this paper is to define and measure POG fixation precision, propose methods for increasing the fixation precision, and examine the improvements when the methods are applied to two POG estimation approaches. To achieve these objectives, techniques for high-speed image processing that allow POG sampling rates of over 400 Hz are presented. With these high-speed POG sampling rates, the fixation precision can be improved by filtering while maintaining an acceptable real-time latency. The high-speed sampling and digital filtering techniques developed were applied to two POG estimation techniques, i.e., the highspeed pupil-corneal reflection (HS P-CR) vector method and a 3-D model-based method allowing free head motion. Evaluation on the subjects has shown that when operating at 407 frames per second (fps) with filtering, the fixation precision for the HS P-CR POG estimation method was improved by a factor of 5.8 to 0.035deg (1.6 screen pixels) compared to the unfiltered operation at 30 fps. For the 3-D POG estimation method, the fixation precision was improved by a factor of 11 to 0.050deg (2.3 screen pixels) compared to the unfiltered operation at 30 fps.","1941-0492","","10.1109/TSMCB.2007.911378","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4410444","Eye-gaze tracking;fixation precision;head free;high speed;human–computer interface;noncontact;remote;Eye-gaze tracking;fixation precision;head free;high speed;human¿computer interface;noncontact;remote","Image sampling;Filtering;Usability;Real time systems;Image processing;Delay;Digital filters;Motion estimation;Reflection;Head","image processing;motion estimation;tracking filters","fixation precision;noncontact eye-gaze tracking;point-of-gaze estimation;image processing;real-time latency;digital filtering;high-speed pupil-corneal reflection vector;3D model-based method;head motion","Algorithms;Artificial Intelligence;Computer Simulation;Fixation, Ocular;Image Enhancement;Image Interpretation, Computer-Assisted;Models, Biological;Pattern Recognition, Automated;Reproducibility of Results;Sensitivity and Specificity;Video Recording","28","17","30","","7 Mar 2008","","","IEEE","IEEE Journals"
"An Adaptive Algorithm for Eye-Gaze-Tracking-Device Calibration","Z. Ramdane-Cherif; A. NaÏt-AliNait-Ali","INSERM U684/ERI 13, Nancy; NA","IEEE Transactions on Instrumentation and Measurement","5 Mar 2008","2008","57","4","716","723","Eye-gaze tracking has been an important and interesting area of research for quite some time now. In laboratory experiments for psychological research, eye-gaze tracking constitutes a helpful tool to probe the perceptual or the cognitive processes of the subjects. In day-to-day applications, eye-gaze tracking can be used as a computer interface for both industrial and nonindustrial applications, which require hands-free installations. It can also be used to help disabled people to use computers for communication and for environmental control. This paper aims to develop a computer system to facilitate the study of visual scanning and eye fixation during image exploration. The system uses an infrared device to detect horizontal and vertical eye movements. This paper mainly focuses its attention on the calibration procedure that can be effectively utilized for correction of the nonlinearities in the oculomotor for the eyes of each individual subject. An adaptive calibration algorithm has been developed by employing a polynomial model. Several experiments have been carried out to demonstrate that the proposed approach is a rapid one and is accurate for the study of visual perception and recognition processes.","1557-9662","","10.1109/TIM.2007.913590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424444","Eye tracking;gaze estimation;geometric correction;polynomial model;visual exploration;Eye tracking;gaze estimation;geometric correction;polynomial model;visual exploration","Adaptive algorithm;Calibration;Application software;Infrared detectors;Laboratories;Psychology;Probes;Computer interfaces;Computer industry;Electrical equipment industry","calibration;image recognition;tracking;user interfaces","eye-gaze-tracking-device calibration;cognitive processes;computer interface;hands-free installations;environmental control;computer system;eye fixation;visual scanning;image exploration;eye movement detection;oculomotor nonlinearities;adaptive calibration algorithm;visual perception;visual recognition processes","","21","","21","","5 Mar 2008","","","IEEE","IEEE Journals"
"Investigation of the Cross-Ratios Method for Point-of-Gaze Estimation","J. J. Kang; M. Eizenman $^*$; E. D. Guestrin; E. Eizenman","Dept. of Electr. & Comput. Eng., Toronto Univ., Toronto, ON; NA; NA; NA","IEEE Transactions on Biomedical Engineering","19 Aug 2008","2008","55","9","2293","2302","The cross-ratios method for point-of-gaze (PoG) estimation uses the invariance property of cross-ratios in projective transformations. The inherent causes of the subject-dependent PoG estimation bias exhibited by this method have not been well characterized in the literature. Using a model of the eye and the components of a system (camera, light sources) that estimates PoG, a theoretical framework for the cross-ratios method is developed. The analysis of the cross-ratios method within this framework shows that the subject-dependent estimation bias is caused mainly by: 1) the angular deviation of the visual axis from the optic axis and 2) the fact that the virtual image of the pupil center is not coplanar with the virtual images of the light sources that illuminate the eye (corneal reflections). The theoretical framework provides a closed-form analytical expression that predicts the estimation bias as a function of subject-specific eye parameters. The theoretical framework also provides a clear physical interpretation for an existing empirically derived two-step procedure that compensates for the estimation bias and shows that the first step of this procedure is equivalent to moving the corneal reflections to a new plane that minimizes the distance from this plane to the virtual image of the pupil center.","1558-2531","","10.1109/TBME.2008.919722","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4463644","Cross-ratios;eye model;eye parameters;point-of-gaze (PoG);remote gaze estimation;Point-of-gaze;remote gaze estimation;cross-ratios;eye model;eye parameters","Optical reflection;Biomedical engineering;Light sources;Cameras;Scholarships;Calibration;Image analysis;Retina;Mood","estimation theory;eye;vision","cross-ratios method;point-of-gaze estimation;projective transformation;eye model;subject dependent estimation bias;visual axis;optic axis;virtual image;pupil center;light sources;corneal reflection","Algorithms;Computer Simulation;Fixation, Ocular;Humans;Iris;Models, Biological;Photometry;Pupil;Reproducibility of Results;Sensitivity and Specificity","25","2","13","","5 Mar 2008","","","IEEE","IEEE Journals"
"3D Gaze Tracking and Analysis for Attentive Human Computer Interaction","J. Ki; Y. Kwon; K. Sohn","Imaging Media Res. Center, Korea Inst. of Sci. & Technol., Seoul; Imaging Media Res. Center, Korea Inst. of Sci. & Technol., Seoul; NA","2007 Frontiers in the Convergence of Bioscience and Information Technologies","16 May 2008","2007","","","617","621","There are several researches on 2D gaze tracking techniques for the 2D screen for the human-computer interaction. However, the researches for the gaze-based interaction to the stereo images or contents are not reported. The 3D display techniques are emerging now for the reality service. Moreover, the 3D interaction techniques are much more needed in the 3D contents service environments. This paper addresses gaze-based 3D interaction techniques on stereo display, such as parallax barrier or lenticular stereo display. This paper presents our researches on 3D gaze estimation and gaze-based interaction to stereo display.","","978-0-7695-2999-8","10.1109/FBIT.2007.15","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4524177","","Human computer interaction;Three dimensional displays;Computer displays;Cameras;Eyes;Pervasive computing;Calibration;Information technology;Image analysis;Information analysis","image recognition;stereo image processing;user interfaces","3D gaze tracking;attentive human-computer interaction;stereo images;parallax barrier;lenticular stereo display;gaze-based interaction;3D gaze estimation","","4","1","7","","16 May 2008","","","IEEE","IEEE Conferences"
"A comprehensive head pose and gaze database","U. Weidenbacher; G. Layher; P. -. Strauss; H. Neumann","Inst. of Neural Inf. Process., Univ. of Ulm, Ulm; Inst. of Neural Inf. Process., Univ. of Ulm, Ulm; NA; NA","2007 3rd IET International Conference on Intelligent Environments","12 Feb 2008","2007","","","455","458","Within the past decade, many computational approaches have been developed to estimate gaze directions of persons based on their facial appearance. Most researchers used common face datasets with only a limited representation of different head poses to train and verify their algorithms. Moreover, in most datasets, faces have neither a defined gaze direction, nor do they incorporate different combinations of eye gaze and head pose. Therefore, we recorded an extended dataset of 20 subjects including faces in various combinations of head pose and eye gaze leading to a total amount of 2220 colour images (111 per subject). The images were produced under controlled conditions, i.e. we used a technique to make sure that the subjects adjust their head and eyes appropriately. Furthermore, all images are manually labelled with landmarks indicating important features of the face. Finally, we evaluate the dataset with two computational methods for head pose and gaze estimation respectively.","0537-9989","978-0-86341-853-2","10.1049/cp:20070407","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4449972","","","face recognition;image colour analysis;pose estimation;visual databases","comprehensive head pose;gaze database;facial appearance;face datasets;gaze estimation","","13","2","","","12 Feb 2008","","","IET","IET Conferences"
"Log-Linear Elliptic Transform for Frontal-Face Parameter Estimation","C. A. Perez; C. M. Aravena","Department of Electrical Engineering, Universidad de Chile, Av. Tupper 2007, Santiago, Chile; Department of Electrical Engineering, Universidad de Chile, Av. Tupper 2007, Santiago, Chile","2007 IEEE International Conference on Systems, Man and Cybernetics","2 Jan 2008","2007","","","1130","1134","Face detection methods are important in many applications such as face recognition, iris detection, eye-gaze estimation, gesture recognition and in man-machine interfaces, among others. In this paper, a new method for face detection of frontal faces is proposed. The method is based on a geometric transform we named Log-Linear Elliptic Transform, LET, that converts the lower semi-elliptical contour of the face into a vertical line in the transformed space. The vertical line can be detected by a simple projected histogram allowing a fast computation. The vertical line detection is sensitive to the face position, eccentricity and rotation. This allows the selection of an ellipse that better represents the face. The method was applied successfully in the detection of frontal faces in the FERET database.","1062-922X","978-1-4244-0990-7","10.1109/ICSMC.2007.4414192","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4414192","","Parameter estimation;Face detection;Image edge detection;Face recognition;Iris;Histograms;User interfaces;Position sensitive particle detectors;Databases;Helium","elliptic equations;face recognition;parameter estimation;statistical analysis;transforms;visual databases","log-linear elliptic transform;face detection;geometric transform;histogram;FERET database;frontal face parameter estimation","","","6","14","","2 Jan 2008","","","IEEE","IEEE Conferences"
"Novel Eye Gaze Tracking Techniques Under Natural Head Movement","Z. Zhu; Q. Ji","Rensselaer Polytech. Inst., Troy; Rensselaer Polytech. Inst., Troy","IEEE Transactions on Biomedical Engineering","19 Nov 2007","2007","54","12","2246","2260","Most available remote eye gaze trackers have two characteristics that hinder them being widely used as the important computer input devices for human computer interaction. First, they have to be calibrated for each user individually; second, they have low tolerance for head movement and require the users to hold their heads unnaturally still. In this paper, by exploiting the eye anatomy, we propose two novel solutions to allow natural head movement and minimize the calibration procedure to only one time for a new individual. The first technique is proposed to estimate the 3D eye gaze directly. In this technique, the cornea of the eyeball is modeled as a convex mirror. Via the properties of convex mirror, a simple method is proposed to estimate the 3D optic axis of the eye. The visual axis, which is the true 3D gaze direction of the user, can be determined subsequently after knowing the angle deviation between the visual axis and optic axis by a simple calibration procedure. Therefore, the gaze point on an object in the scene can be obtained by simply intersecting the estimated 3D gaze direction with the object. Different from the first technique, our second technique does not need to estimate the 3D eye gaze directly, and the gaze point on an object is estimated from a gaze mapping function implicitly. In addition, a dynamic computational head compensation model is developed to automatically update the gaze mapping function whenever the head moves. Hence, the eye gaze can be estimated under natural head movement. Furthermore, it minimizes the calibration procedure to only one time for a new individual. The advantage of the proposed techniques over the current state of the art eye gaze trackers is that it can estimate the eye gaze of the user accurately under natural head movement, without need to perform the gaze calibration every time before using it. Our proposed methods will improve the usability of the eye gaze tracking technology, and we believe that it represents an important step for the eye tracker to be accepted as a natural computer input device.","1558-2531","","10.1109/TBME.2007.895750","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4359993","Eye gaze tracking;gaze estimation;human computer interaction","Tracking;Calibration;Mirrors;Human computer interaction;Anatomy;Cornea;Layout;Computational modeling;State estimation;Usability","biomechanics;eye;vision","eye gaze tracking;natural head movement;human computer interaction;calibration;convex mirror;visual axis;optic axis;dynamic computational head","Computer Simulation;Diagnosis, Computer-Assisted;Diagnosis, Computer-Assisted;Equipment Design;Equipment Failure Analysis;Eye Movement Measurements;Eye Movements;Fixation, Ocular;Head Movements;Humans;Models, Biological;Reproducibility of Results;Sensitivity and Specificity","176","11","33","","19 Nov 2007","","","IEEE","IEEE Journals"
"Remote Point-of-Gaze Estimation with Free Head Movements Requiring a Single-Point Calibration","E. D. Guestrin; M. Eizenman","Member, IEEE, Department of Electrical and Computer Engineering, Institute of Biomaterials and Biomedical Engineering, University of Toronto, 164 College Street, Toronto, ON M5S 3G9, Canada. phone: 1-416-978-2255; fax: 1-416-978-4317; e-mail: elias.guestrin@utoronto.ca; Departments of Electrical and Computer Engineering and Ophthalmology, Institute of Biomaterials and Biomedical Engineering, University of Toronto, Toronto, ON M5S 3G9, Canada. e-mail: eizenm@ecf.utoronto.ca","2007 29th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","22 Oct 2007","2007","","","4556","4560","This paper describes a method for remote, non- contact point-of-gaze (POG) estimation that tolerates free head movements and requires a simple calibration procedure in which the subject has to fixate only on a single point. This method uses the centers of the pupil and at least two corneal reflections (virtual images of light sources) that are estimated from eye images captured by at least two cameras. Experimental results obtained with a prototype system that tolerates head movements in a volume of about 1 dm<sup>3</sup>, exhibited RMS POG estimation errors of approximately 0.6-1deg of visual angle. This system can enable applications with infants that, otherwise, would not be possible with existing POG estimation methods, which typically require multiple-point calibration procedures.","1558-4615","978-1-4244-0787-3","10.1109/IEMBS.2007.4353353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4353353","","Calibration;Cameras;Light sources;Optical reflection;Head;Pediatrics;Mathematical model;Cornea;Prototypes;Ray tracing","biomechanics;biomedical optical imaging;CCD image sensors;eye;paediatrics;visual perception","remote noncontact point-of-gaze estimation;free head movements;single-point calibration;corneal reflections;virtual images;light sources;eye images;infants;pupil;CCD cameras","Calibration;Head;Humans;Locomotion;Models, Theoretical;Movement;Pupil;Telemetry","14","2","8","","22 Oct 2007","","","IEEE","IEEE Conferences"
"IR Image Based Eye Gaze Estimation","D. Xia; Z. Ruan","Southeast University, China; Southeast University, China","Eighth ACIS International Conference on Software Engineering, Artificial Intelligence, Networking, and Parallel/Distributed Computing (SNPD 2007)","13 Aug 2007","2007","1","","220","224","In this paper, we present a novel approach to estimate the human's eye gaze from a single infrared spectrum image of two eyes. Observing that the pupil contour is a circle, we estimate the normal direction of this circle, considered as the eye gaze, from its elliptical image. By assuming that the two gaze vectors of both eyes and the vector which is determined by the two centers of the right and left pupil are coplanar, we can calculate the both eye gaze. Compared to most existing 3-D gaze estimation algorithms, our approach does not require those of face feature information such as the eye corners. Since the focal length can be calculated by our ""Coplanar Constraint"", ours does not need to know the exact focal length of the camera. The extensive experiments over simulated images and real images demonstrate the feasibility and the effectiveness of our method.","","978-0-7695-2909-7","10.1109/SNPD.2007.237","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4287506","","Iris;Cameras;Eyes;Head;Calibration;Computer vision;Artificial neural networks;Frequency estimation;Eyelids;Software engineering","computer vision;estimation theory;eye;feature extraction;infrared imaging","IR image;eye gaze estimation;infrared spectrum image;coplanar constraint;computer vision;feature extraction","","10","","13","","13 Aug 2007","","","IEEE","IEEE Conferences"
"Gaze Estimation of Human Eye","X. Sun; G. Chen; C. Zhao; J. Yang","School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094 P.R.China; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094 P.R.China; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094 P.R.China; School of Computer Science and Technology, Nanjing University of Science and Technology, Nanjing 210094 P.R.China","2006 6th International Conference on ITS Telecommunications","22 Jan 2007","2006","","","310","313","As an indispensable component of intelligent transportation systems, the driving assistant has attracted much attention of scholars and engineers. The gaze estimation of human eye just serves for the driving assistant. In this paper the eye and gaze models are introduced first and then improved by us, so that the new models are presented. Based on the new models the Hough transform method with gradient direction is used to perform the iris detection and gaze estimation. In experiments the two parameters of false estimation rate and average detection time are investigated. To the good-quality frame images the false estimation rate is 5.87% and the average detection time is 94.3 milliseconds. In the end the proposed gaze estimation algorithm is applied to a real-time gaze tracking system and the comparison result shows that it has the more accuracy","","0-7803-9586-7","10.1109/ITST.2006.288901","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4068592","","Humans;Iris;Intelligent transportation systems;Image edge detection;Vehicle safety;Stereo vision;Sun;Computer science;Parameter estimation;Real time systems","eye;Hough transforms;image recognition;real-time systems;tracking","Hough transform method;human eye;gradient direction;iris detection;false estimation rate;average detection time;image processing;real-time gaze tracking system","","3","","5","","22 Jan 2007","","","IEEE","IEEE Conferences"
"Robustifying Eye Interaction","D. W. Hansen; J. P. Hansen","IT University, Copenhagen; NA","2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)","5 Jul 2006","2006","","","152","152","This paper presents a gaze typing system based on consumer hardware. Eye tracking based on consumer hardware is subject to several unknown factors. We propose methods using robust statistical principles to accommodate uncertainties in image data as well as in gaze estimates to improve accuracy. We have succeeded to track the gaze of people with a standard consumer camera, obtaining accuracies about 160 pixels on screen. Proper design of the typing interface, however, reduces the need for high accuracy. We have observed typing speeds in the range of 3 - 5 words per minute for untrained subjects using large on-screen buttons and a new noise tolerant dwell-time principle","2160-7516","0-7695-2646-2","10.1109/CVPRW.2006.181","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1640598","","Robustness;Hardware;Cameras;Costs;Lighting;Cornea;Optical reflection;Neural networks;Data mining;Feature extraction","","","","6","","23","","5 Jul 2006","","","IEEE","IEEE Conferences"
"Non-intrusive eye gaze estimation using a projective invariant under head movement","Dong Hyun Yoo; Myung Jin Chung; Dan Byung Ju; In Ho Choi","LG Electron., Kyunggi; NA; NA; NA","Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006.","26 Jun 2006","2006","","","3443","3448","In this paper, a non-intrusive eye gaze estimation system is proposed. The proposed system consists of five light sources and two cameras, and the direction of the user's eye gaze can be computed by using a projective property without computing the geometrical relation among the eye, the cameras, and the monitor in 3D space. Our method is accurate under head movement, and is comparatively simple and fast. We introduce our method and show experimental results. The experimental results would verify the feasibility of the proposed system as a human-machine interface","1050-4729","0-7803-9505-0","10.1109/ROBOT.2006.1642228","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1642228","","Cameras;Computer vision;Magnetic heads;Light sources;Monitoring;Robot kinematics;Optical reflection;Humans;Robot control;Mice","biology computing;eye;user interfaces","nonintrusive eye gaze estimation;projective invariant;head movement;human-machine interface","","","","11","","26 Jun 2006","","","IEEE","IEEE Conferences"
"General theory of remote gaze estimation using the pupil center and corneal reflections","E. D. Guestrin; M. Eizenman","Dept. of Electr. & Comput. Eng., Inst. of Biomaterials & Biomed. Eng., Toronto, Ont., Canada; Dept. of Electr. & Comput. Eng., Inst. of Biomaterials & Biomed. Eng., Toronto, Ont., Canada","IEEE Transactions on Biomedical Engineering","5 Jun 2006","2006","53","6","1124","1133","This paper presents a general theory for the remote estimation of the point-of-gaze (POG) from the coordinates of the centers of the pupil and corneal reflections. Corneal reflections are produced by light sources that illuminate the eye and the centers of the pupil and corneal reflections are estimated in video images from one or more cameras. The general theory covers the full range of possible system configurations. Using one camera and one light source, the POG can be estimated only if the head is completely stationary. Using one camera and multiple light sources, the POG can be estimated with free head movements, following the completion of a multiple-point calibration procedure. When multiple cameras and multiple light sources are used, the POG can be estimated following a simple one-point calibration procedure. Experimental and simulation results suggest that the main sources of gaze estimation errors are the discrepancy between the shape of real corneas and the spherical corneal shape assumed in the general theory, and the noise in the estimation of the centers of the pupil and corneal reflections. A detailed example of a system that uses the general theory to estimate the POG on a computer screen is presented.","1558-2531","","10.1109/TBME.2005.863952","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1634506","Model;point of regard;pupil center and corneal reflection(s);remote gaze estimation;system configurations;video based gaze estimation","Optical reflection;Light sources;Cameras;Head;Calibration;Shape;Estimation error;Cornea;Multi-stage noise shaping;Acoustic reflection","eye;biomedical optical imaging;medical image processing;estimation theory;biomechanics","remote gaze estimation;pupil center;corneal reflections;point-of-gaze;light sources;eye;video images;free head movement;multiple-point calibration;estimation errors","Computer Simulation;Cornea;Cornea;Corneal Topography;Eye Movements;Fixation, Ocular;Humans;Image Interpretation, Computer-Assisted;Iris;Models, Biological;Ophthalmoscopy;Photometry;Pupil;Video Recording","314","58","31","","5 Jun 2006","","","IEEE","IEEE Journals"
"Driver monitoring with a single high-speed camera and IR illumination","C. Cudalbu; B. Anastasiu; R. Radu; R. Cruceanu; E. Schmidt; E. Barth","SensoMotoric Instruments GmbH, Berlin, Germany; NA; NA; NA; NA; NA","International Symposium on Signals, Circuits and Systems, 2005. ISSCS 2005.","26 Sep 2005","2005","1","","219","222 Vol. 1","The driver's diminished attention or fatigue are a major cause of fatal road accidents. Therefore techniques that can assess the fatigue and attention levels of the driver must be developed. These systems may see future introduction in vehicles only if they are non-intrusive and completely transparent to the driver. To address these challenges we propose a measurement system that uses a single high-speed camera and IR illumination to measure the eyelid opening and the orientation of the driver's head and gaze. These measurements are vital for any system that attempts to assess the driver's fatigue and attention levels. Currently, we require the driver to wear a small headband with markers and to perform a short calibration for gaze estimation. We are working actively towards removing these limitations. The results that we have obtained so far are encouraging and in accordance with our expectations. The system is accurate (gaze accuracy varies from 1/spl deg/ when the head rotation angles are smaller than 15/spl deg/ to 3/spl deg/ for head angles from roughly 15/spl deg/ to roughly 45/spl deg/), and very robust, and performs reliably even with most of the people wearing glasses. To our knowledge, we here present the only system that can reliably measure eyelid opening, and head and gaze orientations with a single camera at high speed (120Hz).","","0-7803-9029-6","10.1109/ISSCS.2005.1509893","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1509893","","Monitoring;Cameras;Lighting;Fatigue;Eyelids;Road accidents;Vehicle driving;Calibration;Robustness;Glass","computerised monitoring;cameras;lighting;road accidents;road safety;measurement systems","driver monitoring;high-speed camera;IR illumination;road accidents;attention levels;vehicles;measurement system;eyelid opening;head orientation;fatigue levels;gaze estimation;120 Hz","","9","","14","","26 Sep 2005","","","IEEE","IEEE Conferences"
"One-time calibration eye gaze detection system","L. C. Kiat; S. Ranganath","Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore; Dept. of Electr. & Comput. Eng., Nat. Univ. of Singapore, Singapore","2004 International Conference on Image Processing, 2004. ICIP '04.","18 Apr 2005","2004","2","","873","876 Vol.2","This paper describes a real-time infrared-based system for eye gaze tracking. Most of the existing gaze tracking systems require a cumbersome calibration process every time an user operates the system and work well only with small head movement. Our system is simpler as it requires only one-time calibration for each user and the calibration data is stored for subsequent reuse. Besides that, our system can perform robust and accurate gaze estimation under rather significant head pose variations. This is made possible by a gaze calibration procedure which maps the pupil and glint parameters to screen coordinates using the Radial Basis Function Neural Network (RBFNN). Our system currently has a spatial gaze resolution of about 3 degrees and it operates at 25 fps with an average accuracy of 90% for gaze estimation.","1522-4880","0-7803-8554-3","10.1109/ICIP.2004.1419438","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1419438","","Calibration;Lenses;Cameras;Light emitting diodes;Iris;Light sources;Head;Coaxial components;Tracking;Robustness","real-time systems;eye;infrared imaging;tracking;edge detection;calibration;radial basis function networks;image resolution;motion estimation","real-time infrared-based system;eye gaze tracking;cumbersome calibration process;operating system;small head movement;radial basis function neural network;RBFNN;spatial gaze resolution;detection system","","1","","8","","18 Apr 2005","","","IEEE","IEEE Conferences"
"Non-intrusive eye gaze tracking under natural head movements","S. M. Kim; M. Sked; Q. Ji","Dept. of Electr. Eng. Syst. Eng., Rensselaer Polytech. Inst., New York, NY, USA; Dept. of Electr. Eng. Syst. Eng., Rensselaer Polytech. Inst., New York, NY, USA; Dept. of Electr. Eng. Syst. Eng., Rensselaer Polytech. Inst., New York, NY, USA","The 26th Annual International Conference of the IEEE Engineering in Medicine and Biology Society","14 Mar 2005","2004","1","","2271","2274","We propose an eye gaze tracking system under natural head movements. The system consists of one CCD camera and two mirrors. Based on geometric and linear algebra calculations, the mirrors rotate to follow head movements in order to keep the eyes within the view of the camera. Our system allows the subjects head to move 30 cm horizontally and 20 cm vertically, with spatial gaze resolutions about 6 degree and 7 degree, respectively and a frame rate about 10 Hz. We also introduce a hierarchical generalized regression neural networks (H-GRNN) scheme to map eye and mirror parameters to gaze, achieving a gaze estimation accuracy of 92% under head movements. The use of H-GRNN also eliminates the need for personal calibration for new subjects since H-GRNN can generalize. Preliminary experiments show our system is accurate and robust in gaze tracking under large head movements.","","0-7803-8439-3","10.1109/IEMBS.2004.1403660","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1403660","Eye movement;Gaze tracking;Generalized Neural Network;Head movement","Tracking;Mirrors;Charge coupled devices;Charge-coupled image sensors;Linear algebra;Eyes;Cameras;Spatial resolution;Neural networks;Calibration","eye;biomechanics;CCD image sensors;neural nets;biomedical optical imaging;medical image processing;image resolution","nonintrusive eye gaze tracking;natural head movements;CCD camera;spatial gaze resolution;hierarchical generalized regression neural networks","","10","","13","","14 Mar 2005","","","IEEE","IEEE Conferences"
"Implicit Calibration of a Remote Gaze Tracker","X. L. C. Brolly; J. B. Mulligan",NASA Ames Research Center; NA,"2004 Conference on Computer Vision and Pattern Recognition Workshop","24 Jan 2005","2004","","","134","134","We describe a system designed to monitor the gaze of a user working naturally at a computer workstation. The system consists of three cameras situated between the keyboard and the monitor. Free head movements are allowed within a three-dimensional volume approximately 40 centimeters in diameter. Two fixed, wide-field ""face"" cameras equipped with active-illumination systems enable rapid localization of the subject's pupils. A third steerable ""eye"" camera has a relatively narrow field of view, and acquires the images of the eyes which are used for gaze estimation. Unlike previous approaches which construct an explicit three-dimensional representation of the subject's head and eye, we derive mappings for steering control and gaze estimation using a procedure we call implicit calibration. Implicit calibration is performed by collecting a ""training set"" of parameters and associated measurements, and solving for a set of coefficients relating the measurements back to the parameters of interest. Preliminary data on three subjects indicate an median gaze estimation error of ap-proximately 0.8 degree.","","","10.1109/CVPR.2004.366","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1384930","","Calibration;Cameras;Computerized monitoring;Remote monitoring;Head;Workstations;Keyboards;Eyes;Performance evaluation;Estimation error","","","","15","3","","","24 Jan 2005","","","IEEE","IEEE Conferences"
"Eye-mouse under large head movement for human-computer interface","Dong Hyun Yoo; Myung Jin Chung","Dept. of Electr. Eng. & Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea; Dept. of Electr. Eng. & Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea","IEEE International Conference on Robotics and Automation, 2004. Proceedings. ICRA '04. 2004","6 Jul 2004","2004","1","","237","242 Vol.1","An eye gaze estimation system is proposed. The eye gaze estimation system estimates the eye-gaze direction of a user. The proposed system consisted of five IR LEDs and a CCD camera. The direction of the user's eye gaze can be computed without computing the geometrical relation among the eye, the camera, and the monitor in 3D space. Our method is comparatively simple and fast. The disabled can manipulate a computer using this system. We would introduce our method and show experimental results. The experimental results verify the feasibility of the proposed system as an interface for the disabled.","1050-4729","0-7803-8232-3","10.1109/ROBOT.2004.1307157","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1307157","","Head;Optical reflection;Computer vision;Light emitting diodes;Charge coupled devices;Charge-coupled image sensors;Cameras;Calibration;Robots;Equations","human computer interaction;eye;handicapped aids;CCD image sensors;light emitting diodes","human-computer interface;eye gaze estimation system;IR LED;CCD camera;handicapped aids","","4","","18","","6 Jul 2004","","","IEEE","IEEE Conferences"
"Non-intrusive eye gaze estimation without knowledge of eye pose","Dong Hyun Yoo; Myung Jin Chung","Dept. of Electr. Eng. & Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea; Dept. of Electr. Eng. & Comput. Sci., Korea Adv. Inst. of Sci. & Technol., Daejeon, South Korea","Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.","7 Jun 2004","2004","","","785","790","Eye gaze estimation is to detect the eye gaze of human. By detection of the eye gaze, a machine can read the intention. Eye gaze is very useful to read human intention. In this paper, a new eye gaze estimation method is suggested. The method allows large head movement without knowledge of eye pose. Also, feature detection is very important to achieve good performance. However, pupil detection is difficult because pupil color is similar to iris color. This paper proposes a robust feature detection method. This eye gaze estimation method is powerful, and it can work in real-time.","","0-7695-2122-3","10.1109/AFGR.2004.1301630","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1301630","","Cameras;Magnetic heads;Computer vision;Humans;Light emitting diodes;Optical reflection;Cornea;Charge-coupled image sensors;Robustness;Charge coupled devices","feature extraction;transforms;behavioural sciences","nonintrusive eye gaze estimation;human intention detection;feature detection;projective transform","","6","4","7","","7 Jun 2004","","","IEEE","IEEE Conferences"
"Conic-based algorithm for visual line estimation from one image","Haiyuan Wu; Qian Chen; T. Wada","Fac. of Syst. Eng., Wakayama Univ., Japan; Fac. of Syst. Eng., Wakayama Univ., Japan; Fac. of Syst. Eng., Wakayama Univ., Japan","Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings.","7 Jun 2004","2004","","","260","265","This work describes a method to estimate visual line from a single monocular image. By assuming that the visual lines of the both eyes are parallel and the iris boundaries are circles, we propose a ""two-circle"" algorithm that can estimate the normal vector of the supporting plane of the iris boundaries, from which the direction of the visual line can be calculated. Most existing gaze estimation algorithms require eye corners and some heuristic knowledge about the structure of the eye in addition to the iris contours. In contrast to the exiting methods, our one does not use either of that additional information. Another advantage of our algorithm is that a camera with an unknown focal length can be used without assuming the orthographical projection. This is a very useful feature because it allows one to use a zoom lens and to change the zooming factor whenever he or she likes. It also gives one more freedom of the camera setting because keeping the camera far from the eyes is not necessary in our method. The extensive experiments over simulated images and real images demonstrate the robustness and the effectiveness of our method.","","0-7695-2122-3","10.1109/AFGR.2004.1301541","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1301541","","Iris;Cameras;Eyes;Systems engineering and theory;Cities and towns;Lenses;Computational modeling;Robustness;Human computer interaction;Application software","visual perception;eye;cameras;image processing;signal detection","conic-based algorithm;visual line estimation;monocular image;iris contours;gaze estimation;eye corners;camera setting","","1","","16","","7 Jun 2004","","","IEEE","IEEE Conferences"
"Eye gaze estimation from a single image of one eye","Wang; Sung; Ronda Venkateswarlu","Inst. for Infocomm Res., Singapore; NA; NA","Proceedings Ninth IEEE International Conference on Computer Vision","3 Apr 2008","2003","","","136","143 vol.1","We present a novel approach, called the ""one-circle "" algorithm, for measuring the eye gaze using a monocular image that zooms in on only one eye of a person. Observing that the iris contour is a circle, we estimate the normal direction of this iris circle, considered as the eye gaze, from its elliptical image. From basic projective geometry, an ellipse can be back-projected into space onto two circles of different orientations. However, by using an anthropometric property of the eyeball, the correct solution can be disambiguated. This allows us to obtain a higher resolution image of the iris with a zoom-in camera and thereby achieving higher accuracies in the estimation. The robustness of our gaze determination approach was verified statistically by the extensive experiments on synthetic and real image data. The two key contributions are that we show the possibility of finding the unique eye gaze direction from a single image of one eye and that one can obtain better accuracy as a consequence of this.","","0-7695-1950-4","10.1109/ICCV.2003.1238328","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1238328","","Iris;Head;Image edge detection;Geometry;Robustness;Humans;Image resolution;Joining processes;Cameras;Sockets","eye;image processing;estimation theory","one circle algorithm;eye gaze estimation;monocular image;iris contour;elliptical image;anthropometric property;gaze determination approach;real image data","","82","5","21","","3 Apr 2008","","","IEEE","IEEE Conferences"
"Model-based head pose estimation for air-traffic controllers","X. L. C. Brolly; C. Stratelos; J. B. Mulligan","San Jose State Univ., CA, USA; San Jose State Univ., CA, USA; NA","Proceedings 2003 International Conference on Image Processing (Cat. No.03CH37429)","24 Nov 2003","2003","2","","II","113","A method for estimating the point of fixation of an air traffic controller from a low resolution video sequence is presented. A geometric model of the head is used to estimate head orientation; head pose estimates are combined with a 3D model of the environment to compute the target of gaze. The head model is constructed from a small set of images. Two lighting models are considered: in the first, we only use ambient lighting; in the second, we add a finite distance point source. In both cases, we jointly estimate the albedo of each facet of the head model and the parameters of the lighting model. Because ground-truth data are unavailable, the absolute accuracy of the gaze estimates is unknown. With either method, the results are sufficiently accurate to answer questions of operational interest, such as ""is the controller looking out the window"".","1522-4880","0-7803-7750-8","10.1109/ICIP.2003.1246629","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1246629","","Shape;Magnetic heads;Computer displays;NASA;Solid modeling;Poles and towers;Rendering (computer graphics);Postal services;State estimation;Air traffic control","air traffic control;video signal processing;image sequences","head pose estimation;air-traffic controllers;video sequence;head orientation;gaze tracking;lighting models;ambient lighting;finite distance point source;albedo estimation","","7","","6","","24 Nov 2003","","","IEEE","IEEE Conferences"
"Image based eye-gaze estimation irrespective of head direction","T. Miyake; S. Haruta; S. Horihata",NA; NA; NA,"Industrial Electronics, 2002. ISIE 2002. Proceedings of the 2002 IEEE International Symposium on","29 Dec 2020","2002","1","","332","336 vol.1","One can roughly estimate eye-gaze direction of a person or a portrait. In particular, it seems likely that one can accurately recognize whether or not one meets a person's eyes directly. If a machine realizes such visual information processing as a man does, one can tell one's intention to the machine with one's eyes. Most previous methods of eye-gaze estimation use a special light source or a hardware device. These methods however are not user friendly because they require the user to restrict his motion. We propose a new method for eye-gaze estimation. This method, based on image processing, uses only a single facial image obtained by a CCD camera under ordinary lighting condition without using any instrument except a pair of small marks. Experimental results show that the proposed method produces good accuracy irrespective of the head direction.","","0-7803-7369-3","10.1109/ISIE.2002.1026088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5730785","","Head;Cameras;Estimation;Lenses;Visualization;Optical imaging;Magnetic heads","CCD image sensors;image processing;lighting;user interfaces","CCD camera;experimental results;facial image;head direction;human machine interface;image based eye-gaze estimation;image processing;lighting condition;user friendly;visual information processing","","","","","","29 Dec 2020","","","IEEE","IEEE Conferences"
"Real-time eye detection and tracking for driver observation under various light conditions","Xia Liu; Fengliang Xu; K. Fujimura","Ohio State Univ., Columbus, OH, USA; NA; NA","Intelligent Vehicle Symposium, 2002. IEEE","26 Mar 2003","2002","2","","344","351 vol.2","Eye tracking is one of the key technologies for future driver assistance systems since human eyes contain much information about the driver's condition such as gaze, attention level, and fatigue level. Thus, nonintrusive methods for eye detection and tracking are important for many applications of vision-based driver-automotive interaction. One problem common to many eye tracking methods proposed so far is their sensitivity to lighting condition change. This tends to significantly limit their scope for automotive applications. In this paper we present a new realtime eye detection and tracking method that works under variable and realistic lighting conditions. By combining imaging by using IR light and appearance-based object recognition techniques, our method can robustly track eyes even when the pupils are not very bright due to significant external illumination interferences. The appearance model is incorporated in both eye detection and tracking via the use of a support vector machine and mean shift tracking. Our experimental results show the feasibility of our approach and the validity for the method is extended for drivers wearing sunglasses.","","0-7803-7346-4","10.1109/IVS.2002.1187975","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1187975","","Eyes;Humans;Fatigue;Automotive applications;Optical imaging;Object recognition;Robustness;Lighting;Interference;Support vector machines","driver information systems;object recognition;Kalman filters;computer vision","real-time eye detection;driver observation;eye tracking;driver assistance systems;gaze;attention level;fatigue level;vision-based driver-automotive interaction;IR light;appearance-based object recognition;appearance model;Kalman filtering","","12","","18","","26 Mar 2003","","","IEEE","IEEE Conferences"
"Appearance-based eye gaze estimation","Kar-Han Tan; D. J. Kriegman; N. Ahuja","Dept. of Comput. Sci., Illinois Univ., Urbana-Champaign, IL, USA; NA; NA","Sixth IEEE Workshop on Applications of Computer Vision, 2002. (WACV 2002). Proceedings.","28 Feb 2003","2002","","","191","195","We present a method for estimating eye gaze direction, which represents a departure from conventional eye gaze estimation methods, the majority of which are based on tracking specific optical phenomena like corneal reflection and the Purkinje images. We employ an appearance manifold model, but instead of using a densely sampled spline to perform the nearest manifold point query, we retain the original set of sparse appearance samples and use linear interpolation among a small subset of samples to approximate the nearest manifold point. The advantage of this approach is that since we are only storing a sparse set of samples, each sample can be a high dimensional vector that retains more representational accuracy than short vectors produced with dimensionality reduction methods. The algorithm was tested with a set of eye images labelled with ground truth point-of-regard coordinates. We have found that the algorithm is capable of estimating eye gaze with a mean angular error of 0.38 degrees, which is comparable to that obtained by commercially available eye trackers.","","0-7695-1858-3","10.1109/ACV.2002.1182180","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1182180","","Iris;Vectors;Humans;Displays;Application software;Gray-scale;Deformable models;Optical reflection;Spline;Interpolation","behavioural sciences;user interfaces;image processing;interpolation","eye gaze direction;gaze estimation;appearance manifold model;sparse appearance samples;eye trackers;visual attention;human users;behavioural studies","","45","12","17","","28 Feb 2003","","","IEEE","IEEE Conferences"
"A human-robot interface using vision-based eye gaze estimation system","Dong Hyun Yoo; Jae Heon Kim; Do Hyung Kim; Myung Jin Chung",EECS; NA; NA; NA,"IEEE/RSJ International Conference on Intelligent Robots and Systems","10 Dec 2002","2002","2","","1196","1201","","","0-7803-7398-7","10.1109/IRDS.2002.1043896","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1043896","","Optical reflection;Light emitting diodes;Monitoring;Charge coupled devices;Charge-coupled image sensors;Cameras;Lenses;Keyboards;Robot vision systems;Skin","","","","","","14","","10 Dec 2002","","","IEEE","IEEE Conferences"
"Study on eye gaze estimation","Jian-Gang Wang; E. Sung","Sch. of Electr. & Electron. Eng., Nanyang Technol. Univ., Singapore; NA","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","7 Aug 2002","2002","32","3","332","350","There are two components to the human visual line-of-sight: pose of human head and the orientation of the eye within their sockets. We have investigated these two aspects but will concentrate on eye gaze estimation. We present a novel approach called the ""one-circle"" algorithm for measuring the eye gaze using a monocular image that zooms in on only one eye of a person. Observing that the iris contour is a circle, we estimate the normal direction of this iris circle, considered as the eye gaze, from its elliptical image. From basic projective geometry, an ellipse can be back-projected into space onto two circles of different orientations. However, by using a geometric constraint, namely, that the distance between the eyeball's center and the two eye corners should be equal to each other, the correct solution can be disambiguated. This allows us to obtain a higher resolution image of the iris with a zoom-in camera, thereby achieving higher accuracies in the estimation. A general approach that combines head pose determination with eye gaze estimation is also proposed. The searching of the eye gaze is guided by the head pose information. The robustness of our gaze determination approach was verified statistically by the extensive experiments on synthetic and real image data. The two key contributions are that we show the possibility of finding the unique eye gaze direction from a single image of one eye and that one can obtain better accuracy as a consequence of this.","1941-0492","","10.1109/TSMCB.2002.999809","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=999809","","Head;Humans;Iris;Robustness;Man machine systems;Sockets;Computer vision;Face;Joining processes;Mouth","user interfaces;interactive devices;image resolution;computational geometry;edge detection","eye gaze estimation;human visual line-of-sight;human head pose;eye orientation;one-circle algorithm;monocular image;iris contour;elliptical image;projective geometry;human-computer interaction;geometric constraint;image resolution;zoom-in camera;head pose determination;gaze determination;experiments","","100","4","41","","7 Aug 2002","","","IEEE","IEEE Journals"
"Real-time stereo tracking for head pose and gaze estimation","R. Newman; Y. Matsumoto; S. Rougeaux; A. Zelinsky","Res. Sch. of Inf. Sci. & Eng., Australian Nat. Univ., Canberra, ACT, Australia; NA; NA; NA","Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580)","6 Aug 2002","2000","","","122","128","Computer systems which analyse human face/head motion have attracted significant attention recently as there are a number of interesting and useful applications. Not least among these is the goal of tracking the head in real time. A useful extension of this problem is to estimate the subject's gaze point in addition to his/her head pose. This paper describes a real-time stereo vision system which determines the head pose and gaze direction of a human subject. Its accuracy makes it useful for a number of applications including human/computer interaction, consumer research and ergonomic assessment.","","0-7695-0580-5","10.1109/AFGR.2000.840622","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=840622","","Humans;Electrical capacitance tomography;Stereo vision;Application software;Ergonomics;Computer vision;Face detection;Cameras;Australia;Uniform resource locators","tracking;stereo image processing;real-time systems;gesture recognition;face recognition","real-time stereo tracking;head pose estimation;gaze estimation;computer systems;motion analysis;head tracking;real-time stereo vision;gaze direction;human/computer interaction;consumer research;ergonomic assessment","","57","14","14","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Gaze detection via self-organizing gray-scale units","M. Betke; J. Kawai","Dept. of Comput. Sci., Boston Coll., Chestnut Hill, MA, USA; NA","Proceedings International Workshop on Recognition, Analysis, and Tracking of Faces and Gestures in Real-Time Systems. In Conjunction with ICCV'99 (Cat. No.PR00378)","6 Aug 2002","1999","","","70","76","We present a gaze estimation algorithm that detects an eye in a face image and estimates the gaze direction by computing the position of the pupil with respect to the center of the eye. The algorithm is information conserving and based on unsupervised learning. It creates a map of self-organized gray-scale image units that collectively learn to describe the eye outline.","","0-7695-0378-0","10.1109/RATFG.1999.799226","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=799226","","Gray-scale;Phase estimation","object detection;unsupervised learning;computer vision;user interfaces;eye;self-organising feature maps","gaze detection;self-organizing gray-scale units;gaze estimation algorithm;eye detection;face image;gaze direction estimation;unsupervised learning","","20","","11","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Gaze estimation using morphable models","T. D. Rikert; M. J. Jones","Digital Equipment Corp., Cambridge, MA, USA; NA","Proceedings Third IEEE International Conference on Automatic Face and Gesture Recognition","6 Aug 2002","1998","","","436","441","The paper presents preliminary work on a novel technique for gaze estimation from a single image. The goal is to provide rough estimates of where a person is looking at a monitor. Many applications for human-computer interaction are possible for such a technique. The approach uses the morphable model framework of Jones and Poggio (1998) to model a region around the eyes of a person. After matching this model to an input image of a person's eye region, the resulting model parameters are sent to a neural network which approximates the screen coordinates being viewed. The system is user independent and can handle changes in both head orientation and iris location. Currently the system is not real-time. Future work will focus on improving the speed of the system.","","0-8186-8344-9","10.1109/AFGR.1998.670987","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=670987","","Face detection;Head;Neural networks;Eyes;Iris;Monitoring;Facial animation;Cameras;Impedance matching;Detectors","image segmentation;image matching;neural nets","morphable models;gaze estimation;single image;monitor;human-computer interaction;eye region modelling;model parameters;neural network;screen coordinates;head orientation;iris location","","29","31","10","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Estimating gaze from a single view of a face","A. Gee; R. Cipolla","Dept. of Eng., Cambridge Univ., UK; NA","Proceedings of 12th International Conference on Pattern Recognition","6 Aug 2002","1994","1","","758","760 vol.1","Current approaches to gaze tracking tend to be highly intrusive: the subject must either remain perfectly still, or wear cumbersome headgear to maintain a constant separation between the sensor and the eye. This paper describes a more flexible vision-based approach, which can estimate the direction of gaze from a single, monocular view of a face. The technique makes minimal assumptions about the structure of the face, requires very few image measurements, and produces a useful estimate of the facial orientation. The computational requirements are insignificant, so with automatic tracking of a few facial features it is possible to produce real-time gaze estimates.","","0-8186-6265-4","10.1109/ICPR.1994.576433","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=576433","","Head;Eyes;Nose;Calibration;Wearable sensors;Maintenance engineering;Facial features;Current measurement;Optical reflection;Retina","tracking","gaze tracking;vision-based approach;facial orientation;automatic tracking;real-time gaze estimates","","23","2","8","","6 Aug 2002","","","IEEE","IEEE Conferences"
"Pattern Recognition, Part 2","C. Liu; B. Lovell; D. Tao; M. Tistarelli","Chinese Academy of Sciences; University of Queensland, Australia; University of Technology Sydney, Australia; University of Sassari, Italy","IEEE Intelligent Systems","25 May 2016","2016","31","3","3","5","This second part of the special issue on pattern recognition reports the advances in pattern recognition for visual data. The selected articles address visual categorization by cross-domain dictionary learning, facial expression recognition, face sketch-photo matching, heartbeat rate measurement from facial video, driver gaze estimation, nighttime vehicle detection, and overlaid arrow detection in biomedical images, respectively.","1941-1294","","10.1109/MIS.2016.55","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478487","pattern recognition;computer vision;cross-domain dictionary learning;facial expression;sketch-photo matching;heartbeat rate measurement;driver gaze estimation;vehicle detection;arrow detection;intelligent systems","Special issues and sections;Pattern recognition;Visualization;VIsual analytics;Visual data","","","","","","","","25 May 2016","","","IEEE","IEEE Magazines"
"Erratum to “General Theory of Remote Gaze Estimation Using the Pupil Center and Corneal Reflections”","E. D. Guestrin; M. Eizenman",NA; NA,"IEEE Transactions on Biomedical Engineering","17 Jul 2006","2006","53","8","1728","1728","","1558-2531","","10.1109/TBME.2006.880503","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1658174","","Estimation","","","","1","1","1","","17 Jul 2006","","","IEEE","IEEE Journals"
"Forging a Close Relationship with Multimedia Communities","W. Zeng; Z. Liu; E. Steinbach",University of Missouri and Microsoft Research Asia; Microsoft Research; Technical University of Munich,"IEEE MultiMedia","3 Nov 2014","2014","21","4","14","15","In May 2014, the authors of the top 26 papers from the IEEE International Conference on Multimedia & Expo (ICME) 2014 were invited to submit extended versions of their papers to this fast track special issue. After a rigorous peer-review process, eight of those submissions were accepted for this special issue, now titled ""Hot Topics in Multimedia Research."" This is just the beginning of a close collaboration between MM and major multimedia conferences.","1941-0166","","10.1109/MMUL.2014.60","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6945277","multimedia;multimedia research;ICME;visual analysis;object tracking;image quality assessment;learning algorithm;location propagation;gaze estimation;distributed video coding","","","","","","","","","3 Nov 2014","","","IEEE","IEEE Magazines"
